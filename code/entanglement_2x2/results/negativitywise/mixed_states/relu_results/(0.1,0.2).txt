#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 80; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.72000000000001%
#Sample standard deviation for averaged success rate: 0.05042717521336711%
#Same average success rate for supplementary tests: [0.5006449999999999, 0.524395, 0.721995, 0.9727950000000001, 0.9992, 0.9988699999999999, 0.998985, 0.998575, 0.999685, 0.9998049999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.0035213923664865867, 0.0033805427373056534, 0.0023866160140982033, 0.0005472379726864637, 6.299206299208369e-05, 0.00015933033295642966, 0.00012358595692875065, 0.0001522283441084246, 4.2071828460367546e-05, 3.119453077074135e-05]%
#Epoch	Loss	Sample STD
1	0.616891	0.006283
2	0.381965	0.014807
3	0.210376	0.015108
4	0.128618	0.013351
5	0.083200	0.010461
6	0.057546	0.007740
7	0.042398	0.005585
8	0.033095	0.004082
9	0.027055	0.003016
10	0.022786	0.002367
11	0.019834	0.001916
12	0.017578	0.001621
13	0.015775	0.001395
14	0.014353	0.001276
15	0.013302	0.001204
16	0.012397	0.001117
17	0.011607	0.001069
18	0.010850	0.001017
19	0.010287	0.000972
20	0.009894	0.000981
21	0.009372	0.000965
22	0.009021	0.000954
23	0.008677	0.000909
24	0.008332	0.000921
25	0.008051	0.000891
26	0.007796	0.000875
27	0.007583	0.000864
28	0.007297	0.000847
29	0.007082	0.000837
30	0.006886	0.000851
31	0.006578	0.000817
32	0.006422	0.000792
33	0.006264	0.000830
34	0.006150	0.000800
35	0.005867	0.000776
36	0.005865	0.000802
37	0.005609	0.000783
38	0.005512	0.000768
39	0.005325	0.000755
40	0.005292	0.000756
41	0.005094	0.000728
42	0.005026	0.000771
43	0.004960	0.000748
44	0.004828	0.000735
45	0.004660	0.000708
46	0.004634	0.000693
47	0.004501	0.000677
48	0.004360	0.000693
49	0.004251	0.000641
50	0.004300	0.000687
51	0.004156	0.000649
52	0.004006	0.000629
53	0.003967	0.000657
54	0.003871	0.000605
55	0.003823	0.000605
56	0.003758	0.000609
57	0.003685	0.000611
58	0.003699	0.000597
59	0.003556	0.000592
60	0.003509	0.000582
61	0.003415	0.000580
62	0.003392	0.000588
63	0.003300	0.000541
64	0.003201	0.000568
65	0.003164	0.000561
66	0.003147	0.000534
67	0.003038	0.000539
68	0.003013	0.000532
69	0.002926	0.000527
70	0.002906	0.000519
71	0.002707	0.000496
72	0.002791	0.000515
73	0.002685	0.000486
74	0.002692	0.000497
75	0.002598	0.000459
76	0.002546	0.000474
77	0.002616	0.000491
78	0.002513	0.000471
79	0.002457	0.000464
80	0.002359	0.000422
