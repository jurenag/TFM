#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 80; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.85000000000001%
#Sample standard deviation for averaged success rate: 0.030114365342804723%
#Same average success rate for supplementary tests: [0.50071, 0.52185, 0.7192799999999999, 0.980985, 0.9995700000000001, 0.9979199999999999, 0.997025, 0.999555, 0.99979, 0.9998450000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.003525616938210956, 0.003397746146344661, 0.0024187319156946706, 0.0004535377478777215, 4.680550181327209e-05, 0.0002066002904160718, 0.00024735090206020764, 6.830152816743059e-05, 3.4609175084181006e-05, 2.870189453646951e-05]%
#Epoch	Loss	Sample STD
1	0.618598	0.009899
2	0.387538	0.028251
3	0.211053	0.029472
4	0.123819	0.022565
5	0.075464	0.016178
6	0.050023	0.012387
7	0.035768	0.009658
8	0.027235	0.007762
9	0.021723	0.006307
10	0.017665	0.005009
11	0.014632	0.003968
12	0.012372	0.003145
13	0.010652	0.002543
14	0.009360	0.002105
15	0.008295	0.001794
16	0.007574	0.001622
17	0.006952	0.001457
18	0.006364	0.001352
19	0.005974	0.001284
20	0.005530	0.001183
21	0.005216	0.001163
22	0.004873	0.001090
23	0.004551	0.001033
24	0.004269	0.000968
25	0.004086	0.000922
26	0.003815	0.000873
27	0.003648	0.000831
28	0.003478	0.000817
29	0.003279	0.000766
30	0.003167	0.000733
31	0.003004	0.000703
32	0.002916	0.000698
33	0.002815	0.000674
34	0.002712	0.000655
35	0.002611	0.000636
36	0.002611	0.000614
37	0.002449	0.000621
38	0.002486	0.000634
39	0.002367	0.000595
40	0.002250	0.000606
41	0.002190	0.000574
42	0.002155	0.000590
43	0.002019	0.000547
44	0.002056	0.000574
45	0.001934	0.000604
46	0.001920	0.000568
47	0.001872	0.000568
48	0.001895	0.000551
49	0.001772	0.000538
50	0.001833	0.000562
51	0.001795	0.000574
52	0.001719	0.000540
53	0.001659	0.000525
54	0.001732	0.000560
55	0.001586	0.000553
56	0.001482	0.000533
57	0.001484	0.000503
58	0.001523	0.000542
59	0.001565	0.000521
60	0.001451	0.000496
61	0.001392	0.000492
62	0.001336	0.000475
63	0.001384	0.000480
64	0.001242	0.000449
65	0.001335	0.000481
66	0.001222	0.000459
67	0.001239	0.000477
68	0.001205	0.000475
69	0.001151	0.000471
70	0.001136	0.000454
71	0.001113	0.000443
72	0.001077	0.000428
73	0.001104	0.000443
74	0.001078	0.000431
75	0.001021	0.000460
76	0.001017	0.000431
77	0.001043	0.000443
78	0.000971	0.000419
79	0.000974	0.000419
80	0.000921	0.000426
