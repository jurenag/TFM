#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 80; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.8175%
#Sample standard deviation for averaged success rate: 0.038378684676526074%
#Same average success rate for supplementary tests: [0.5008699999999999, 0.52408, 0.66678, 0.9029, 0.993025, 0.999315, 0.99559, 0.995915, 0.9975700000000001, 0.9995]%
#Sample STD for averaged success rate in supplementary tests: [0.003525253488048767, 0.003389293094437246, 0.0026307359388581734, 0.0011643365063417023, 0.00020950529527911642, 0.00010583732210333598, 0.00030220455820521137, 0.0002651238177719949, 0.0001576539089271109, 5.037360419885361e-05]%
#Epoch	Loss	Sample STD
1	0.612427	0.006651
2	0.379506	0.017545
3	0.199287	0.017380
4	0.107614	0.012933
5	0.063548	0.010094
6	0.041708	0.007617
7	0.029903	0.005621
8	0.023004	0.004224
9	0.018606	0.003213
10	0.015608	0.002574
11	0.013533	0.002128
12	0.011917	0.001816
13	0.010670	0.001594
14	0.009718	0.001449
15	0.009021	0.001300
16	0.008315	0.001202
17	0.007808	0.001117
18	0.007330	0.001012
19	0.006984	0.000971
20	0.006640	0.000900
21	0.006322	0.000857
22	0.006088	0.000835
23	0.005843	0.000805
24	0.005593	0.000764
25	0.005420	0.000735
26	0.005141	0.000695
27	0.005002	0.000660
28	0.004792	0.000669
29	0.004765	0.000643
30	0.004558	0.000632
31	0.004528	0.000598
32	0.004391	0.000576
33	0.004314	0.000553
34	0.004251	0.000579
35	0.004020	0.000565
36	0.004109	0.000572
37	0.004006	0.000526
38	0.003895	0.000557
39	0.003780	0.000546
40	0.003706	0.000545
41	0.003672	0.000528
42	0.003621	0.000549
43	0.003605	0.000516
44	0.003404	0.000505
45	0.003421	0.000534
46	0.003288	0.000493
47	0.003305	0.000503
48	0.003255	0.000527
49	0.003269	0.000488
50	0.003168	0.000509
51	0.003149	0.000520
52	0.003094	0.000487
53	0.003025	0.000499
54	0.002960	0.000500
55	0.002934	0.000501
56	0.002924	0.000515
57	0.002854	0.000495
58	0.002865	0.000496
59	0.002898	0.000504
60	0.002853	0.000514
61	0.002785	0.000493
62	0.002831	0.000520
63	0.002650	0.000494
64	0.002700	0.000519
65	0.002701	0.000488
66	0.002656	0.000507
67	0.002592	0.000541
68	0.002548	0.000515
69	0.002530	0.000524
70	0.002481	0.000504
71	0.002495	0.000501
72	0.002395	0.000530
73	0.002316	0.000495
74	0.002425	0.000481
75	0.002350	0.000475
76	0.002391	0.000463
77	0.002425	0.000468
78	0.002389	0.000460
79	0.002296	0.000482
80	0.002262	0.000470
