#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/mixed_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.675%
#Sample standard deviation for averaged success rate: 0.06900067934448022%
#Same average success rate for supplementary tests: [0.998625, 0.99777, 0.9982850000000001, 0.999675, 0.999965]%
#Sample STD for averaged success rate in supplementary tests: [0.00019321094366003705, 0.00023569334950311973, 0.00018446663316164252, 5.449512592884864e-05, 1.322644132021871e-05]%
#Epoch	Loss	Sample STD
1	0.696403	0.001682
2	0.693334	0.000034
3	0.693291	0.000032
4	0.693225	0.000050
5	0.693097	0.000076
6	0.692990	0.000126
7	0.692772	0.000216
8	0.692389	0.000477
9	0.691459	0.001063
10	0.689528	0.002443
11	0.685654	0.005160
12	0.679608	0.009211
13	0.671432	0.013963
14	0.662031	0.018247
15	0.651448	0.022079
16	0.640344	0.025851
17	0.630173	0.029388
18	0.621510	0.032291
19	0.614349	0.034230
20	0.607796	0.035117
21	0.600046	0.035291
22	0.590212	0.035054
23	0.577574	0.034700
24	0.562304	0.034276
25	0.544292	0.033862
26	0.524187	0.033584
27	0.502653	0.033506
28	0.480715	0.033567
29	0.459304	0.033427
30	0.438507	0.033134
31	0.418856	0.032692
32	0.400523	0.032407
33	0.383149	0.032068
34	0.366306	0.031385
35	0.350163	0.030254
36	0.334660	0.028921
37	0.319906	0.027524
38	0.306078	0.026457
39	0.293259	0.025868
40	0.281384	0.025587
41	0.269903	0.025403
42	0.258466	0.025248
43	0.246760	0.025048
44	0.235053	0.024688
45	0.222875	0.024250
46	0.210781	0.023701
47	0.198645	0.023043
48	0.186765	0.022319
49	0.174987	0.021498
50	0.163725	0.020695
51	0.152837	0.019792
52	0.142380	0.018952
53	0.132576	0.018141
54	0.123087	0.017360
55	0.114185	0.016605
56	0.105760	0.015907
57	0.097713	0.015247
58	0.090122	0.014603
59	0.083038	0.014029
60	0.076403	0.013459
61	0.070240	0.012945
62	0.064625	0.012408
63	0.059447	0.011885
64	0.054653	0.011323
65	0.050254	0.010773
66	0.046206	0.010222
67	0.042585	0.009607
68	0.039200	0.009026
69	0.036119	0.008398
70	0.033328	0.007766
71	0.030770	0.007121
72	0.028408	0.006510
73	0.026278	0.005909
74	0.024413	0.005378
75	0.022711	0.004869
76	0.021210	0.004437
77	0.019880	0.004074
78	0.018680	0.003734
79	0.017648	0.003445
80	0.016726	0.003220
81	0.015940	0.002991
82	0.015173	0.002818
83	0.014505	0.002642
84	0.013912	0.002499
85	0.013364	0.002385
86	0.012822	0.002243
87	0.012425	0.002166
88	0.011978	0.002042
89	0.011582	0.001971
90	0.011200	0.001880
91	0.010825	0.001814
92	0.010552	0.001726
93	0.010267	0.001680
94	0.009966	0.001613
95	0.009691	0.001544
96	0.009466	0.001505
97	0.009199	0.001458
98	0.009017	0.001411
99	0.008813	0.001366
100	0.008606	0.001329
