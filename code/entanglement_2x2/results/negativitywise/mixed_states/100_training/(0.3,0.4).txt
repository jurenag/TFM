#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/mixed_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.92999999999999%
#Sample standard deviation for averaged success rate: 0.015371239377481619%
#Same average success rate for supplementary tests: [0.99075, 0.9886299999999999, 0.9939050000000001, 0.99935, 0.9999550000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.00047039544534359693, 0.0005610135069675342, 0.0003817951659594272, 7.469186702718249e-05, 1.4996624619885833e-05]%
#Epoch	Loss	Sample STD
1	0.695934	0.000988
2	0.693334	0.000026
3	0.693233	0.000033
4	0.693109	0.000046
5	0.692988	0.000090
6	0.692763	0.000176
7	0.692415	0.000382
8	0.691767	0.000804
9	0.690416	0.001683
10	0.687971	0.003409
11	0.683633	0.006383
12	0.677265	0.010560
13	0.669227	0.015481
14	0.659946	0.020764
15	0.650253	0.026044
16	0.641546	0.030807
17	0.634266	0.034524
18	0.628242	0.037097
19	0.622597	0.038768
20	0.616026	0.039976
21	0.607520	0.040881
22	0.596007	0.041486
23	0.580294	0.041911
24	0.560011	0.042248
25	0.535083	0.042604
26	0.506605	0.042806
27	0.476437	0.042911
28	0.446898	0.043103
29	0.420218	0.043530
30	0.396736	0.044187
31	0.376234	0.044869
32	0.358615	0.045496
33	0.343102	0.045941
34	0.329482	0.046244
35	0.317226	0.046503
36	0.306340	0.046701
37	0.296244	0.046928
38	0.287049	0.047223
39	0.278436	0.047455
40	0.270242	0.047599
41	0.262115	0.047649
42	0.254111	0.047629
43	0.245963	0.047503
44	0.237513	0.047210
45	0.228932	0.046776
46	0.220205	0.046270
47	0.211084	0.045685
48	0.201874	0.045055
49	0.192669	0.044551
50	0.183651	0.044099
51	0.175000	0.043717
52	0.166662	0.043303
53	0.158861	0.042808
54	0.151308	0.042151
55	0.144015	0.041288
56	0.136846	0.040260
57	0.129787	0.039001
58	0.122886	0.037656
59	0.116008	0.036113
60	0.109331	0.034511
61	0.102859	0.032889
62	0.096366	0.031176
63	0.090122	0.029435
64	0.083905	0.027628
65	0.077844	0.025774
66	0.071835	0.023851
67	0.066143	0.021961
68	0.060631	0.020081
69	0.055347	0.018284
70	0.050469	0.016630
71	0.045887	0.015070
72	0.041721	0.013720
73	0.037922	0.012524
74	0.034508	0.011500
75	0.031352	0.010534
76	0.028539	0.009702
77	0.026030	0.008943
78	0.023781	0.008254
79	0.021690	0.007615
80	0.019776	0.007002
81	0.018059	0.006423
82	0.016442	0.005889
83	0.014987	0.005387
84	0.013616	0.004891
85	0.012428	0.004466
86	0.011321	0.004067
87	0.010314	0.003712
88	0.009411	0.003363
89	0.008613	0.003080
90	0.007850	0.002793
91	0.007173	0.002543
92	0.006559	0.002320
93	0.006019	0.002123
94	0.005495	0.001928
95	0.005058	0.001768
96	0.004672	0.001630
97	0.004313	0.001494
98	0.003974	0.001376
99	0.003665	0.001267
100	0.003391	0.001168
