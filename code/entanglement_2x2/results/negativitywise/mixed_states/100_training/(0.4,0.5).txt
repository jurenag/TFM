#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/mixed_states/negativity_(0.4, 0.5).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.81499999999998%
#Sample standard deviation for averaged success rate: 0.0243196165677154%
#Same average success rate for supplementary tests: [0.8848450000000001, 0.905745, 0.9482150000000001, 0.984, 0.998445]%
#Sample STD for averaged success rate in supplementary tests: [0.0013296959422176172, 0.0012741172429372447, 0.0010362145958970066, 0.0005418486873657672, 9.567705445921516e-05]%
#Epoch	Loss	Sample STD
1	0.697332	0.002313
2	0.693245	0.000038
3	0.693214	0.000049
4	0.693068	0.000110
5	0.692788	0.000269
6	0.692146	0.000815
7	0.690345	0.002304
8	0.686072	0.006027
9	0.678733	0.012343
10	0.670228	0.019044
11	0.661388	0.024617
12	0.651147	0.028258
13	0.636764	0.031138
14	0.617717	0.035059
15	0.596791	0.039685
16	0.576951	0.042728
17	0.557322	0.043833
18	0.536687	0.043789
19	0.514810	0.043154
20	0.492494	0.042266
21	0.471670	0.041578
22	0.453712	0.041319
23	0.438917	0.041257
24	0.426831	0.041081
25	0.415567	0.040348
26	0.403967	0.039051
27	0.391557	0.037514
28	0.378497	0.036056
29	0.365504	0.034875
30	0.352972	0.033673
31	0.340665	0.032363
32	0.328770	0.030866
33	0.317199	0.029266
34	0.305666	0.027713
35	0.294234	0.026322
36	0.282812	0.025306
37	0.271675	0.024549
38	0.260954	0.024114
39	0.250581	0.023826
40	0.240476	0.023675
41	0.230551	0.023538
42	0.220890	0.023464
43	0.211690	0.023446
44	0.202694	0.023423
45	0.194012	0.023345
46	0.185654	0.023196
47	0.177458	0.022960
48	0.169627	0.022689
49	0.162007	0.022282
50	0.154548	0.021802
51	0.147347	0.021253
52	0.140350	0.020567
53	0.133413	0.019818
54	0.126718	0.019088
55	0.120256	0.018324
56	0.113935	0.017492
57	0.107782	0.016683
58	0.101876	0.015913
59	0.096119	0.015182
60	0.090587	0.014468
61	0.085412	0.013806
62	0.080482	0.013172
63	0.075688	0.012553
64	0.071126	0.011947
65	0.066654	0.011334
66	0.062435	0.010756
67	0.058403	0.010151
68	0.054523	0.009590
69	0.050988	0.009066
70	0.047540	0.008571
71	0.044362	0.008107
72	0.041419	0.007699
73	0.038567	0.007299
74	0.035981	0.006947
75	0.033577	0.006621
76	0.031305	0.006352
77	0.029196	0.006093
78	0.027257	0.005883
79	0.025395	0.005671
80	0.023758	0.005494
81	0.022177	0.005317
82	0.020706	0.005165
83	0.019331	0.005029
84	0.018086	0.004892
85	0.016968	0.004766
86	0.015915	0.004635
87	0.014884	0.004492
88	0.013999	0.004397
89	0.013121	0.004250
90	0.012336	0.004137
91	0.011586	0.003999
92	0.010904	0.003885
93	0.010234	0.003758
94	0.009605	0.003614
95	0.009056	0.003513
96	0.008500	0.003375
97	0.008004	0.003247
98	0.007505	0.003119
99	0.007031	0.002997
100	0.006576	0.002857
