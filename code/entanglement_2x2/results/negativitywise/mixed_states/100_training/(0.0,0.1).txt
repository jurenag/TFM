#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/mixed_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.75%
#Sample standard deviation for averaged success rate: 0.05276717729801153%
#Same average success rate for supplementary tests: [0.9979950000000001, 0.9939300000000001, 0.994925, 0.9966700000000001, 0.999235]%
#Sample STD for averaged success rate in supplementary tests: [0.00022830680837417725, 0.0003902021975847783, 0.00031366736959716863, 0.00020480614004466456, 7.10333636399374e-05]%
#Epoch	Loss	Sample STD
1	0.700231	0.002728
2	0.693358	0.000063
3	0.693249	0.000061
4	0.693175	0.000111
5	0.692965	0.000192
6	0.692598	0.000416
7	0.691982	0.000867
8	0.690799	0.001700
9	0.688827	0.002939
10	0.685908	0.004584
11	0.681243	0.006729
12	0.673843	0.010131
13	0.663549	0.015045
14	0.651501	0.020650
15	0.639198	0.026012
16	0.627588	0.030935
17	0.618382	0.034526
18	0.611026	0.036781
19	0.603377	0.038267
20	0.594207	0.039279
21	0.582372	0.039990
22	0.567559	0.040645
23	0.550282	0.041402
24	0.530773	0.042178
25	0.510592	0.042828
26	0.490231	0.043073
27	0.469926	0.042707
28	0.449118	0.041571
29	0.427601	0.039983
30	0.406057	0.038294
31	0.385284	0.036695
32	0.365273	0.035005
33	0.346000	0.033236
34	0.327401	0.031722
35	0.310344	0.030673
36	0.294658	0.030360
37	0.280633	0.030680
38	0.268099	0.031545
39	0.256672	0.032590
40	0.245981	0.033720
41	0.236035	0.034752
42	0.226375	0.035587
43	0.216794	0.036206
44	0.207240	0.036573
45	0.197545	0.036547
46	0.187822	0.036294
47	0.177907	0.035678
48	0.167674	0.034808
49	0.157431	0.033767
50	0.147249	0.032600
51	0.137454	0.031424
52	0.128139	0.030240
53	0.119486	0.029136
54	0.111250	0.028026
55	0.103603	0.026878
56	0.096455	0.025747
57	0.089596	0.024553
58	0.083326	0.023354
59	0.077360	0.022116
60	0.071709	0.020829
61	0.066443	0.019586
62	0.061498	0.018309
63	0.056839	0.017047
64	0.052362	0.015813
65	0.048231	0.014599
66	0.044387	0.013440
67	0.040725	0.012310
68	0.037489	0.011242
69	0.034446	0.010275
70	0.031676	0.009356
71	0.029149	0.008497
72	0.026792	0.007706
73	0.024655	0.006964
74	0.022773	0.006304
75	0.021023	0.005686
76	0.019529	0.005172
77	0.018120	0.004689
78	0.016910	0.004264
79	0.015859	0.003922
80	0.014851	0.003585
81	0.014072	0.003289
82	0.013261	0.003048
83	0.012619	0.002828
84	0.011991	0.002619
85	0.011525	0.002431
86	0.011020	0.002292
87	0.010567	0.002147
88	0.010169	0.002013
89	0.009876	0.001902
90	0.009495	0.001817
91	0.009219	0.001718
92	0.008968	0.001623
93	0.008754	0.001559
94	0.008514	0.001483
95	0.008273	0.001417
96	0.008076	0.001360
97	0.007875	0.001310
98	0.007716	0.001258
99	0.007597	0.001213
100	0.007386	0.001188
