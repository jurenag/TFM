#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/mixed_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.75250000000001%
#Sample standard deviation for averaged success rate: 0.052234432489494956%
#Same average success rate for supplementary tests: [0.99685, 0.995195, 0.9979400000000002, 0.99944, 0.99975]%
#Sample STD for averaged success rate in supplementary tests: [0.0002816981984322919, 0.0003609994442516421, 0.00020158824370482175, 6.312147019838489e-05, 3.53111172295557e-05]%
#Epoch	Loss	Sample STD
1	0.695693	0.001073
2	0.693340	0.000027
3	0.693273	0.000047
4	0.693163	0.000056
5	0.693074	0.000086
6	0.692914	0.000134
7	0.692591	0.000277
8	0.692111	0.000534
9	0.691016	0.001070
10	0.688992	0.002166
11	0.684943	0.004354
12	0.677408	0.008498
13	0.665855	0.014419
14	0.651526	0.020794
15	0.635393	0.026429
16	0.618924	0.031127
17	0.603721	0.034410
18	0.589475	0.036179
19	0.575335	0.037351
20	0.560534	0.038638
21	0.545417	0.040424
22	0.530692	0.042539
23	0.516629	0.044846
24	0.503273	0.046965
25	0.490156	0.048605
26	0.476351	0.049482
27	0.461346	0.049575
28	0.445500	0.049066
29	0.429921	0.048434
30	0.415724	0.047836
31	0.403102	0.047131
32	0.391702	0.046126
33	0.380619	0.044649
34	0.369159	0.042612
35	0.356855	0.040177
36	0.343784	0.037262
37	0.330216	0.034151
38	0.316602	0.031032
39	0.303111	0.028152
40	0.290129	0.025737
41	0.277943	0.023843
42	0.266332	0.022296
43	0.254672	0.021019
44	0.243333	0.019954
45	0.232002	0.019027
46	0.220826	0.018213
47	0.209799	0.017472
48	0.198832	0.016802
49	0.188165	0.016059
50	0.177442	0.015183
51	0.166725	0.014233
52	0.156094	0.013250
53	0.145553	0.012409
54	0.135365	0.011844
55	0.125477	0.011516
56	0.115908	0.011334
57	0.106829	0.011299
58	0.098227	0.011268
59	0.090073	0.011171
60	0.082342	0.011016
61	0.075305	0.010845
62	0.068785	0.010567
63	0.062875	0.010191
64	0.057547	0.009843
65	0.052782	0.009424
66	0.048461	0.009017
67	0.044648	0.008637
68	0.041169	0.008252
69	0.038007	0.007891
70	0.035208	0.007542
71	0.032623	0.007184
72	0.030344	0.006866
73	0.028255	0.006538
74	0.026402	0.006257
75	0.024703	0.005953
76	0.023162	0.005673
77	0.021721	0.005404
78	0.020462	0.005168
79	0.019264	0.004939
80	0.018163	0.004719
81	0.017201	0.004488
82	0.016289	0.004302
83	0.015447	0.004100
84	0.014741	0.003926
85	0.014042	0.003761
86	0.013333	0.003595
87	0.012802	0.003451
88	0.012260	0.003290
89	0.011721	0.003143
90	0.011229	0.002990
91	0.010788	0.002853
92	0.010385	0.002737
93	0.009956	0.002609
94	0.009611	0.002502
95	0.009264	0.002372
96	0.008904	0.002253
97	0.008580	0.002148
98	0.008309	0.002047
99	0.008027	0.001952
100	0.007718	0.001844
