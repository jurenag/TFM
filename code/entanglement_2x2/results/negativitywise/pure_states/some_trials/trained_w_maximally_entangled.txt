#Tensor product hilbert space dimension: 4; Number of simulations: 5;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.97999999999999%
#Sample standard deviation for averaged success rate: 3.531998372042015e-05%
#Same average success rate for supplementary tests: [0.49978997899789984, 0.5001100110011001, 0.51016101610161, 0.582008200820082, 0.7951595159515952]%
#Sample STD for averaged success rate in supplementary tests: [0.000987327996874988, 0.0009863318564984713, 0.0009542329916498344, 0.0007415006087298479, 0.00026493018799335095]%
#Epoch	Loss	Sample STD
1	0.697905	0.002199
2	0.693311	0.000062
3	0.693198	0.000042
4	0.693024	0.000094
5	0.692878	0.000153
6	0.692633	0.000258
7	0.692171	0.000551
8	0.691041	0.001276
9	0.688867	0.002998
10	0.684321	0.006643
11	0.676418	0.012903
12	0.663998	0.022378
13	0.645560	0.035781
14	0.621839	0.051653
15	0.595889	0.066421
16	0.571414	0.076908
17	0.548933	0.083760
18	0.528996	0.088048
19	0.511209	0.090635
20	0.495116	0.091899
21	0.480281	0.092451
22	0.466481	0.092620
23	0.454678	0.092631
24	0.444126	0.092457
25	0.434512	0.092242
26	0.425514	0.091878
27	0.416300	0.091322
28	0.407002	0.090615
29	0.397314	0.089817
30	0.387254	0.088922
31	0.376777	0.088024
32	0.366044	0.087218
33	0.354876	0.086365
34	0.343258	0.085425
35	0.331385	0.084389
36	0.318926	0.083264
37	0.306209	0.082069
38	0.293100	0.080758
39	0.279690	0.079520
40	0.266083	0.078368
41	0.252736	0.077282
42	0.239369	0.076311
43	0.225676	0.075406
44	0.211581	0.074391
45	0.197097	0.073336
46	0.182396	0.071939
47	0.167684	0.070014
48	0.152767	0.067461
49	0.138151	0.063501
50	0.123329	0.058275
51	0.108870	0.052080
52	0.094581	0.044954
53	0.081310	0.037720
54	0.069467	0.031186
55	0.059042	0.025345
56	0.050128	0.020549
57	0.042304	0.016504
58	0.035690	0.013214
59	0.030171	0.010685
60	0.025458	0.008712
61	0.021532	0.007223
62	0.018344	0.006124
63	0.015671	0.005238
64	0.013426	0.004471
65	0.011688	0.003874
66	0.010120	0.003332
67	0.008892	0.002888
68	0.007782	0.002474
69	0.006833	0.002155
70	0.006047	0.001904
71	0.005387	0.001712
72	0.004817	0.001582
73	0.004346	0.001490
74	0.003936	0.001435
75	0.003547	0.001365
76	0.003264	0.001351
77	0.002984	0.001324
78	0.002759	0.001290
79	0.002579	0.001288
80	0.002438	0.001274
81	0.002261	0.001228
82	0.002165	0.001241
83	0.002058	0.001202
84	0.001956	0.001187
85	0.001885	0.001162
86	0.001800	0.001129
87	0.001726	0.001112
88	0.001704	0.001099
89	0.001609	0.001052
90	0.001562	0.001040
91	0.001560	0.001036
92	0.001500	0.001017
93	0.001462	0.000989
94	0.001390	0.000945
95	0.001401	0.000963
96	0.001347	0.000926
97	0.001311	0.000892
98	0.001307	0.000908
99	0.001267	0.000886
100	0.001221	0.000861
101	0.001217	0.000840
102	0.001156	0.000828
103	0.001160	0.000805
104	0.001186	0.000828
105	0.001137	0.000815
106	0.001102	0.000782
107	0.001096	0.000767
108	0.001078	0.000765
109	0.001048	0.000741
110	0.001028	0.000738
111	0.000998	0.000717
112	0.000984	0.000692
113	0.000943	0.000690
114	0.000961	0.000680
115	0.000971	0.000695
116	0.000965	0.000695
117	0.000927	0.000671
118	0.000877	0.000650
119	0.000897	0.000641
120	0.000888	0.000628
121	0.000862	0.000611
122	0.000857	0.000616
123	0.000832	0.000611
124	0.000836	0.000600
125	0.000843	0.000597
126	0.000785	0.000583
127	0.000778	0.000563
128	0.000767	0.000563
129	0.000754	0.000543
130	0.000726	0.000547
131	0.000759	0.000537
132	0.000702	0.000518
133	0.000696	0.000525
134	0.000692	0.000496
135	0.000695	0.000518
136	0.000666	0.000500
137	0.000683	0.000521
138	0.000613	0.000480
139	0.000659	0.000494
140	0.000629	0.000495
141	0.000681	0.000503
142	0.000635	0.000472
143	0.000610	0.000472
144	0.000604	0.000454
145	0.000627	0.000464
146	0.000621	0.000464
147	0.000607	0.000464
148	0.000599	0.000473
149	0.000567	0.000445
150	0.000615	0.000447
151	0.000572	0.000424
152	0.000547	0.000430
153	0.000546	0.000423
154	0.000602	0.000432
155	0.000549	0.000410
156	0.000526	0.000417
157	0.000511	0.000407
158	0.000558	0.000438
159	0.000508	0.000405
160	0.000523	0.000408
161	0.000482	0.000371
162	0.000493	0.000387
163	0.000483	0.000390
164	0.000498	0.000377
165	0.000478	0.000392
166	0.000478	0.000366
167	0.000435	0.000359
168	0.000453	0.000365
169	0.000431	0.000363
170	0.000470	0.000382
171	0.000429	0.000362
172	0.000435	0.000358
173	0.000430	0.000358
174	0.000425	0.000353
175	0.000412	0.000352
176	0.000427	0.000349
177	0.000412	0.000340
178	0.000440	0.000378
179	0.000478	0.000404
180	0.000399	0.000346
181	0.000420	0.000373
182	0.000438	0.000361
183	0.000413	0.000355
184	0.000369	0.000321
185	0.000416	0.000362
186	0.000379	0.000328
187	0.000423	0.000391
188	0.000416	0.000356
189	0.000401	0.000345
190	0.000398	0.000339
191	0.000373	0.000320
192	0.000416	0.000357
193	0.000380	0.000329
194	0.000386	0.000308
195	0.000395	0.000349
196	0.000390	0.000326
197	0.000342	0.000292
198	0.000360	0.000315
199	0.000338	0.000288
200	0.000331	0.000279
