#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/pure_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 95.8339584896224%
#Sample standard deviation for averaged success rate: 0.1556814893479216%
#Same average success rate for supplementary tests: [0.5033603360336033, 0.545899589958996, 0.7657465746574659, 0.9574657465746576, 0.9713221322132213]%
#Sample STD for averaged success rate in supplementary tests: [0.0032483142808485046, 0.003070404256466466, 0.0021253339438321254, 0.0007139697861706651, 0.0006471412992886821]%
#Epoch	Loss	Sample STD
1	0.696079	0.001084
2	0.693399	0.000034
3	0.693346	0.000032
4	0.693282	0.000030
5	0.693191	0.000069
6	0.693118	0.000129
7	0.692978	0.000234
8	0.692678	0.000453
9	0.692202	0.000834
10	0.691328	0.001530
11	0.690042	0.002590
12	0.688076	0.004174
13	0.685601	0.006104
14	0.682888	0.008089
15	0.680246	0.009786
16	0.677582	0.011042
17	0.674671	0.012013
18	0.671186	0.012934
19	0.666974	0.013908
20	0.662317	0.014897
21	0.657105	0.015764
22	0.651224	0.016433
23	0.644417	0.016955
24	0.636600	0.017458
25	0.627723	0.017966
26	0.617945	0.018555
27	0.607452	0.019209
28	0.597012	0.019875
29	0.586985	0.020589
30	0.577858	0.021205
31	0.569457	0.021759
32	0.561657	0.022217
33	0.554399	0.022581
34	0.547315	0.022813
35	0.540450	0.022981
36	0.533775	0.023052
37	0.527049	0.023026
38	0.520249	0.022978
39	0.513627	0.022981
40	0.507382	0.022997
41	0.501390	0.023052
42	0.495907	0.023093
43	0.490755	0.023122
44	0.485849	0.023068
45	0.481195	0.022932
46	0.476588	0.022718
47	0.472112	0.022365
48	0.467467	0.021896
49	0.462646	0.021096
50	0.457415	0.019927
51	0.451743	0.018362
52	0.445921	0.016705
53	0.440445	0.015328
54	0.435353	0.014308
55	0.430668	0.013622
56	0.426242	0.013255
57	0.421932	0.013110
58	0.417750	0.013081
59	0.413565	0.013184
60	0.409230	0.013366
61	0.404796	0.013655
62	0.400233	0.013997
63	0.395467	0.014426
64	0.390522	0.014896
65	0.385441	0.015406
66	0.380168	0.015951
67	0.374693	0.016484
68	0.369026	0.017045
69	0.363242	0.017641
70	0.357140	0.018168
71	0.350888	0.018746
72	0.344265	0.019298
73	0.337374	0.019878
74	0.330041	0.020486
75	0.322549	0.021185
76	0.314496	0.021977
77	0.306183	0.022849
78	0.297642	0.023784
79	0.288764	0.024838
80	0.279745	0.025946
81	0.270648	0.027054
82	0.261478	0.028173
83	0.252411	0.029214
84	0.243345	0.030172
85	0.234580	0.031040
86	0.225852	0.031791
87	0.217606	0.032455
88	0.209374	0.032958
89	0.201502	0.033413
90	0.193649	0.033712
91	0.186268	0.033950
92	0.178925	0.034086
93	0.171928	0.034117
94	0.164980	0.034060
95	0.158204	0.033958
96	0.151661	0.033748
97	0.145311	0.033517
98	0.139045	0.033259
99	0.132900	0.032990
100	0.127129	0.032770
