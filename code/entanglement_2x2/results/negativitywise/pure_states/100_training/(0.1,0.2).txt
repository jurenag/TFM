#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/pure_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 60.08752188047013%
#Sample standard deviation for averaged success rate: 0.6402024987942434%
#Same average success rate for supplementary tests: [0.5063956395639565, 0.5945544554455446, 0.6553805380538055, 0.687988798879888, 0.6890939093909391]%
#Sample STD for averaged success rate in supplementary tests: [0.0029906385085041523, 0.002878958153155506, 0.002728182298997823, 0.0026400816673484012, 0.0026855112690381566]%
#Epoch	Loss	Sample STD
1	0.695342	0.001035
2	0.693380	0.000033
3	0.693292	0.000031
4	0.693261	0.000034
5	0.693258	0.000021
6	0.693203	0.000023
7	0.693170	0.000022
8	0.693114	0.000032
9	0.693102	0.000026
10	0.693053	0.000035
11	0.693050	0.000027
12	0.693063	0.000035
13	0.693000	0.000043
14	0.692981	0.000034
15	0.692959	0.000046
16	0.692953	0.000066
17	0.692891	0.000070
18	0.692833	0.000082
19	0.692811	0.000106
20	0.692792	0.000125
21	0.692707	0.000127
22	0.692676	0.000179
23	0.692634	0.000211
24	0.692530	0.000247
25	0.692443	0.000302
26	0.692362	0.000345
27	0.692221	0.000402
28	0.692104	0.000491
29	0.691948	0.000548
30	0.691841	0.000636
31	0.691603	0.000744
32	0.691425	0.000826
33	0.691229	0.000939
34	0.690970	0.001058
35	0.690748	0.001175
36	0.690435	0.001299
37	0.690114	0.001389
38	0.689942	0.001507
39	0.689591	0.001612
40	0.689337	0.001717
41	0.688969	0.001823
42	0.688623	0.001923
43	0.688247	0.002041
44	0.687894	0.002107
45	0.687520	0.002196
46	0.687153	0.002298
47	0.686781	0.002381
48	0.686296	0.002483
49	0.685964	0.002579
50	0.685640	0.002650
51	0.685234	0.002728
52	0.684770	0.002821
53	0.684503	0.002871
54	0.684145	0.002934
55	0.683783	0.002987
56	0.683390	0.003026
57	0.683076	0.003083
58	0.682710	0.003108
59	0.682396	0.003114
60	0.681977	0.003141
61	0.681647	0.003159
62	0.681329	0.003163
63	0.680866	0.003183
64	0.680461	0.003177
65	0.680118	0.003175
66	0.679670	0.003181
67	0.679210	0.003182
68	0.678745	0.003163
69	0.678263	0.003160
70	0.677786	0.003157
71	0.677263	0.003149
72	0.676844	0.003137
73	0.676343	0.003128
74	0.675815	0.003107
75	0.675289	0.003078
76	0.674780	0.003041
77	0.674307	0.002999
78	0.673763	0.002938
79	0.673303	0.002911
80	0.672780	0.002849
81	0.672320	0.002793
82	0.671784	0.002727
83	0.671277	0.002679
84	0.670880	0.002630
85	0.670414	0.002561
86	0.669988	0.002529
87	0.669569	0.002481
88	0.669207	0.002415
89	0.668810	0.002373
90	0.668450	0.002307
91	0.668019	0.002268
92	0.667741	0.002229
93	0.667377	0.002180
94	0.666997	0.002172
95	0.666757	0.002094
96	0.666393	0.002075
97	0.666083	0.002048
98	0.665762	0.002028
99	0.665491	0.001993
100	0.665113	0.001948
