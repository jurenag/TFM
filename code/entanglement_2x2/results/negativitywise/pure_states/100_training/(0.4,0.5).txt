#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/pure_states/negativity_(0.4, 0.5).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.37484371092773%
#Sample standard deviation for averaged success rate: 0.04447817847404345%
#Same average success rate for supplementary tests: [0.5017701770177018, 0.5107510751075107, 0.5817481748174818, 0.8864286428642864, 0.9937843784378438]%
#Sample STD for averaged success rate in supplementary tests: [0.0034780903727319942, 0.003428698349818018, 0.0030751807202579226, 0.0015135719313176638, 0.00019570680991635832]%
#Epoch	Loss	Sample STD
1	0.696223	0.001014
2	0.693388	0.000024
3	0.693288	0.000031
4	0.693209	0.000042
5	0.693089	0.000062
6	0.692923	0.000115
7	0.692685	0.000209
8	0.692210	0.000445
9	0.691201	0.000948
10	0.689251	0.002031
11	0.685227	0.004226
12	0.678284	0.008036
13	0.668120	0.013380
14	0.655722	0.019083
15	0.642354	0.024031
16	0.628727	0.027562
17	0.614628	0.029606
18	0.598921	0.030990
19	0.582067	0.032329
20	0.564545	0.033494
21	0.547381	0.034400
22	0.530507	0.034900
23	0.514356	0.035119
24	0.498957	0.035246
25	0.484736	0.035266
26	0.471824	0.035148
27	0.460355	0.034849
28	0.449857	0.034384
29	0.440269	0.033929
30	0.431399	0.033520
31	0.422706	0.033110
32	0.413723	0.032592
33	0.404305	0.031950
34	0.393978	0.031165
35	0.382956	0.030302
36	0.371601	0.029475
37	0.359803	0.028755
38	0.348073	0.028153
39	0.336677	0.027797
40	0.325708	0.027621
41	0.315254	0.027549
42	0.305375	0.027589
43	0.295723	0.027651
44	0.286365	0.027745
45	0.277308	0.027776
46	0.268423	0.027829
47	0.259520	0.027832
48	0.250566	0.027837
49	0.241563	0.027786
50	0.232399	0.027783
51	0.223111	0.027807
52	0.213899	0.027873
53	0.204872	0.028032
54	0.195920	0.028268
55	0.187030	0.028587
56	0.178418	0.028940
57	0.170329	0.029340
58	0.162412	0.029758
59	0.154786	0.030167
60	0.147576	0.030555
61	0.140615	0.030909
62	0.134137	0.031228
63	0.127922	0.031496
64	0.122099	0.031722
65	0.116545	0.031872
66	0.111323	0.031960
67	0.106387	0.032005
68	0.101694	0.032021
69	0.097254	0.031992
70	0.093009	0.031885
71	0.088929	0.031725
72	0.085075	0.031542
73	0.081358	0.031300
74	0.077668	0.030986
75	0.074231	0.030629
76	0.070935	0.030222
77	0.067665	0.029739
78	0.064562	0.029177
79	0.061539	0.028523
80	0.058485	0.027807
81	0.055608	0.026981
82	0.052689	0.026020
83	0.049880	0.025022
84	0.047032	0.023907
85	0.044182	0.022669
86	0.041406	0.021424
87	0.038765	0.020222
88	0.036285	0.019057
89	0.033946	0.017961
90	0.031840	0.017000
91	0.029885	0.016155
92	0.028158	0.015441
93	0.026608	0.014798
94	0.025223	0.014266
95	0.023914	0.013794
96	0.022729	0.013364
97	0.021627	0.012950
98	0.020569	0.012559
99	0.019562	0.012185
100	0.018628	0.011839
