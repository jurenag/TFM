#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 74.01350337584397%
#Sample standard deviation for averaged success rate: 0.5222825642464739%
#Same average success rate for supplementary tests: [0.4824782478247825, 0.5987248724872487, 0.7408440844084409, 0.8088808880888089, 0.8245924592459246]%
#Sample STD for averaged success rate in supplementary tests: [0.0030238021644225714, 0.002731535446079153, 0.0023376353934775923, 0.002192149160349642, 0.002208515118895005]%
#Epoch	Loss	Sample STD
1	0.696979	0.001566
2	0.693468	0.000035
3	0.693385	0.000021
4	0.693387	0.000037
5	0.693353	0.000038
6	0.693282	0.000034
7	0.693289	0.000024
8	0.693275	0.000043
9	0.693242	0.000039
10	0.693199	0.000054
11	0.693099	0.000067
12	0.693039	0.000104
13	0.692990	0.000135
14	0.692857	0.000205
15	0.692695	0.000336
16	0.692450	0.000493
17	0.692133	0.000737
18	0.691643	0.001041
19	0.691085	0.001430
20	0.690304	0.001916
21	0.689479	0.002442
22	0.688440	0.002993
23	0.687371	0.003512
24	0.686168	0.003993
25	0.684766	0.004374
26	0.683137	0.004730
27	0.681275	0.005074
28	0.679019	0.005444
29	0.676479	0.005890
30	0.673721	0.006405
31	0.670681	0.007021
32	0.667704	0.007675
33	0.664673	0.008273
34	0.661948	0.008835
35	0.659256	0.009244
36	0.656634	0.009567
37	0.654064	0.009806
38	0.651481	0.009947
39	0.648841	0.010064
40	0.646002	0.010142
41	0.643142	0.010167
42	0.640316	0.010250
43	0.637539	0.010314
44	0.634795	0.010447
45	0.632226	0.010597
46	0.629774	0.010745
47	0.627420	0.010924
48	0.625142	0.011076
49	0.622799	0.011243
50	0.620536	0.011370
51	0.618236	0.011495
52	0.615969	0.011594
53	0.613787	0.011628
54	0.611418	0.011585
55	0.609076	0.011485
56	0.606674	0.011293
57	0.604097	0.010952
58	0.601449	0.010584
59	0.598651	0.010072
60	0.595868	0.009531
61	0.592974	0.008925
62	0.590113	0.008318
63	0.587281	0.007691
64	0.584582	0.007121
65	0.582086	0.006630
66	0.579768	0.006242
67	0.577649	0.005923
68	0.575626	0.005665
69	0.573812	0.005488
70	0.572144	0.005375
71	0.570524	0.005273
72	0.569027	0.005204
73	0.567545	0.005170
74	0.566180	0.005176
75	0.564897	0.005141
76	0.563585	0.005141
77	0.562361	0.005161
78	0.561139	0.005172
79	0.559923	0.005172
80	0.558651	0.005164
81	0.557569	0.005207
82	0.556235	0.005223
83	0.555070	0.005215
84	0.553753	0.005243
85	0.552459	0.005262
86	0.551114	0.005297
87	0.549820	0.005333
88	0.548395	0.005378
89	0.547099	0.005439
90	0.545732	0.005522
91	0.544351	0.005608
92	0.542859	0.005693
93	0.541480	0.005798
94	0.540079	0.005905
95	0.538682	0.006018
96	0.537196	0.006133
97	0.535771	0.006254
98	0.534345	0.006394
99	0.532891	0.006526
100	0.531482	0.006642
