#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2_merge/input_data/generated/pure_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 49.52988247061766%
#Sample standard deviation for averaged success rate: 0.5690181295616427%
#Same average success rate for supplementary tests: [0.5106810681068107, 0.5104910491049105, 0.49916991699169905, 0.485983598359836, 0.463891389138914]%
#Sample STD for averaged success rate in supplementary tests: [0.0025341331084879825, 0.002562735043122604, 0.0025582365803399837, 0.0025529603331558293, 0.0025418757035012766]%
#Epoch	Loss	Sample STD
1	0.696422	0.001426
2	0.693423	0.000023
3	0.693311	0.000025
4	0.693328	0.000020
5	0.693233	0.000022
6	0.693230	0.000022
7	0.693188	0.000025
8	0.693194	0.000023
9	0.693107	0.000040
10	0.693142	0.000016
11	0.693151	0.000028
12	0.693134	0.000022
13	0.693098	0.000015
14	0.693053	0.000028
15	0.693071	0.000032
16	0.693073	0.000034
17	0.693080	0.000016
18	0.693033	0.000024
19	0.693057	0.000011
20	0.693036	0.000015
21	0.693113	0.000016
22	0.693056	0.000025
23	0.693030	0.000017
24	0.693053	0.000024
25	0.692999	0.000020
26	0.693056	0.000014
27	0.693001	0.000026
28	0.693005	0.000021
29	0.693038	0.000007
30	0.693036	0.000024
31	0.693002	0.000016
32	0.692994	0.000028
33	0.693027	0.000014
34	0.693015	0.000018
35	0.693001	0.000028
36	0.693016	0.000022
37	0.692981	0.000026
38	0.693023	0.000028
39	0.693049	0.000019
40	0.692995	0.000033
41	0.693004	0.000020
42	0.692972	0.000022
43	0.692999	0.000022
44	0.693004	0.000028
45	0.692991	0.000032
46	0.693036	0.000017
47	0.693042	0.000014
48	0.692971	0.000019
49	0.693015	0.000020
50	0.692977	0.000024
51	0.692974	0.000033
52	0.692956	0.000025
53	0.693008	0.000021
54	0.692978	0.000025
55	0.692980	0.000023
56	0.692992	0.000043
57	0.693002	0.000026
58	0.692974	0.000029
59	0.692945	0.000039
60	0.692954	0.000021
61	0.692927	0.000038
62	0.692968	0.000027
63	0.692998	0.000038
64	0.692935	0.000039
65	0.692932	0.000026
66	0.692896	0.000044
67	0.692954	0.000036
68	0.692925	0.000036
69	0.692898	0.000051
70	0.692890	0.000056
71	0.692944	0.000054
72	0.692914	0.000044
73	0.692959	0.000061
74	0.692903	0.000051
75	0.692901	0.000055
76	0.692928	0.000079
77	0.692883	0.000074
78	0.692925	0.000061
79	0.692883	0.000063
80	0.692883	0.000073
81	0.692881	0.000070
82	0.692852	0.000092
83	0.692850	0.000084
84	0.692796	0.000145
85	0.692787	0.000091
86	0.692824	0.000094
87	0.692824	0.000096
88	0.692805	0.000106
89	0.692750	0.000131
90	0.692766	0.000116
91	0.692773	0.000128
92	0.692766	0.000133
93	0.692752	0.000129
94	0.692727	0.000141
95	0.692685	0.000152
96	0.692703	0.000150
97	0.692682	0.000150
98	0.692638	0.000158
99	0.692628	0.000183
100	0.692623	0.000176
