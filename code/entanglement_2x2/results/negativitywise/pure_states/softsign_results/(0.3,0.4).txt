#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 80; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: softsign; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.98999749937484%
#Sample standard deviation for averaged success rate: 0.004998748436354564%
#Same average success rate for supplementary tests: [0.5002100210021002, 0.5406590659065906, 0.8754375437543754, 0.9999699969997, 1.0]%
#Sample STD for averaged success rate in supplementary tests: [0.0035346918506617858, 0.0033064122015840853, 0.0015697070029485104, 1.2246835958557749e-05, 0.0]%
#Epoch	Loss	Sample STD
1	0.688888	0.000495
2	0.671760	0.001520
3	0.642587	0.002821
4	0.601632	0.003859
5	0.554941	0.004643
6	0.509525	0.005318
7	0.468501	0.005614
8	0.432702	0.005555
9	0.401867	0.005420
10	0.374499	0.005635
11	0.349417	0.006344
12	0.325325	0.007726
13	0.302007	0.009275
14	0.278417	0.010734
15	0.254671	0.012037
16	0.230618	0.013085
17	0.206859	0.014006
18	0.183315	0.014574
19	0.160594	0.014914
20	0.138931	0.014953
21	0.118726	0.014638
22	0.100343	0.013920
23	0.084197	0.012947
24	0.069851	0.011678
25	0.057783	0.010387
26	0.047673	0.009125
27	0.039148	0.007840
28	0.032148	0.006682
29	0.026446	0.005661
30	0.021832	0.004765
31	0.018009	0.003993
32	0.014968	0.003358
33	0.012489	0.002818
34	0.010473	0.002366
35	0.008867	0.002017
36	0.007587	0.001718
37	0.006470	0.001467
38	0.005553	0.001249
39	0.004800	0.001079
40	0.004158	0.000929
41	0.003676	0.000814
42	0.003206	0.000709
43	0.002881	0.000637
44	0.002510	0.000552
45	0.002284	0.000492
46	0.002055	0.000437
47	0.001800	0.000398
48	0.001659	0.000353
49	0.001536	0.000309
50	0.001327	0.000281
51	0.001208	0.000246
52	0.001139	0.000234
53	0.001084	0.000218
54	0.000942	0.000195
55	0.000878	0.000183
56	0.000843	0.000179
57	0.000751	0.000167
58	0.000697	0.000139
59	0.000650	0.000138
60	0.000629	0.000135
61	0.000591	0.000134
62	0.000559	0.000116
63	0.000508	0.000105
64	0.000485	0.000093
65	0.000424	0.000085
66	0.000421	0.000092
67	0.000439	0.000086
68	0.000403	0.000084
69	0.000393	0.000092
70	0.000342	0.000077
71	0.000336	0.000073
72	0.000328	0.000078
73	0.000301	0.000067
74	0.000290	0.000075
75	0.000286	0.000063
76	0.000266	0.000070
77	0.000255	0.000069
78	0.000249	0.000051
79	0.000262	0.000070
80	0.000212	0.000045
