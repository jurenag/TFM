#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: softsign; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.32483120780195%
#Sample standard deviation for averaged success rate: 0.045684884701807205%
#Same average success rate for supplementary tests: [0.5164716471647164, 0.7704670467046705, 0.9965996599659966, 0.9990999099909991, 0.9991099109910991]%
#Sample STD for averaged success rate in supplementary tests: [0.003423781525933645, 0.0021853946409395443, 0.00014009235229445224, 6.715949660386865e-05, 6.67928383222423e-05]%
#Epoch	Loss	Sample STD
1	0.692489	0.000380
2	0.686050	0.000847
3	0.677219	0.001437
4	0.664912	0.001960
5	0.649981	0.002422
6	0.633674	0.002919
7	0.617475	0.003384
8	0.602132	0.003817
9	0.588348	0.004178
10	0.575673	0.004332
11	0.564139	0.004320
12	0.553462	0.004197
13	0.543821	0.004023
14	0.534261	0.003821
15	0.525454	0.003562
16	0.516565	0.003387
17	0.507900	0.003334
18	0.499218	0.003493
19	0.490412	0.003852
20	0.481540	0.004407
21	0.472320	0.005143
22	0.462915	0.005859
23	0.452940	0.006754
24	0.442353	0.007696
25	0.431368	0.008708
26	0.419779	0.009635
27	0.407914	0.010660
28	0.395709	0.011722
29	0.383211	0.012643
30	0.370471	0.013621
31	0.357568	0.014501
32	0.344967	0.015359
33	0.331794	0.016095
34	0.319136	0.016674
35	0.306130	0.017197
36	0.293339	0.017550
37	0.280617	0.017875
38	0.268321	0.018023
39	0.256109	0.018078
40	0.244204	0.018045
41	0.232755	0.017943
42	0.221476	0.017837
43	0.210557	0.017543
44	0.200243	0.017339
45	0.190177	0.016956
46	0.180805	0.016654
47	0.171844	0.016200
48	0.163361	0.015833
49	0.155305	0.015414
50	0.147935	0.014929
51	0.140623	0.014503
52	0.134108	0.013969
53	0.127424	0.013484
54	0.121570	0.013096
55	0.115920	0.012569
56	0.110739	0.012056
57	0.105731	0.011633
58	0.100812	0.011152
59	0.096557	0.010716
60	0.092386	0.010239
61	0.088612	0.009737
62	0.084809	0.009420
63	0.081376	0.009019
64	0.078488	0.008593
65	0.074920	0.008292
66	0.072307	0.007898
67	0.069674	0.007548
68	0.066944	0.007246
69	0.064598	0.006948
70	0.062585	0.006642
71	0.060488	0.006366
72	0.058166	0.006143
73	0.056399	0.005899
74	0.054549	0.005658
75	0.052930	0.005453
76	0.051338	0.005262
77	0.049807	0.005078
78	0.048496	0.004843
79	0.047112	0.004756
80	0.045940	0.004616
81	0.044577	0.004483
82	0.043423	0.004291
83	0.042520	0.004142
84	0.041404	0.004038
85	0.040278	0.003933
86	0.039636	0.003879
87	0.038620	0.003733
88	0.037685	0.003669
89	0.036861	0.003640
90	0.036099	0.003446
91	0.035290	0.003358
92	0.034669	0.003353
93	0.033682	0.003355
94	0.033210	0.003268
95	0.032424	0.003171
96	0.032009	0.003070
97	0.031350	0.003006
98	0.030886	0.002969
99	0.030253	0.002930
100	0.029728	0.002902
101	0.029247	0.002860
102	0.028681	0.002829
103	0.028132	0.002718
104	0.027846	0.002685
105	0.027328	0.002671
106	0.027064	0.002629
107	0.026565	0.002649
108	0.026187	0.002578
109	0.025992	0.002557
110	0.025275	0.002543
111	0.025187	0.002422
112	0.024562	0.002434
113	0.024402	0.002320
114	0.024071	0.002376
115	0.023829	0.002356
116	0.023414	0.002329
117	0.023229	0.002304
118	0.022841	0.002249
119	0.022690	0.002259
120	0.022508	0.002187
121	0.022172	0.002142
122	0.021776	0.002136
123	0.021458	0.002188
124	0.021472	0.002100
125	0.021315	0.002100
126	0.020996	0.002102
127	0.020612	0.001989
128	0.020456	0.002043
129	0.020229	0.002009
130	0.019983	0.001908
131	0.019929	0.001983
132	0.019559	0.001996
133	0.019384	0.001963
134	0.019240	0.001961
135	0.019021	0.001894
136	0.018894	0.001851
137	0.018726	0.001825
138	0.018437	0.001825
139	0.018274	0.001841
140	0.018331	0.001797
141	0.018065	0.001760
142	0.017605	0.001747
143	0.017759	0.001727
144	0.017607	0.001727
145	0.017596	0.001683
146	0.017240	0.001678
147	0.016991	0.001630
148	0.016961	0.001648
149	0.016893	0.001595
150	0.016821	0.001563
151	0.016715	0.001611
152	0.016357	0.001542
153	0.016354	0.001556
154	0.016152	0.001501
155	0.016064	0.001516
156	0.015974	0.001514
157	0.015893	0.001493
158	0.015740	0.001502
159	0.015798	0.001484
160	0.015463	0.001462
161	0.015432	0.001445
162	0.015220	0.001450
163	0.015291	0.001416
164	0.015072	0.001335
165	0.014963	0.001399
166	0.014849	0.001401
167	0.014860	0.001365
168	0.014774	0.001371
169	0.014646	0.001305
170	0.014519	0.001380
171	0.014205	0.001307
172	0.014419	0.001358
173	0.014209	0.001302
174	0.014161	0.001307
175	0.014057	0.001233
176	0.014074	0.001250
177	0.014026	0.001248
178	0.013969	0.001260
179	0.013870	0.001271
180	0.013598	0.001251
181	0.013465	0.001226
182	0.013705	0.001205
183	0.013596	0.001189
184	0.013376	0.001211
185	0.013404	0.001104
186	0.013092	0.001186
187	0.013100	0.001145
188	0.013401	0.001149
189	0.012943	0.001138
190	0.013105	0.001143
191	0.012912	0.001099
192	0.012801	0.001158
193	0.012501	0.001154
194	0.012610	0.001118
195	0.012500	0.001139
196	0.012531	0.001102
197	0.012358	0.001112
198	0.012531	0.001033
199	0.012357	0.000969
200	0.012120	0.001077
