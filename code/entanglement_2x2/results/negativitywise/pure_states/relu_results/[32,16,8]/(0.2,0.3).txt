#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7425%
#Sample standard deviation for averaged success rate: 0.02602255632139613%
#Same average success rate for supplementary tests: [0.5226149999999999, 0.8194449999999999, 0.9992, 0.9995299999999999, 0.9995300000000001, 0.9922549999999999, 0.99851, 0.99889, 0.99889, 0.99889]%
#Sample STD for averaged success rate in supplementary tests: [0.0033928016577380415, 0.0019473365396741303, 6.45600495663099e-05, 4.836274392555265e-05, 4.836274392532309e-05, 0.00027609010983736527, 9.348259196233321e-05, 7.508924689991288e-05, 7.508924689998681e-05, 7.508924689998681e-05]%
#Epoch	Loss	Sample STD
1	0.663355	0.002823
2	0.571327	0.006810
3	0.483747	0.011380
4	0.384113	0.019006
5	0.267708	0.024284
6	0.171882	0.021799
7	0.111148	0.016371
8	0.076576	0.011581
9	0.055783	0.008125
10	0.042694	0.005960
11	0.033704	0.004569
12	0.027588	0.003621
13	0.023173	0.003081
14	0.019823	0.002506
15	0.017137	0.002236
16	0.014926	0.001904
17	0.013404	0.001801
18	0.011651	0.001558
19	0.010313	0.001390
20	0.009591	0.001279
21	0.008930	0.001288
22	0.007884	0.001171
23	0.007091	0.000975
24	0.006776	0.000968
25	0.006092	0.000893
26	0.005830	0.000877
27	0.005331	0.000884
28	0.005015	0.000839
29	0.004762	0.000759
30	0.004560	0.000781
31	0.004063	0.000678
32	0.004257	0.000765
33	0.003643	0.000579
34	0.003445	0.000632
35	0.003140	0.000541
36	0.003212	0.000519
37	0.003137	0.000608
38	0.002723	0.000525
39	0.002513	0.000522
40	0.002657	0.000451
41	0.002388	0.000399
42	0.002553	0.000488
43	0.002501	0.000490
44	0.002236	0.000450
45	0.002103	0.000324
46	0.002144	0.000473
47	0.001900	0.000370
48	0.001792	0.000320
49	0.002063	0.000424
50	0.001886	0.000362
51	0.001895	0.000429
52	0.001598	0.000254
53	0.001677	0.000376
54	0.001600	0.000289
55	0.001464	0.000295
56	0.001412	0.000301
57	0.001556	0.000348
58	0.001410	0.000242
59	0.001389	0.000284
60	0.001358	0.000323
61	0.001385	0.000324
62	0.001279	0.000250
63	0.001354	0.000281
64	0.001171	0.000314
65	0.001323	0.000275
66	0.001089	0.000227
67	0.001068	0.000180
68	0.001054	0.000269
69	0.001170	0.000264
70	0.000879	0.000177
71	0.001242	0.000285
72	0.001147	0.000264
73	0.000964	0.000241
74	0.000986	0.000250
75	0.001015	0.000232
76	0.000928	0.000131
77	0.000908	0.000216
78	0.000948	0.000189
79	0.000886	0.000157
80	0.000849	0.000127
81	0.000955	0.000236
82	0.000715	0.000194
83	0.000805	0.000131
84	0.000762	0.000207
85	0.000815	0.000148
86	0.000850	0.000210
87	0.000768	0.000178
88	0.000713	0.000128
89	0.000716	0.000191
90	0.000743	0.000212
91	0.000977	0.000230
92	0.000816	0.000221
93	0.000717	0.000192
94	0.000841	0.000224
95	0.000823	0.000240
96	0.000804	0.000216
97	0.000617	0.000172
98	0.000575	0.000129
99	0.000718	0.000205
100	0.000686	0.000206
