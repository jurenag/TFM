#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 90; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.95%
#Sample standard deviation for averaged success rate: 0.011152354011580753%
#Same average success rate for supplementary tests: [0.50092, 0.55073, 0.8624649999999999, 0.999885, 0.9999950000000001, 0.9555450000000001, 0.9889600000000001, 0.9991700000000001, 0.9999, 0.9999049999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.003529618630957174, 0.003240443388642979, 0.0016026169812996514, 2.396536563467071e-05, 4.999874998221446e-06, 0.0006869225929826397, 0.0003891091363615063, 8.896940485350927e-05, 2.2349496638644085e-05, 2.1784139873009754e-05]%
#Epoch	Loss	Sample STD
1	0.664174	0.001980
2	0.566924	0.004070
3	0.450731	0.009726
4	0.366422	0.013510
5	0.301656	0.015991
6	0.245497	0.018335
7	0.196861	0.019353
8	0.155100	0.018828
9	0.120831	0.017085
10	0.094083	0.014857
11	0.073435	0.012351
12	0.057461	0.009802
13	0.045369	0.007623
14	0.036171	0.005838
15	0.029158	0.004445
16	0.023967	0.003504
17	0.019964	0.002780
18	0.016786	0.002266
19	0.014414	0.001933
20	0.012394	0.001663
21	0.010845	0.001422
22	0.009474	0.001253
23	0.008381	0.001122
24	0.007568	0.001033
25	0.006771	0.000973
26	0.006109	0.000883
27	0.005546	0.000836
28	0.004961	0.000757
29	0.004631	0.000732
30	0.004204	0.000678
31	0.003875	0.000640
32	0.003553	0.000619
33	0.003300	0.000583
34	0.003017	0.000559
35	0.002827	0.000536
36	0.002617	0.000501
37	0.002422	0.000484
38	0.002266	0.000467
39	0.002166	0.000453
40	0.001971	0.000421
41	0.001890	0.000415
42	0.001783	0.000392
43	0.001602	0.000353
44	0.001538	0.000359
45	0.001432	0.000339
46	0.001441	0.000363
47	0.001301	0.000316
48	0.001224	0.000315
49	0.001194	0.000301
50	0.001091	0.000282
51	0.001043	0.000275
52	0.000982	0.000280
53	0.000895	0.000247
54	0.000874	0.000246
55	0.000822	0.000236
56	0.000782	0.000230
57	0.000760	0.000225
58	0.000728	0.000225
59	0.000663	0.000208
60	0.000618	0.000197
61	0.000580	0.000178
62	0.000565	0.000182
63	0.000530	0.000158
64	0.000501	0.000164
65	0.000464	0.000144
66	0.000468	0.000156
67	0.000423	0.000142
68	0.000401	0.000140
69	0.000383	0.000135
70	0.000367	0.000131
71	0.000354	0.000135
72	0.000328	0.000113
73	0.000308	0.000110
74	0.000282	0.000103
75	0.000279	0.000102
76	0.000275	0.000102
77	0.000260	0.000102
78	0.000238	0.000096
79	0.000220	0.000086
80	0.000236	0.000096
81	0.000218	0.000093
82	0.000221	0.000096
83	0.000192	0.000088
84	0.000180	0.000078
85	0.000173	0.000076
86	0.000189	0.000090
87	0.000167	0.000078
88	0.000161	0.000072
89	0.000151	0.000070
90	0.000153	0.000071
