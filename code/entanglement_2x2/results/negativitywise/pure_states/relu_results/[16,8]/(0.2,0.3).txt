#Tensor product hilbert space dimension: 4; Number of simulations: 1;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.75%
#Sample standard deviation for averaged success rate: 0.17566836653193985%
#Same average success rate for supplementary tests: [0.5202, 0.75765, 0.9932, 0.9987, 0.9987, 0.9916, 0.99655, 0.99695, 0.997, 0.997]%
#Sample STD for averaged success rate in supplementary tests: [0.003532647449151981, 0.003029987438092772, 0.0005811092840421694, 0.000254785203652009, 0.000254785203652009, 0.0006453464186001155, 0.0004146141278345387, 0.0003899164974709343, 0.00038671695075338797, 0.00038671695075338797]%
#Epoch	Loss	Sample STD
1	0.674418	0.000000
2	0.634966	0.000000
3	0.596143	0.000000
4	0.570135	0.000000
5	0.550111	0.000000
6	0.531289	0.000000
7	0.513879	0.000000
8	0.493945	0.000000
9	0.472046	0.000000
10	0.446729	0.000000
11	0.420764	0.000000
12	0.394903	0.000000
13	0.369059	0.000000
14	0.341886	0.000000
15	0.313702	0.000000
16	0.289778	0.000000
17	0.267007	0.000000
18	0.249053	0.000000
19	0.232366	0.000000
20	0.218381	0.000000
21	0.204625	0.000000
22	0.192523	0.000000
23	0.182075	0.000000
24	0.172109	0.000000
25	0.161821	0.000000
26	0.153404	0.000000
27	0.145572	0.000000
28	0.139000	0.000000
29	0.132640	0.000000
30	0.126744	0.000000
31	0.120639	0.000000
32	0.116331	0.000000
33	0.111892	0.000000
34	0.106666	0.000000
35	0.103338	0.000000
36	0.100128	0.000000
37	0.096574	0.000000
38	0.093362	0.000000
39	0.090478	0.000000
40	0.088585	0.000000
41	0.085378	0.000000
42	0.083016	0.000000
43	0.080385	0.000000
44	0.078757	0.000000
45	0.076790	0.000000
46	0.075117	0.000000
47	0.073919	0.000000
48	0.071946	0.000000
49	0.070042	0.000000
50	0.069031	0.000000
51	0.068344	0.000000
52	0.066060	0.000000
53	0.065148	0.000000
54	0.064264	0.000000
55	0.062402	0.000000
56	0.061226	0.000000
57	0.060078	0.000000
58	0.059070	0.000000
59	0.058003	0.000000
60	0.056633	0.000000
61	0.055615	0.000000
62	0.054909	0.000000
63	0.053921	0.000000
64	0.053636	0.000000
65	0.052294	0.000000
66	0.051824	0.000000
67	0.051522	0.000000
68	0.051695	0.000000
69	0.050056	0.000000
70	0.049928	0.000000
71	0.049051	0.000000
72	0.048530	0.000000
73	0.047474	0.000000
74	0.047327	0.000000
75	0.046769	0.000000
76	0.046243	0.000000
77	0.045819	0.000000
78	0.044505	0.000000
79	0.044708	0.000000
80	0.044454	0.000000
81	0.044189	0.000000
82	0.043527	0.000000
83	0.043227	0.000000
84	0.042143	0.000000
85	0.041800	0.000000
86	0.042331	0.000000
87	0.042113	0.000000
88	0.041271	0.000000
89	0.041171	0.000000
90	0.040582	0.000000
91	0.039902	0.000000
92	0.040143	0.000000
93	0.039619	0.000000
94	0.039488	0.000000
95	0.038404	0.000000
96	0.039113	0.000000
97	0.038203	0.000000
98	0.037565	0.000000
99	0.037650	0.000000
100	0.036893	0.000000
101	0.037053	0.000000
102	0.036842	0.000000
103	0.036194	0.000000
104	0.036278	0.000000
105	0.035409	0.000000
106	0.035964	0.000000
107	0.035683	0.000000
108	0.035217	0.000000
109	0.035662	0.000000
110	0.034977	0.000000
111	0.035093	0.000000
112	0.033758	0.000000
113	0.034067	0.000000
114	0.033524	0.000000
115	0.033129	0.000000
116	0.033338	0.000000
117	0.032946	0.000000
118	0.033095	0.000000
119	0.033114	0.000000
120	0.031991	0.000000
121	0.033075	0.000000
122	0.032071	0.000000
123	0.031834	0.000000
124	0.030719	0.000000
125	0.031609	0.000000
126	0.031068	0.000000
127	0.030697	0.000000
128	0.030325	0.000000
129	0.030669	0.000000
130	0.030704	0.000000
131	0.030578	0.000000
132	0.030526	0.000000
133	0.030105	0.000000
134	0.030269	0.000000
135	0.029461	0.000000
136	0.029975	0.000000
137	0.029051	0.000000
138	0.029923	0.000000
139	0.028562	0.000000
140	0.029217	0.000000
141	0.028801	0.000000
142	0.028701	0.000000
143	0.029079	0.000000
144	0.028854	0.000000
145	0.027964	0.000000
146	0.028444	0.000000
147	0.028362	0.000000
148	0.027508	0.000000
149	0.027717	0.000000
150	0.028045	0.000000
151	0.027194	0.000000
152	0.027326	0.000000
153	0.027586	0.000000
154	0.027084	0.000000
155	0.026577	0.000000
156	0.026811	0.000000
157	0.026355	0.000000
158	0.026516	0.000000
159	0.026277	0.000000
160	0.026227	0.000000
161	0.025365	0.000000
162	0.025666	0.000000
163	0.026014	0.000000
164	0.025456	0.000000
165	0.025069	0.000000
166	0.024560	0.000000
167	0.025313	0.000000
168	0.025099	0.000000
169	0.025224	0.000000
170	0.024537	0.000000
171	0.024905	0.000000
172	0.023594	0.000000
173	0.024291	0.000000
174	0.024341	0.000000
175	0.024510	0.000000
176	0.024145	0.000000
177	0.023837	0.000000
178	0.024448	0.000000
179	0.024156	0.000000
180	0.023759	0.000000
181	0.023526	0.000000
182	0.024024	0.000000
183	0.023248	0.000000
184	0.023390	0.000000
185	0.023219	0.000000
186	0.023036	0.000000
187	0.022353	0.000000
188	0.022657	0.000000
189	0.022880	0.000000
190	0.022733	0.000000
191	0.022136	0.000000
192	0.022452	0.000000
193	0.021806	0.000000
194	0.022545	0.000000
195	0.021955	0.000000
196	0.021663	0.000000
197	0.020902	0.000000
198	0.021510	0.000000
199	0.020783	0.000000
200	0.021083	0.000000
