#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; entangled (n\in(0.1,0.2) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [64, 32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.83500000000001%
#Sample standard deviation for averaged success rate: 0.06872549563298923%
#Same average success rate for supplementary tests: [0.70675, 0.9955950000000001, 0.99821, 0.99821, 0.99821, 0.9836050000000001, 0.9965350000000001, 0.9965350000000001, 0.9965350000000001, 0.9965349999999998]%
#Sample STD for averaged success rate in supplementary tests: [0.002616624137127839, 0.00017479930992426367, 9.919574083598318e-05, 9.919574083598318e-05, 9.919574083598318e-05, 0.00038429831999372204, 0.00013592162723423794, 0.0001359216272341971, 0.0001359216272341971, 0.0001359216272343196]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.679880	0.000784	0.666922	0.001352
2	0.652960	0.001953	0.646647	0.003698
3	0.623337	0.006275	0.607824	0.010271
4	0.560486	0.015729	0.529031	0.020029
5	0.456996	0.023627	0.412098	0.023494
6	0.343030	0.022906	0.307454	0.017595
7	0.256136	0.018328	0.266902	0.015969
8	0.202256	0.014930	0.187071	0.010415
9	0.166024	0.013042	0.170608	0.011767
10	0.141159	0.011854	0.159722	0.020302
11	0.120900	0.010815	0.136004	0.010362
12	0.106604	0.010348	0.137232	0.015590
13	0.094965	0.009162	0.118583	0.011362
14	0.085622	0.008414	0.099596	0.014716
15	0.076806	0.007868	0.093749	0.013912
16	0.069581	0.006982	0.089015	0.009536
17	0.064885	0.007155	0.090715	0.009420
18	0.059657	0.006663	0.101471	0.012936
19	0.055332	0.005829	0.081766	0.006858
20	0.052265	0.005338	0.082032	0.008511
21	0.048264	0.005223	0.099528	0.026054
22	0.045991	0.005206	0.091297	0.011314
23	0.043247	0.004665	0.060309	0.004945
24	0.040369	0.004357	0.084159	0.009438
25	0.038118	0.004214	0.072194	0.009258
26	0.035900	0.004001	0.055704	0.005765
27	0.035086	0.004056	0.083456	0.017357
28	0.033194	0.003580	0.058286	0.004156
29	0.031930	0.003557	0.066197	0.003806
30	0.029357	0.003220	0.056701	0.004819
31	0.029528	0.003321	0.067187	0.006864
32	0.029285	0.003591	0.051472	0.002456
33	0.027874	0.003002	0.061742	0.008776
34	0.027438	0.003253	0.064239	0.008123
35	0.025187	0.002744	0.059255	0.004411
36	0.026108	0.003368	0.046406	0.002821
37	0.030149	0.002362	0.095726	0.034925
38	0.027560	0.001279	0.069147	0.006737
39	0.027866	0.002172	0.048687	0.005577
40	0.026980	0.001859	0.079293	0.014240
41	0.026371	0.001416	0.062051	0.007448
42	0.025189	0.001792	0.063870	0.006786
43	0.023245	0.001787	0.076167	0.010679
44	0.024494	0.001569	0.077103	0.019242
45	0.025914	0.001698	0.054148	0.005175
46	0.024999	0.001398	0.065389	0.006193
47	0.023020	0.001387	0.059995	0.014447
48	0.024565	0.002186	0.054864	0.005632
49	0.023949	0.001263	0.042535	0.005028
50	0.023575	0.000736	0.114814	0.039454
51	0.022497	0.001334	0.047038	0.002301
52	0.022433	0.000070	0.113971	0.041105
53	0.021064	0.001502	0.052549	0.001566
54	0.021735	0.000270	0.051972	0.000125
55	0.020599	0.001050	0.057227	0.007963
56	0.018750	0.000000	0.041902	0.000000
57	0.019926	0.000000	0.040571	0.000000
58	0.016453	0.000000	0.084008	0.000000
59	0.019389	0.000000	0.038202	0.000000
