#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.0,0.1)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [128, 64, 32, 16, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 94.22500000000001%
#Sample standard deviation for averaged success rate: 0.2459925684853094%
#Same average success rate for supplementary tests: [0.970095, 0.9929300000000001, 0.99321, 0.99321, 0.99321, 0.98357, 0.9856, 0.9856, 0.9856, 0.9856]%
#Sample STD for averaged success rate in supplementary tests: [0.0007629610401258984, 0.00022022886958797473, 0.00021689812124587365, 0.00021689812124587365, 0.00021689812124587365, 0.00034842898128599985, 0.00033716761410312337, 0.00033716761410312337, 0.00033716761410313985, 0.00033716761410313985]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.692651	0.000077	0.691321	0.000257
2	0.689796	0.000213	0.688361	0.000216
3	0.686592	0.000297	0.685256	0.000415
4	0.683527	0.000319	0.684461	0.000672
5	0.680718	0.000373	0.683585	0.001069
6	0.678050	0.000381	0.681568	0.000920
7	0.675628	0.000508	0.681244	0.001235
8	0.672508	0.000757	0.679306	0.001054
9	0.668685	0.001297	0.678216	0.001682
10	0.663258	0.002077	0.670712	0.002557
11	0.655807	0.003499	0.664481	0.005260
12	0.644898	0.005541	0.655903	0.007215
13	0.629932	0.008597	0.636831	0.010117
14	0.611470	0.012145	0.620650	0.015072
15	0.588497	0.016157	0.597446	0.018655
16	0.562154	0.020528	0.576842	0.021603
17	0.533759	0.024182	0.546017	0.027144
18	0.501783	0.027039	0.518288	0.030660
19	0.468993	0.029021	0.481462	0.031022
20	0.437723	0.029578	0.460762	0.027974
21	0.407313	0.029103	0.423267	0.028025
22	0.378885	0.028810	0.400409	0.029120
23	0.355103	0.027851	0.385610	0.025836
24	0.334609	0.026598	0.374471	0.028014
25	0.315688	0.025644	0.372452	0.029216
26	0.299873	0.023941	0.344555	0.027426
27	0.283091	0.022290	0.329016	0.023829
28	0.269367	0.020435	0.305640	0.021413
29	0.258814	0.019750	0.291655	0.017601
30	0.246075	0.017909	0.300164	0.020270
31	0.235268	0.016705	0.291061	0.019588
32	0.226378	0.015820	0.280890	0.017346
33	0.216560	0.015042	0.286521	0.016877
34	0.208657	0.014341	0.266281	0.013996
35	0.203006	0.013477	0.253659	0.012358
36	0.194988	0.013139	0.262080	0.017445
37	0.188196	0.012321	0.258565	0.018673
38	0.182021	0.012114	0.270533	0.023825
39	0.176328	0.011149	0.240185	0.010036
40	0.172371	0.010852	0.234975	0.010679
41	0.166769	0.010373	0.241604	0.008448
42	0.162200	0.009868	0.241641	0.013369
43	0.158260	0.009693	0.231151	0.012303
44	0.153243	0.009255	0.229774	0.010464
45	0.149064	0.008727	0.250944	0.022274
46	0.146080	0.008600	0.230498	0.011416
47	0.142014	0.008545	0.232663	0.009538
48	0.139800	0.008580	0.221798	0.005385
49	0.135577	0.007825	0.223898	0.009407
50	0.133398	0.007898	0.222041	0.011767
51	0.129518	0.007563	0.227110	0.005961
52	0.127064	0.006978	0.236331	0.015743
53	0.124783	0.006968	0.222135	0.008033
54	0.121848	0.006996	0.225366	0.006025
55	0.119121	0.006457	0.231861	0.008919
56	0.116518	0.006168	0.215744	0.007368
57	0.115008	0.006593	0.213277	0.006842
58	0.112820	0.006297	0.209163	0.006590
59	0.109916	0.005757	0.224734	0.012836
60	0.108703	0.005843	0.216772	0.007273
61	0.106867	0.005700	0.200471	0.005737
62	0.104231	0.005564	0.212031	0.006279
63	0.103571	0.005640	0.209627	0.008435
64	0.103104	0.005533	0.220577	0.007006
65	0.101389	0.005410	0.223096	0.007375
66	0.101133	0.005339	0.216087	0.009246
67	0.099685	0.005401	0.218659	0.009854
68	0.096334	0.004719	0.225035	0.007566
69	0.095397	0.005015	0.208475	0.004756
70	0.095820	0.004644	0.213730	0.006842
71	0.092698	0.004977	0.219766	0.008993
72	0.091273	0.004827	0.226260	0.011080
73	0.091753	0.004483	0.210113	0.007001
74	0.088270	0.004118	0.220451	0.008873
75	0.087592	0.004420	0.208284	0.006039
76	0.086055	0.004313	0.215675	0.010688
77	0.085481	0.004441	0.223044	0.007863
78	0.084308	0.004032	0.219841	0.004631
79	0.083069	0.004417	0.220138	0.006204
80	0.080795	0.003925	0.221092	0.005271
81	0.081692	0.004002	0.219017	0.008757
82	0.081244	0.003612	0.217996	0.006494
83	0.077954	0.003681	0.214988	0.008454
84	0.078004	0.003962	0.214535	0.007182
85	0.077478	0.004318	0.218296	0.009244
86	0.077108	0.004197	0.226170	0.013210
87	0.074926	0.003716	0.219555	0.007386
88	0.074584	0.003594	0.213520	0.005584
89	0.074749	0.003883	0.221976	0.008723
90	0.075162	0.003907	0.219224	0.005681
91	0.073876	0.004479	0.221009	0.012642
92	0.075475	0.003657	0.219605	0.012277
93	0.074296	0.003604	0.218856	0.012508
94	0.073580	0.003232	0.230220	0.009439
95	0.073913	0.003691	0.218479	0.009412
96	0.071960	0.003568	0.219305	0.010211
97	0.075012	0.003620	0.238429	0.017716
98	0.075167	0.003040	0.219268	0.009918
99	0.073865	0.003188	0.246619	0.004905
100	0.071358	0.003356	0.230443	0.005458
101	0.076043	0.002162	0.221798	0.006709
102	0.074639	0.003783	0.242249	0.015885
103	0.072747	0.002025	0.216980	0.004297
104	0.072327	0.002261	0.236586	0.009579
105	0.070703	0.001667	0.248721	0.032055
106	0.071660	0.001782	0.267298	0.001499
107	0.071436	0.001369	0.217797	0.004852
