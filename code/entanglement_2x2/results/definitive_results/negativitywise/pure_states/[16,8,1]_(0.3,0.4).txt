#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.3,0.4)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.88749999999999%
#Sample standard deviation for averaged success rate: 0.01811314267321291%
#Same average success rate for supplementary tests: [0.50148, 0.56174, 0.87528, 0.999655, 0.9999099999999999, 0.9482950000000001, 0.986885, 0.9987950000000001, 0.9996900000000002, 0.99969]%
#Sample STD for averaged success rate in supplementary tests: [0.0035250376565364515, 0.0031807559824670617, 0.0014766841503855868, 4.380694864984105e-05, 2.1203655345415935e-05, 0.0007358020445405092, 0.0004088701979235049, 9.72234475319242e-05, 3.930896844214589e-05, 3.9308968442428326e-05]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.660816	0.002664	0.617719	0.005329
2	0.555518	0.007585	0.489820	0.009268
3	0.438421	0.009216	0.388654	0.009413
4	0.358437	0.008988	0.320046	0.009442
5	0.295753	0.009751	0.263724	0.010168
6	0.238504	0.011240	0.210240	0.011503
7	0.185420	0.012284	0.162926	0.012062
8	0.139781	0.012412	0.121636	0.011489
9	0.104361	0.011707	0.091919	0.010708
10	0.078454	0.010443	0.070114	0.009597
11	0.060086	0.008879	0.054641	0.008355
12	0.046934	0.007312	0.044542	0.007149
13	0.037361	0.006010	0.036330	0.005811
14	0.030231	0.004936	0.030370	0.004950
15	0.025080	0.004140	0.025646	0.004167
16	0.021019	0.003535	0.022086	0.003686
17	0.017828	0.003020	0.019371	0.003293
18	0.015425	0.002673	0.017279	0.002897
19	0.013341	0.002373	0.015447	0.002644
20	0.011720	0.002108	0.014002	0.002384
21	0.010366	0.001897	0.012858	0.002400
22	0.009307	0.001746	0.011819	0.002001
23	0.008359	0.001599	0.010906	0.001790
24	0.007517	0.001463	0.010064	0.001857
25	0.006810	0.001357	0.010140	0.001880
26	0.006227	0.001259	0.009058	0.001583
27	0.005719	0.001171	0.008283	0.001627
28	0.005268	0.001115	0.008543	0.001533
29	0.004866	0.001029	0.007464	0.001336
30	0.004503	0.000969	0.007524	0.001306
31	0.004105	0.000892	0.007186	0.001357
32	0.003899	0.000875	0.007098	0.001427
33	0.003586	0.000828	0.006398	0.001144
34	0.003386	0.000787	0.006205	0.001249
35	0.003117	0.000746	0.006039	0.001119
36	0.002933	0.000708	0.005473	0.000983
37	0.002724	0.000665	0.005538	0.001091
38	0.002572	0.000627	0.005509	0.001062
39	0.002398	0.000617	0.005443	0.001163
40	0.002227	0.000563	0.005395	0.001112
41	0.002284	0.000553	0.005532	0.000945
42	0.002154	0.000541	0.005145	0.000982
43	0.002220	0.000532	0.005488	0.001075
44	0.002028	0.000568	0.004411	0.001003
45	0.002045	0.000605	0.004881	0.000792
46	0.002016	0.000645	0.004833	0.001127
47	0.001872	0.000806	0.004818	0.001669
48	0.001728	0.000731	0.004589	0.001358
49	0.001681	0.000713	0.004408	0.001291
50	0.001550	0.000651	0.003970	0.001331
51	0.001464	0.000611	0.004304	0.001559
52	0.001404	0.000590	0.003696	0.001310
53	0.001485	0.000725	0.004591	0.001215
54	0.001444	0.000684	0.004869	0.001921
55	0.001363	0.000627	0.003807	0.001454
56	0.001209	0.000602	0.004016	0.001765
57	0.001619	0.000717	0.004539	0.001787
58	0.001511	0.000623	0.004050	0.001820
59	0.001410	0.000620	0.004907	0.002247
60	0.001298	0.000552	0.003898	0.001975
61	0.001267	0.000560	0.004218	0.001943
62	0.001195	0.000517	0.003726	0.001574
63	0.001134	0.000500	0.003717	0.001664
64	0.001008	0.000461	0.003396	0.001755
65	0.001607	0.000000	0.006383	0.000000
66	0.001519	0.000000	0.006804	0.000000
67	0.001437	0.000000	0.006462	0.000000
68	0.001365	0.000000	0.005327	0.000000
69	0.001254	0.000000	0.005221	0.000000
70	0.001124	0.000000	0.004826	0.000000
71	0.001205	0.000000	0.004886	0.000000
72	0.001084	0.000000	0.004193	0.000000
73	0.001009	0.000000	0.004902	0.000000
74	0.000976	0.000000	0.004270	0.000000
75	0.000968	0.000000	0.004650	0.000000
76	0.000904	0.000000	0.004321	0.000000
77	0.000849	0.000000	0.004069	0.000000
78	0.000829	0.000000	0.003849	0.000000
79	0.000707	0.000000	0.003307	0.000000
80	0.000708	0.000000	0.003512	0.000000
81	0.000739	0.000000	0.003360	0.000000
82	0.000634	0.000000	0.003644	0.000000
83	0.000617	0.000000	0.004220	0.000000
84	0.000621	0.000000	0.003493	0.000000
85	0.000557	0.000000	0.003174	0.000000
86	0.000566	0.000000	0.003253	0.000000
87	0.000515	0.000000	0.003560	0.000000
88	0.000495	0.000000	0.002899	0.000000
89	0.000472	0.000000	0.003261	0.000000
