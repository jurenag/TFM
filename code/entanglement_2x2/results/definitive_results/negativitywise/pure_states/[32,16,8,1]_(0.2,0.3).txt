#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.2,0.3)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7825%
#Sample standard deviation for averaged success rate: 0.024635816152510457%
#Same average success rate for supplementary tests: [0.519025, 0.8077049999999999, 0.999445, 0.999835, 0.999835, 0.9758550000000001, 0.9981849999999999, 0.99935, 0.9993650000000002, 0.999365]%
#Sample STD for averaged success rate in supplementary tests: [0.0034179712504276575, 0.002020001645729532, 5.531363981875792e-05, 2.8699107128918408e-05, 2.8699107128918408e-05, 0.0004860822448413356, 0.00011471830172217198, 5.682319068840556e-05, 5.616795127108821e-05, 5.616795127123646e-05]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.663369	0.001679	0.621936	0.003460
2	0.570433	0.004219	0.532177	0.005710
3	0.484777	0.008217	0.443571	0.010669
4	0.385472	0.014957	0.335764	0.017303
5	0.277111	0.019781	0.231389	0.020293
6	0.186205	0.018959	0.154479	0.016002
7	0.121143	0.014253	0.103278	0.011333
8	0.079962	0.009254	0.070388	0.007455
9	0.056052	0.006417	0.054788	0.005606
10	0.041914	0.005019	0.046499	0.007100
11	0.032620	0.004084	0.034313	0.004296
12	0.026292	0.003453	0.029815	0.004428
13	0.021679	0.002831	0.024701	0.002389
14	0.018084	0.002491	0.023361	0.002479
15	0.015425	0.002232	0.020425	0.002090
16	0.013130	0.001930	0.019751	0.002432
17	0.011343	0.001656	0.022061	0.003500
18	0.010278	0.001487	0.018260	0.003355
19	0.008909	0.001385	0.015236	0.002135
20	0.007690	0.001109	0.016438	0.002401
21	0.007267	0.001218	0.012192	0.001473
22	0.006636	0.001084	0.012582	0.001091
23	0.005890	0.000970	0.016759	0.002947
24	0.005538	0.000962	0.010640	0.001247
25	0.004831	0.000772	0.015512	0.002351
26	0.004394	0.000794	0.010078	0.001263
27	0.004237	0.000761	0.016950	0.004565
28	0.003798	0.000644	0.011428	0.001865
29	0.003554	0.000592	0.009921	0.001365
30	0.003433	0.000653	0.008875	0.001161
31	0.003528	0.000603	0.014005	0.003491
32	0.003281	0.000544	0.011948	0.001679
33	0.003117	0.000580	0.012438	0.002556
34	0.003279	0.000550	0.011870	0.002087
35	0.002621	0.000445	0.010574	0.001906
36	0.002868	0.000505	0.010513	0.000984
37	0.002196	0.000283	0.011187	0.001795
38	0.001936	0.000313	0.016348	0.006718
39	0.002135	0.000322	0.008426	0.000753
40	0.001634	0.000214	0.008423	0.001253
41	0.001767	0.000235	0.010007	0.002852
42	0.002393	0.000101	0.010229	0.000737
43	0.001986	0.000139	0.008814	0.000092
44	0.002035	0.000041	0.015132	0.000010
45	0.001994	0.000075	0.006951	0.001122
