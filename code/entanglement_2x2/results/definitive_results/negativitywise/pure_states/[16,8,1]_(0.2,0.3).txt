#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.2,0.3)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.0425%
#Sample standard deviation for averaged success rate: 0.05398655793343477%
#Same average success rate for supplementary tests: [0.529615, 0.795345, 0.9939600000000001, 0.99816, 0.99816, 0.9213300000000001, 0.9895400000000002, 0.996925, 0.9969599999999998, 0.9969599999999998]%
#Sample STD for averaged success rate in supplementary tests: [0.003333979992254003, 0.0019498696235261488, 0.0001859460136706398, 0.00010015348221601045, 0.00010015348221601045, 0.0009533108386040694, 0.0002661379717364375, 0.00012846096196903, 0.00012781987325928708, 0.00012781987325928708]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.683689	0.001313	0.668752	0.002014
2	0.649288	0.002510	0.625509	0.002661
3	0.601633	0.001771	0.576036	0.002915
4	0.556973	0.004135	0.537370	0.006089
5	0.522602	0.006997	0.505489	0.008097
6	0.492309	0.008812	0.476716	0.009463
7	0.462794	0.010003	0.447310	0.010848
8	0.432130	0.011097	0.418255	0.011752
9	0.400141	0.012143	0.385038	0.012791
10	0.366605	0.013260	0.352732	0.014132
11	0.333491	0.014330	0.320914	0.015373
12	0.301902	0.015052	0.289864	0.015646
13	0.272193	0.015543	0.262163	0.015758
14	0.245527	0.015694	0.236987	0.016157
15	0.221742	0.015545	0.214655	0.015633
16	0.200670	0.015217	0.195951	0.015262
17	0.182504	0.014667	0.177295	0.014793
18	0.166393	0.014073	0.164225	0.013711
19	0.152675	0.013416	0.149656	0.013093
20	0.140825	0.012813	0.139209	0.012610
21	0.130418	0.012261	0.129676	0.011785
22	0.121341	0.011682	0.122283	0.011909
23	0.113405	0.011251	0.113539	0.010758
24	0.106211	0.010847	0.106073	0.010024
25	0.099931	0.010554	0.101035	0.009598
26	0.094299	0.010141	0.095459	0.009574
27	0.089371	0.009946	0.090618	0.008890
28	0.084616	0.009685	0.087760	0.008816
29	0.080642	0.009547	0.084091	0.008855
30	0.076963	0.009267	0.080147	0.009003
31	0.073343	0.009118	0.077314	0.008580
32	0.070488	0.008907	0.073726	0.008043
33	0.067722	0.008728	0.071196	0.008027
34	0.065140	0.008481	0.069690	0.007595
35	0.062715	0.008367	0.066050	0.007603
36	0.060416	0.008222	0.064682	0.007459
37	0.058333	0.007988	0.062746	0.007894
38	0.056377	0.007949	0.062088	0.007623
39	0.054509	0.007749	0.060071	0.007433
40	0.052910	0.007711	0.058007	0.006954
41	0.051515	0.007553	0.055440	0.006916
42	0.049907	0.007426	0.056862	0.007955
43	0.048770	0.007417	0.054182	0.006976
44	0.047350	0.007284	0.052510	0.006703
45	0.046223	0.007146	0.051589	0.007124
46	0.045056	0.007065	0.050247	0.006729
47	0.044046	0.006987	0.050416	0.006533
48	0.042944	0.006894	0.048476	0.006566
49	0.042096	0.006778	0.047812	0.006750
50	0.041088	0.006737	0.046989	0.006545
51	0.040408	0.006649	0.046492	0.005929
52	0.039409	0.006593	0.046457	0.006655
53	0.038669	0.006482	0.044594	0.006921
54	0.037854	0.006416	0.043967	0.006075
55	0.037222	0.006385	0.045338	0.006577
56	0.036222	0.006291	0.043104	0.006306
57	0.035695	0.006242	0.041512	0.006068
58	0.035003	0.006190	0.041311	0.006345
59	0.034145	0.006118	0.041892	0.006121
60	0.033761	0.006036	0.042544	0.006085
61	0.033109	0.005988	0.039182	0.006270
62	0.032653	0.005905	0.038438	0.005899
63	0.031831	0.005756	0.037549	0.005766
64	0.031428	0.005728	0.037407	0.005809
65	0.030884	0.005675	0.038266	0.006219
66	0.030313	0.005607	0.037154	0.005772
67	0.029809	0.005602	0.037132	0.005624
68	0.029346	0.005485	0.035983	0.005457
69	0.028693	0.005401	0.036378	0.005492
70	0.028351	0.005340	0.035150	0.005360
71	0.027880	0.005294	0.034994	0.005387
72	0.027450	0.005205	0.035466	0.005276
73	0.026940	0.005163	0.034976	0.005824
74	0.026480	0.005106	0.034099	0.005294
75	0.025996	0.005625	0.032896	0.005555
76	0.025525	0.005504	0.032336	0.005810
77	0.025336	0.005409	0.033255	0.005765
78	0.024822	0.005361	0.032371	0.005987
79	0.024373	0.005343	0.032329	0.005824
80	0.023939	0.005318	0.032900	0.005398
81	0.023746	0.005216	0.031713	0.005611
82	0.023449	0.005217	0.031374	0.005509
83	0.023024	0.005148	0.032581	0.005617
84	0.022921	0.005744	0.033494	0.008196
85	0.024843	0.006075	0.034033	0.006296
86	0.024426	0.005975	0.032652	0.005947
87	0.021107	0.006052	0.032814	0.006998
88	0.020660	0.005984	0.031496	0.007148
89	0.020522	0.006010	0.030106	0.006294
90	0.020345	0.005980	0.030100	0.007043
91	0.020038	0.005863	0.030227	0.007160
92	0.019782	0.005843	0.029460	0.006257
93	0.022480	0.006002	0.032382	0.006378
94	0.022251	0.005902	0.034262	0.006430
95	0.021771	0.005822	0.032513	0.006631
96	0.021991	0.005875	0.033056	0.007182
97	0.023891	0.006912	0.033407	0.007507
98	0.023286	0.006962	0.035542	0.007324
99	0.023475	0.006965	0.033961	0.008050
100	0.023060	0.006748	0.032275	0.007778
