#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.4,0.5)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.4, 0.5).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.9975%
#Sample standard deviation for averaged success rate: 0.0024996874803931207%
#Same average success rate for supplementary tests: [0.500055, 0.50288, 0.58699, 0.9197050000000001, 0.9999950000000001, 0.90812, 0.9535850000000002, 0.9847799999999999, 0.9986450000000001, 1.0]%
#Sample STD for averaged success rate in supplementary tests: [0.0035351838493563525, 0.0035177528736396475, 0.003100336593823322, 0.0012329479505437328, 4.999874997111195e-06, 0.0013314665898925145, 0.0010406764092406311, 0.0005960852120292942, 0.00014083039000863971, 0.0]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.622678	0.003882	0.524240	0.007735
2	0.391951	0.009466	0.268668	0.010632
3	0.193674	0.010182	0.138617	0.009683
4	0.104138	0.008595	0.077361	0.007459
5	0.057153	0.006031	0.043256	0.005029
6	0.031463	0.003681	0.023890	0.002823
7	0.017403	0.002139	0.013401	0.001711
8	0.009602	0.001284	0.007600	0.001070
9	0.005238	0.000750	0.004490	0.000665
10	0.002951	0.000471	0.002588	0.000442
11	0.001711	0.000309	0.001674	0.000305
12	0.001043	0.000210	0.001095	0.000233
13	0.000640	0.000136	0.000667	0.000148
14	0.000413	0.000092	0.000543	0.000145
15	0.000277	0.000067	0.000333	0.000078
16	0.000181	0.000046	0.000247	0.000056
17	0.000127	0.000034	0.000172	0.000039
18	0.000088	0.000023	0.000131	0.000037
19	0.000060	0.000016	0.000093	0.000021
20	0.000041	0.000013	0.000117	0.000048
21	0.000033	0.000010	0.000060	0.000019
22	0.000023	0.000007	0.000051	0.000013
23	0.000020	0.000006	0.000041	0.000014
24	0.000018	0.000005	0.000037	0.000009
25	0.000017	0.000006	0.000039	0.000017
26	0.000013	0.000006	0.000041	0.000019
27	0.000010	0.000004	0.000028	0.000016
28	0.000003	0.000000	0.000008	0.000000
29	0.000002	0.000000	0.000005	0.000000
30	0.000001	0.000000	0.000004	0.000000
