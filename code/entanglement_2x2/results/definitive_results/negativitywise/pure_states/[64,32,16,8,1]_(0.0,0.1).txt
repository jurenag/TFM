#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.0,0.1)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [64, 32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 87.17500000000001%
#Sample standard deviation for averaged success rate: 0.3127176741887152%
#Same average success rate for supplementary tests: [0.908475, 0.971835, 0.976975, 0.9776100000000001, 0.9778100000000001, 0.9220150000000001, 0.961385, 0.96321, 0.9635100000000001, 0.9636]%
#Sample STD for averaged success rate in supplementary tests: [0.0011805438233077183, 0.00044778525963904407, 0.0004222173240761322, 0.0004200409444327914, 0.00041938072797876626, 0.0007241491481386892, 0.0005799302447277537, 0.0005775160560538563, 0.0005771689484024546, 0.0005770632547650204]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.692661	0.000079	0.691885	0.000157
2	0.690646	0.000168	0.690420	0.000246
3	0.688184	0.000236	0.688990	0.000325
4	0.685797	0.000313	0.687731	0.000428
5	0.683584	0.000454	0.686514	0.000467
6	0.681370	0.000485	0.685926	0.000760
7	0.679279	0.000606	0.683911	0.000751
8	0.677537	0.000685	0.683352	0.000664
9	0.675801	0.000714	0.682670	0.001073
10	0.673487	0.001007	0.681272	0.001054
11	0.671370	0.001238	0.681222	0.001030
12	0.669188	0.001509	0.678457	0.001999
13	0.666029	0.002036	0.676658	0.002524
14	0.662693	0.002715	0.674613	0.002300
15	0.658506	0.003691	0.670191	0.004224
16	0.653879	0.004663	0.665586	0.005673
17	0.648154	0.005929	0.664905	0.005963
18	0.641901	0.007329	0.655957	0.008233
19	0.635188	0.008752	0.652124	0.010334
20	0.626821	0.010413	0.644592	0.010161
21	0.618088	0.012040	0.635238	0.013481
22	0.608066	0.013639	0.625839	0.015194
23	0.597948	0.015461	0.617477	0.014872
24	0.586357	0.017329	0.610305	0.017222
25	0.575384	0.018683	0.595092	0.017783
26	0.563546	0.020180	0.588578	0.019982
27	0.552185	0.021275	0.578222	0.021885
28	0.540321	0.022367	0.566889	0.023737
29	0.528582	0.023487	0.566030	0.019511
30	0.517208	0.024254	0.548340	0.023061
31	0.505509	0.024933	0.532244	0.024686
32	0.494531	0.025384	0.528339	0.024018
33	0.483267	0.026073	0.516314	0.026220
34	0.473014	0.026466	0.512725	0.025855
35	0.463519	0.026695	0.497169	0.027797
36	0.452916	0.027019	0.487623	0.026983
37	0.443021	0.027489	0.485864	0.026876
38	0.434204	0.027452	0.471635	0.027949
39	0.424571	0.027615	0.465483	0.028357
40	0.417045	0.027713	0.461470	0.027362
41	0.408269	0.028002	0.457318	0.027464
42	0.400321	0.028000	0.458236	0.024646
43	0.393416	0.027596	0.443275	0.031527
44	0.386159	0.027507	0.440295	0.029497
45	0.379483	0.027443	0.444487	0.025616
46	0.372928	0.027220	0.433041	0.025906
47	0.367014	0.026977	0.423081	0.027112
48	0.359111	0.026724	0.431777	0.023159
49	0.353848	0.026644	0.418656	0.025380
50	0.347959	0.026079	0.409262	0.025131
51	0.342026	0.025794	0.410954	0.027848
52	0.336346	0.025344	0.403565	0.026302
53	0.330666	0.025149	0.395776	0.024940
54	0.326494	0.024575	0.394729	0.026842
55	0.320892	0.024410	0.396760	0.024152
56	0.317491	0.023973	0.402998	0.022603
57	0.313890	0.023528	0.386565	0.021930
58	0.308007	0.023192	0.383462	0.021709
59	0.304165	0.022844	0.401324	0.026005
60	0.300772	0.022521	0.365475	0.021237
61	0.295430	0.022096	0.373380	0.024123
62	0.291161	0.021890	0.367955	0.022792
63	0.288109	0.021007	0.371510	0.020281
64	0.284199	0.020814	0.376192	0.021593
65	0.281228	0.020522	0.376182	0.026857
66	0.279012	0.020161	0.368854	0.022526
67	0.273405	0.020039	0.375674	0.020170
68	0.272283	0.019311	0.364217	0.019797
69	0.268612	0.019083	0.356638	0.018468
70	0.265379	0.018709	0.374678	0.022827
71	0.261010	0.020840	0.367670	0.023399
72	0.259244	0.019913	0.350422	0.019688
73	0.262530	0.021281	0.366678	0.019548
74	0.260028	0.020557	0.355688	0.017708
75	0.257489	0.020309	0.361878	0.018006
76	0.264091	0.021065	0.351729	0.019815
77	0.279530	0.023791	0.371739	0.025329
78	0.276569	0.023542	0.372044	0.019835
79	0.273202	0.023049	0.387768	0.025132
80	0.270714	0.022602	0.367700	0.022518
81	0.268907	0.022538	0.367469	0.023762
82	0.265801	0.022138	0.368692	0.021622
83	0.263207	0.021688	0.373801	0.019851
84	0.261485	0.021740	0.372272	0.033042
85	0.258202	0.021447	0.376887	0.037147
86	0.256415	0.020975	0.358712	0.023130
87	0.253840	0.020974	0.369079	0.017362
88	0.253959	0.020658	0.351813	0.022595
89	0.250267	0.020749	0.368928	0.018344
90	0.248351	0.020153	0.360594	0.021506
91	0.245434	0.019806	0.355794	0.017894
92	0.243567	0.019314	0.355335	0.024001
93	0.241289	0.019842	0.347164	0.019710
94	0.239769	0.019140	0.361441	0.017363
95	0.250050	0.020811	0.358006	0.021696
96	0.248155	0.019848	0.359279	0.022003
97	0.243801	0.019981	0.354302	0.027184
98	0.241967	0.019188	0.372825	0.021909
99	0.253962	0.020813	0.381021	0.013788
100	0.254235	0.019657	0.367261	0.022531
