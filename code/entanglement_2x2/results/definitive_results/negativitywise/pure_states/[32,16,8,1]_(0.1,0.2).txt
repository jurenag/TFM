#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled (n\in(0.1,0.2)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/pure_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 96.78000000000002%
#Sample standard deviation for averaged success rate: 0.11333754894120364%
#Same average success rate for supplementary tests: [0.693115, 0.9809899999999999, 0.9928549999999999, 0.992925, 0.992925, 0.97677, 0.9888100000000001, 0.9890100000000001, 0.9890100000000001, 0.9890100000000002]%
#Sample STD for averaged success rate in supplementary tests: [0.002520120004831119, 0.00036909483198767286, 0.0002075390294619519, 0.00020681445488650806, 0.00020681445488650806, 0.00040400291459344686, 0.0002656862717567226, 0.0002642177037974499, 0.0002642177037974499, 0.00026421770379742894]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.685859	0.000819	0.679989	0.001319
2	0.668573	0.001906	0.666019	0.001839
3	0.652742	0.002717	0.654085	0.003109
4	0.637719	0.004507	0.638978	0.005918
5	0.619952	0.006950	0.621326	0.007785
6	0.596192	0.009452	0.593761	0.010587
7	0.565609	0.011889	0.560111	0.013362
8	0.527929	0.013797	0.519533	0.014518
9	0.485243	0.015333	0.478964	0.016522
10	0.441934	0.016320	0.431855	0.016045
11	0.399221	0.016412	0.394640	0.014941
12	0.361419	0.015941	0.358604	0.015239
13	0.327647	0.015431	0.327460	0.013923
14	0.298854	0.014699	0.305468	0.014545
15	0.274141	0.013895	0.275363	0.013265
16	0.252919	0.013108	0.259578	0.014285
17	0.235079	0.012576	0.244176	0.013064
18	0.218837	0.011984	0.234180	0.012873
19	0.204531	0.011400	0.214968	0.012711
20	0.192809	0.011152	0.207849	0.011334
21	0.181604	0.011014	0.201106	0.012591
22	0.172659	0.010723	0.191798	0.014000
23	0.163986	0.010477	0.183265	0.011106
24	0.156570	0.010426	0.187435	0.014708
25	0.150231	0.010297	0.174596	0.013158
26	0.143460	0.009975	0.171270	0.010567
27	0.138455	0.009935	0.157975	0.010150
28	0.132686	0.009956	0.164130	0.009083
29	0.128433	0.009920	0.152154	0.008960
30	0.123980	0.009680	0.151490	0.011856
31	0.119761	0.009601	0.147489	0.011645
32	0.115910	0.009339	0.148908	0.012905
33	0.112746	0.009126	0.146075	0.011445
34	0.109604	0.009234	0.140877	0.012293
35	0.106422	0.008893	0.141371	0.013580
36	0.103445	0.008821	0.130153	0.011157
37	0.100942	0.008863	0.132818	0.009679
38	0.098103	0.008405	0.125903	0.008369
39	0.096039	0.008272	0.130410	0.012675
40	0.093470	0.008251	0.120005	0.009687
41	0.091022	0.008139	0.126853	0.010129
42	0.089127	0.007947	0.133678	0.014129
43	0.088089	0.007966	0.128321	0.007862
44	0.086020	0.007764	0.122200	0.008679
45	0.084003	0.007674	0.113060	0.006200
46	0.082912	0.007797	0.120186	0.009438
47	0.080397	0.007348	0.117165	0.007841
48	0.079261	0.007430	0.124939	0.014384
49	0.077950	0.007235	0.114631	0.007733
50	0.077104	0.007243	0.129704	0.011150
51	0.075229	0.007145	0.115292	0.007175
52	0.074413	0.007090	0.110257	0.008476
53	0.073739	0.007156	0.114195	0.010023
54	0.071899	0.006904	0.113377	0.008599
55	0.070670	0.006579	0.110467	0.010345
56	0.069401	0.006615	0.111652	0.008426
57	0.069029	0.006534	0.103949	0.007833
58	0.066962	0.006366	0.105048	0.005921
59	0.066557	0.006368	0.099386	0.007370
60	0.066085	0.006240	0.107430	0.009736
61	0.064516	0.006164	0.118704	0.012976
62	0.063733	0.005907	0.114164	0.010045
63	0.064267	0.006494	0.118775	0.020397
64	0.063248	0.007297	0.115750	0.008942
65	0.062671	0.007981	0.105100	0.012158
66	0.061447	0.008042	0.097453	0.011059
67	0.060404	0.007649	0.100706	0.008736
68	0.058914	0.007643	0.102665	0.009854
69	0.059117	0.007441	0.102830	0.013532
70	0.051001	0.007642	0.088408	0.007665
71	0.044691	0.006343	0.084413	0.006498
72	0.043934	0.006427	0.087442	0.010315
73	0.044100	0.006829	0.090982	0.005775
74	0.042936	0.006597	0.079526	0.007548
75	0.043090	0.006772	0.089080	0.009918
76	0.045786	0.007602	0.094736	0.015334
77	0.045556	0.007366	0.099756	0.016748
78	0.040352	0.010450	0.081397	0.013359
79	0.040485	0.008936	0.072288	0.004830
80	0.037950	0.009622	0.076497	0.008170
81	0.038412	0.009089	0.094072	0.002770
82	0.039130	0.008245	0.081267	0.014195
83	0.038012	0.009381	0.079478	0.010578
84	0.038052	0.009251	0.077299	0.005559
85	0.037681	0.008818	0.080332	0.014933
86	0.038134	0.009013	0.072588	0.007526
87	0.036146	0.009179	0.074893	0.001589
88	0.034966	0.008545	0.080691	0.013586
89	0.035883	0.008208	0.091149	0.001060
90	0.036482	0.008412	0.078232	0.008695
91	0.036577	0.008680	0.076839	0.010305
92	0.034980	0.007201	0.074310	0.001224
93	0.035041	0.008330	0.076642	0.011059
94	0.033755	0.007923	0.083586	0.002393
95	0.034418	0.007920	0.069652	0.002338
96	0.046391	0.000000	0.070965	0.000000
97	0.046055	0.000000	0.102234	0.000000
98	0.045788	0.000000	0.079359	0.000000
99	0.043521	0.000000	0.070165	0.000000
100	0.043398	0.000000	0.069762	0.000000
