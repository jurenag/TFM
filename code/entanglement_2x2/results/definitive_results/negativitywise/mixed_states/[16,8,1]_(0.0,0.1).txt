#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled (n\in(0.0,0.1)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7775%
#Sample standard deviation for averaged success rate: 0.047501299324334424%
#Same average success rate for supplementary tests: [0.5007349999999999, 0.5152950000000001, 0.64867, 0.91489, 0.996015, 0.99875, 0.995425, 0.9963799999999999, 0.99811, 0.999365]%
#Sample STD for averaged success rate in supplementary tests: [0.00352665535440451, 0.003437001330920603, 0.0027084703348938476, 0.0010547342769626866, 0.00014639326743400515, 0.0001631621126364622, 0.00031981005104593816, 0.00024978947135539557, 0.0001253849871396143, 5.6611295251070524e-05]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.605699	0.009471	0.492052	0.018325
2	0.358987	0.020523	0.238015	0.018524
3	0.171526	0.016457	0.119153	0.013902
4	0.090730	0.012385	0.068128	0.010096
5	0.052999	0.008841	0.043013	0.007084
6	0.034617	0.006431	0.031231	0.005156
7	0.025093	0.004928	0.023892	0.004104
8	0.019721	0.003969	0.020017	0.003200
9	0.016155	0.003279	0.017400	0.002691
10	0.013857	0.002824	0.015596	0.002360
11	0.012260	0.002511	0.014099	0.002007
12	0.011036	0.002256	0.013217	0.001952
13	0.010079	0.002057	0.012210	0.001736
14	0.009355	0.001911	0.012034	0.001655
15	0.008656	0.001810	0.011315	0.001463
16	0.008160	0.001714	0.011184	0.001318
17	0.007711	0.001625	0.010557	0.001253
18	0.007304	0.001564	0.010630	0.001215
19	0.006964	0.001460	0.010207	0.001310
20	0.006632	0.001426	0.009640	0.001058
21	0.006318	0.001328	0.009933	0.001257
22	0.006133	0.001304	0.009443	0.001065
23	0.005971	0.001276	0.009574	0.000988
24	0.005745	0.001198	0.009613	0.001006
25	0.005550	0.001176	0.009943	0.000695
26	0.005489	0.001146	0.009628	0.000858
27	0.005309	0.001107	0.009353	0.000931
28	0.005053	0.001080	0.009449	0.000848
29	0.005034	0.001048	0.009909	0.001007
30	0.004829	0.001051	0.009710	0.001112
31	0.004721	0.001009	0.009696	0.001066
32	0.004667	0.000993	0.009855	0.000759
33	0.004541	0.000965	0.009525	0.000726
34	0.004371	0.000944	0.009601	0.000803
35	0.004370	0.000912	0.009111	0.000790
36	0.004316	0.000925	0.009673	0.001005
37	0.004163	0.000912	0.009215	0.000861
38	0.004085	0.000908	0.010057	0.000785
39	0.004068	0.000901	0.009738	0.001021
40	0.003903	0.000877	0.009561	0.001053
41	0.003847	0.000959	0.010360	0.000995
42	0.004171	0.001061	0.010027	0.001179
43	0.003909	0.000982	0.010269	0.001620
44	0.004178	0.001145	0.008755	0.001287
45	0.004275	0.001346	0.008694	0.000942
46	0.004226	0.001619	0.009113	0.001210
47	0.004086	0.001595	0.008749	0.001204
48	0.004181	0.001568	0.008385	0.001034
49	0.004529	0.001897	0.008636	0.001775
50	0.004990	0.002293	0.009509	0.001605
51	0.005076	0.002436	0.010496	0.001427
52	0.005073	0.002328	0.010157	0.000659
53	0.004954	0.002325	0.009596	0.001226
54	0.005087	0.002297	0.008650	0.001212
55	0.004976	0.002270	0.011103	0.002177
56	0.004953	0.002345	0.010354	0.001091
57	0.004850	0.002252	0.009290	0.000845
58	0.004898	0.002233	0.009324	0.000559
59	0.005784	0.003095	0.009440	0.000886
60	0.005649	0.003033	0.011035	0.000019
61	0.005576	0.003180	0.011853	0.001265
62	0.005449	0.003112	0.010538	0.001675
63	0.005669	0.003110	0.010217	0.000967
64	0.005543	0.003185	0.009681	0.000343
65	0.005398	0.003177	0.010342	0.000826
66	0.005485	0.003133	0.009077	0.001482
67	0.005595	0.003215	0.011972	0.000190
68	0.005415	0.003171	0.010965	0.000334
69	0.005271	0.003128	0.010691	0.001697
70	0.009542	0.000000	0.011174	0.000000
71	0.009492	0.000000	0.011489	0.000000
72	0.009537	0.000000	0.010643	0.000000
73	0.009163	0.000000	0.010304	0.000000
74	0.009552	0.000000	0.011066	0.000000
75	0.009358	0.000000	0.010689	0.000000
76	0.009208	0.000000	0.010657	0.000000
77	0.009292	0.000000	0.012512	0.000000
78	0.008862	0.000000	0.013530	0.000000
79	0.009285	0.000000	0.011714	0.000000
80	0.009067	0.000000	0.011165	0.000000
81	0.008977	0.000000	0.010499	0.000000
82	0.009006	0.000000	0.010763	0.000000
83	0.009147	0.000000	0.010829	0.000000
84	0.009070	0.000000	0.010078	0.000000
85	0.008612	0.000000	0.010799	0.000000
86	0.008799	0.000000	0.011583	0.000000
