#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled (n\in(0.2,0.3)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.885%
#Sample standard deviation for averaged success rate: 0.0329270975034244%
#Same average success rate for supplementary tests: [0.500205, 0.5117400000000001, 0.6864150000000001, 0.9784950000000001, 0.9996200000000001, 0.99859, 0.9969950000000001, 0.999495, 0.999865, 0.9999399999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.0035294224596596536, 0.0034594014829157937, 0.0026153771025896034, 0.000512544387102218, 4.519712380229545e-05, 0.000190133098118136, 0.0002753606703035121, 8.781940987046649e-05, 3.0398828102385566e-05, 1.7315311143909217e-05]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.603485	0.008357	0.483091	0.015742
2	0.341213	0.016607	0.231457	0.015874
3	0.169994	0.013411	0.128242	0.012691
4	0.099656	0.011271	0.077767	0.010691
5	0.062416	0.009397	0.048956	0.008233
6	0.040842	0.006929	0.033369	0.005955
7	0.028478	0.004873	0.024402	0.004096
8	0.020990	0.003382	0.018521	0.002981
9	0.016482	0.002489	0.015195	0.002248
10	0.013425	0.001932	0.013112	0.001714
11	0.011324	0.001546	0.011608	0.001515
12	0.009776	0.001283	0.009988	0.001342
13	0.008605	0.001128	0.009044	0.001178
14	0.007706	0.001001	0.008223	0.000958
15	0.006891	0.000889	0.007294	0.000861
16	0.006328	0.000821	0.007270	0.000804
17	0.005774	0.000744	0.007023	0.000782
18	0.005273	0.000687	0.006445	0.000723
19	0.005008	0.000685	0.006459	0.000737
20	0.004634	0.000641	0.006736	0.000776
21	0.004398	0.000598	0.006139	0.000723
22	0.004014	0.000555	0.005840	0.000566
23	0.003873	0.000535	0.005986	0.000742
24	0.003611	0.000517	0.005584	0.000619
25	0.003422	0.000521	0.005250	0.000572
26	0.003290	0.000495	0.004908	0.000562
27	0.003109	0.000475	0.005131	0.000587
28	0.003004	0.000488	0.004849	0.000512
29	0.002886	0.000427	0.005221	0.000730
30	0.002737	0.000442	0.005104	0.000639
31	0.002630	0.000401	0.004808	0.000635
32	0.002509	0.000408	0.004679	0.000588
33	0.002332	0.000377	0.005007	0.000665
34	0.002262	0.000416	0.004914	0.000599
35	0.002163	0.000375	0.004731	0.000629
36	0.002114	0.000372	0.004846	0.000683
37	0.002066	0.000368	0.004777	0.000536
38	0.002007	0.000376	0.004387	0.000527
39	0.001908	0.000324	0.004721	0.000660
40	0.001860	0.000357	0.004672	0.000635
41	0.001800	0.000350	0.004583	0.000583
42	0.001773	0.000352	0.004592	0.000647
43	0.001642	0.000346	0.004871	0.000716
44	0.001686	0.000332	0.004355	0.000599
45	0.001607	0.000301	0.005025	0.000799
46	0.001507	0.000287	0.004331	0.000653
47	0.001442	0.000321	0.004660	0.000690
48	0.001407	0.000298	0.004896	0.000571
49	0.001339	0.000286	0.004604	0.000779
50	0.001294	0.000263	0.004483	0.000679
51	0.001409	0.000373	0.004899	0.000731
52	0.001321	0.000285	0.004724	0.000613
53	0.001487	0.000363	0.004679	0.000870
54	0.001444	0.000289	0.004251	0.000655
55	0.001423	0.000329	0.004488	0.000600
56	0.001412	0.000292	0.003984	0.000611
57	0.001316	0.000292	0.005195	0.001477
58	0.001263	0.000287	0.004010	0.000713
59	0.001303	0.000278	0.003607	0.000542
60	0.001172	0.000270	0.005029	0.000559
61	0.001183	0.000290	0.003995	0.000630
62	0.001187	0.000267	0.004410	0.000698
63	0.001176	0.000259	0.003732	0.000685
64	0.000997	0.000244	0.004704	0.000515
65	0.001102	0.000216	0.004417	0.000751
66	0.001139	0.000289	0.004235	0.000858
67	0.001162	0.000288	0.003669	0.000727
68	0.001109	0.000270	0.003541	0.000736
69	0.000982	0.000289	0.004067	0.000706
70	0.001112	0.000319	0.003608	0.001111
71	0.001227	0.000398	0.003840	0.001286
72	0.001197	0.000389	0.004374	0.000935
73	0.001085	0.000313	0.004099	0.000746
74	0.001033	0.000300	0.004319	0.000745
75	0.000660	0.000062	0.003304	0.000812
76	0.000556	0.000000	0.001968	0.000000
77	0.000682	0.000000	0.002297	0.000000
78	0.000674	0.000000	0.001986	0.000000
