#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled (n\in(0.1,0.2)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.1, 0.2).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.70750000000001%
#Sample standard deviation for averaged success rate: 0.05871423113266427%
#Same average success rate for supplementary tests: [0.500375, 0.5178550000000001, 0.7123400000000001, 0.97729, 0.9993350000000001, 0.9988800000000001, 0.9985349999999998, 0.99874, 0.99971, 0.999815]%
#Sample STD for averaged success rate in supplementary tests: [0.0035247082104409725, 0.003419639593400158, 0.0024547986108844023, 0.0004990619149965309, 5.790413413557946e-05, 0.00015743976625995162, 0.00016842710218374578, 0.00014839346346792794, 4.1180031568786744e-05, 3.0385666851401475e-05]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.622305	0.005883	0.523835	0.011142
2	0.394607	0.013920	0.278187	0.015557
3	0.205245	0.013148	0.147020	0.011816
4	0.113632	0.010668	0.085028	0.009439
5	0.068267	0.008231	0.053768	0.006960
6	0.045400	0.006015	0.038594	0.005032
7	0.033209	0.004297	0.029711	0.003640
8	0.026163	0.003085	0.024795	0.002588
9	0.021485	0.002262	0.021266	0.001959
10	0.018321	0.001711	0.019706	0.001467
11	0.016012	0.001354	0.017982	0.001138
12	0.014257	0.001101	0.016346	0.000995
13	0.012909	0.000934	0.016267	0.000753
14	0.011846	0.000802	0.014910	0.000767
15	0.010968	0.000736	0.015033	0.000685
16	0.010217	0.000668	0.013897	0.000505
17	0.009575	0.000629	0.012890	0.000592
18	0.009065	0.000621	0.013555	0.000651
19	0.008594	0.000557	0.012364	0.000464
20	0.007983	0.000519	0.013171	0.000556
21	0.007736	0.000504	0.013492	0.000559
22	0.007527	0.000498	0.012720	0.000513
23	0.007132	0.000503	0.012835	0.000564
24	0.006839	0.000475	0.012554	0.000452
25	0.006645	0.000484	0.013028	0.000799
26	0.006471	0.000475	0.012622	0.000577
27	0.006136	0.000474	0.011850	0.000568
28	0.005900	0.000463	0.012213	0.000619
29	0.005787	0.000459	0.012089	0.000644
30	0.005601	0.000416	0.012038	0.000623
31	0.005320	0.000428	0.012395	0.000996
32	0.005273	0.000411	0.011668	0.000746
33	0.005119	0.000429	0.012283	0.000840
34	0.004927	0.000410	0.012781	0.000726
35	0.004813	0.000450	0.012596	0.000842
36	0.004695	0.000414	0.012419	0.000804
37	0.004618	0.000413	0.012325	0.000635
38	0.004462	0.000441	0.011798	0.000651
39	0.004242	0.000404	0.012800	0.000707
40	0.004287	0.000425	0.012656	0.001018
41	0.004113	0.000397	0.012061	0.000592
42	0.004126	0.000418	0.012106	0.000674
43	0.003943	0.000500	0.013195	0.001007
44	0.003858	0.000460	0.012784	0.001077
45	0.004021	0.000462	0.011880	0.000720
46	0.003910	0.000509	0.011578	0.000767
47	0.003743	0.000487	0.011698	0.000710
48	0.003699	0.000479	0.012507	0.000495
49	0.003606	0.000472	0.011261	0.000554
50	0.003718	0.000512	0.011416	0.000552
51	0.003474	0.000550	0.011225	0.000553
52	0.003356	0.000527	0.010860	0.000675
53	0.003108	0.000539	0.011370	0.001075
54	0.002989	0.000605	0.012343	0.001556
55	0.002164	0.000263	0.011748	0.001566
56	0.002227	0.000274	0.010545	0.000881
57	0.002183	0.000246	0.010823	0.001267
58	0.002111	0.000314	0.011145	0.000575
59	0.002099	0.000241	0.012918	0.001487
60	0.002014	0.000297	0.011593	0.000654
61	0.001901	0.000300	0.011822	0.000879
62	0.002067	0.000267	0.011494	0.000659
63	0.002073	0.000380	0.010136	0.001192
64	0.001924	0.000496	0.009709	0.000932
65	0.001610	0.000236	0.012548	0.001064
66	0.002299	0.000000	0.010395	0.000000
67	0.002220	0.000000	0.010722	0.000000
68	0.001885	0.000000	0.010243	0.000000
69	0.001790	0.000000	0.010081	0.000000
