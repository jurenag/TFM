#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled (n\in(0.3,0.4)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.97999999999999%
#Sample standard deviation for averaged success rate: 0.009348796714034824%
#Same average success rate for supplementary tests: [0.500125, 0.5032599999999999, 0.60169, 0.948835, 0.9999899999999999, 0.9879049999999999, 0.98977, 0.996125, 0.999935, 1.0]%
#Sample STD for averaged success rate in supplementary tests: [0.0035346250464158143, 0.0035149208554389956, 0.0030178315385388896, 0.0009407348929161718, 7.070714249708022e-06, 0.00045228370382980877, 0.0005142152807920008, 0.00030178505388769335, 2.1789647771337554e-05, 0.0]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.603753	0.006194	0.481135	0.013403
2	0.327505	0.014537	0.209821	0.011576
3	0.142358	0.007300	0.100810	0.005068
4	0.070800	0.003746	0.052988	0.003341
5	0.037687	0.002702	0.030090	0.002560
6	0.021495	0.002014	0.018523	0.001855
7	0.013331	0.001392	0.011916	0.001283
8	0.008790	0.000958	0.008954	0.000904
9	0.006119	0.000685	0.006446	0.000639
10	0.004557	0.000496	0.005011	0.000524
11	0.003381	0.000388	0.004077	0.000402
12	0.002665	0.000306	0.003571	0.000323
13	0.002139	0.000260	0.002897	0.000275
14	0.001714	0.000213	0.002708	0.000293
15	0.001409	0.000172	0.002410	0.000237
16	0.001252	0.000168	0.002044	0.000215
17	0.001056	0.000134	0.001910	0.000208
18	0.000910	0.000123	0.001755	0.000192
19	0.000754	0.000110	0.001803	0.000273
20	0.000674	0.000099	0.001452	0.000208
21	0.000612	0.000102	0.001246	0.000165
22	0.000526	0.000087	0.001221	0.000209
23	0.000477	0.000076	0.001409	0.000298
24	0.000430	0.000070	0.001096	0.000149
25	0.000376	0.000067	0.001170	0.000164
26	0.000348	0.000054	0.001113	0.000189
27	0.000323	0.000048	0.001117	0.000243
28	0.000270	0.000050	0.001116	0.000279
29	0.000246	0.000044	0.000983	0.000194
30	0.000243	0.000050	0.001183	0.000278
31	0.000210	0.000042	0.000922	0.000245
32	0.000190	0.000039	0.000968	0.000195
33	0.000159	0.000039	0.001101	0.000276
34	0.000151	0.000037	0.001041	0.000210
35	0.000162	0.000041	0.001050	0.000289
36	0.000118	0.000024	0.000972	0.000320
37	0.000111	0.000026	0.000854	0.000178
38	0.000098	0.000025	0.000851	0.000183
39	0.000087	0.000026	0.001024	0.000293
40	0.000105	0.000041	0.000875	0.000205
41	0.000081	0.000026	0.000994	0.000300
42	0.000076	0.000033	0.000993	0.000301
43	0.000064	0.000024	0.000697	0.000198
44	0.000061	0.000020	0.001139	0.000313
45	0.000049	0.000019	0.001035	0.000276
46	0.000057	0.000031	0.001016	0.000294
47	0.000077	0.000037	0.000847	0.000305
48	0.000051	0.000028	0.000919	0.000347
49	0.000068	0.000040	0.001191	0.000287
50	0.000059	0.000035	0.000619	0.000193
51	0.000042	0.000018	0.000823	0.000296
52	0.000173	0.000075	0.000517	0.000298
53	0.000095	0.000045	0.000741	0.000471
54	0.000162	0.000083	0.000526	0.000252
55	0.000017	0.000000	0.000194	0.000000
56	0.000016	0.000000	0.000126	0.000000
57	0.000024	0.000000	0.000136	0.000000
