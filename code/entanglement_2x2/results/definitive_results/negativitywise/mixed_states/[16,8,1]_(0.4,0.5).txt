#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled (n\in(0.4,0.5)) DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.4, 0.5).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.9975%
#Sample standard deviation for averaged success rate: 0.0024996874803931207%
#Same average success rate for supplementary tests: [0.5000100000000001, 0.501085, 0.538105, 0.8183500000000001, 0.99851, 0.893835, 0.93115, 0.96852, 0.993505, 0.9999899999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.003535470265042544, 0.0035288760446847665, 0.0033460163551229083, 0.0020659777043811473, 0.00010788417400154765, 0.0014577635743665712, 0.001316599360094029, 0.0009274699348226878, 0.000368464039968639, 7.070714249708022e-06]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.600197	0.005829	0.462507	0.011772
2	0.295427	0.011755	0.158638	0.008993
3	0.096027	0.005694	0.055485	0.003251
4	0.036873	0.002407	0.024574	0.001729
5	0.016591	0.001305	0.012014	0.001111
6	0.007803	0.000735	0.005959	0.000721
7	0.003734	0.000438	0.003134	0.000519
8	0.001914	0.000282	0.001703	0.000307
9	0.001024	0.000185	0.000930	0.000209
10	0.000578	0.000132	0.000614	0.000172
11	0.000373	0.000098	0.000393	0.000123
12	0.000217	0.000064	0.000272	0.000090
13	0.000147	0.000049	0.000225	0.000081
14	0.000091	0.000034	0.000174	0.000072
15	0.000055	0.000022	0.000184	0.000079
16	0.000037	0.000016	0.000091	0.000035
17	0.000024	0.000011	0.000110	0.000060
18	0.000016	0.000008	0.000075	0.000040
19	0.000011	0.000006	0.000045	0.000019
20	0.000008	0.000004	0.000055	0.000034
21	0.000006	0.000003	0.000062	0.000041
22	0.000003	0.000002	0.000031	0.000017
23	0.000002	0.000001	0.000015	0.000009
24	0.000002	0.000001	0.000041	0.000027
25	0.000001	0.000000	0.000037	0.000027
26	0.000001	0.000000	0.000025	0.000020
27	0.000000	0.000000	0.000013	0.000010
28	0.000000	0.000000	0.000039	0.000028
29	0.000000	0.000000	0.000011	0.000006
30	0.000000	0.000000	0.000012	0.000009
31	0.000000	0.000000	0.000019	0.000014
32	0.000000	0.000000	0.000023	0.000018
33	0.000000	0.000000	0.000009	0.000005
34	0.000000	0.000000	0.000014	0.000009
35	0.000000	0.000000	0.000019	0.000014
36	0.000000	0.000000	0.000013	0.000009
37	0.000000	0.000000	0.000013	0.000006
38	0.000000	0.000000	0.000009	0.000008
39	0.000000	0.000000	0.000010	0.000008
40	0.000000	0.000000	0.000019	0.000014
41	0.000000	0.000000	0.000013	0.000009
42	0.000000	0.000000	0.000012	0.000008
43	0.000000	0.000000	0.000012	0.000008
44	0.000000	0.000000	0.000008	0.000006
45	0.000000	0.000000	0.000025	0.000000
46	0.000000	0.000000	0.000017	0.000000
47	0.000000	0.000000	0.000032	0.000000
48	0.000000	0.000000	0.000039	0.000000
