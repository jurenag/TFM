#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7625%
#Sample standard deviation for averaged success rate: 0.02701200543277947%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.701237	0.005347	0.693087	0.000042
2	0.693270	0.000036	0.693335	0.000122
3	0.693118	0.000067	0.693145	0.000144
4	0.692900	0.000102	0.693068	0.000130
5	0.692494	0.000242	0.692590	0.000359
6	0.691733	0.000602	0.691485	0.001016
7	0.689749	0.001630	0.688314	0.002726
8	0.684358	0.004581	0.680287	0.007344
9	0.671854	0.011529	0.663363	0.016871
10	0.650894	0.022389	0.639932	0.028215
11	0.625781	0.033492	0.614193	0.038349
12	0.599631	0.042653	0.588287	0.046330
13	0.572816	0.049905	0.561126	0.052651
14	0.544299	0.055127	0.531159	0.056630
15	0.511803	0.057964	0.496279	0.058364
16	0.474536	0.058863	0.456834	0.058847
17	0.434785	0.059653	0.417975	0.060172
18	0.399555	0.061701	0.386338	0.062615
19	0.372021	0.064361	0.362306	0.065186
20	0.350994	0.066788	0.343732	0.067350
21	0.334244	0.068692	0.329217	0.069020
22	0.320163	0.070061	0.315885	0.069945
23	0.307719	0.070719	0.304186	0.070210
24	0.296071	0.070705	0.292234	0.069702
25	0.283942	0.069878	0.279979	0.068465
26	0.271524	0.068320	0.267426	0.066555
27	0.259116	0.066344	0.255040	0.064514
28	0.247190	0.064264	0.243695	0.062301
29	0.236182	0.062305	0.233074	0.060563
30	0.226130	0.060474	0.223456	0.058766
31	0.216495	0.058652	0.214210	0.056912
32	0.206958	0.056718	0.204816	0.054952
33	0.197141	0.054634	0.195255	0.052887
34	0.187135	0.052509	0.185205	0.050790
35	0.176939	0.050408	0.175456	0.048666
36	0.166851	0.048485	0.165228	0.046969
37	0.156780	0.046736	0.155535	0.045237
38	0.147360	0.045280	0.146055	0.043915
39	0.138456	0.044033	0.138334	0.042896
40	0.130329	0.042881	0.130545	0.041576
41	0.122785	0.041765	0.122548	0.040530
42	0.115529	0.040609	0.115711	0.039237
43	0.108416	0.039372	0.108618	0.038029
44	0.101529	0.038027	0.101684	0.036615
45	0.094707	0.036557	0.094818	0.035120
46	0.088120	0.034925	0.087966	0.033389
47	0.081682	0.033120	0.081829	0.031490
48	0.075398	0.031056	0.075709	0.029405
49	0.069409	0.028877	0.069799	0.027059
50	0.063597	0.026419	0.063515	0.024568
51	0.057800	0.023767	0.057971	0.021883
52	0.052324	0.021038	0.052774	0.019079
53	0.047021	0.018197	0.047408	0.016318
54	0.042127	0.015516	0.042771	0.013839
55	0.037731	0.013112	0.038503	0.011611
56	0.033837	0.010966	0.035000	0.009732
57	0.030424	0.009228	0.031628	0.008271
58	0.027421	0.007798	0.028932	0.007059
59	0.024908	0.006728	0.026384	0.006262
60	0.022769	0.005949	0.024682	0.005697
61	0.020885	0.005362	0.023018	0.005344
62	0.019263	0.004958	0.021047	0.005052
63	0.017792	0.004707	0.019936	0.004997
64	0.016590	0.004531	0.018812	0.004904
65	0.015501	0.004473	0.017605	0.004826
66	0.014563	0.004414	0.016917	0.004949
67	0.013793	0.004416	0.015968	0.004812
68	0.013116	0.004385	0.015205	0.004870
69	0.012446	0.004380	0.014684	0.004834
70	0.011903	0.004343	0.014178	0.004871
71	0.011414	0.004327	0.013751	0.004796
72	0.010984	0.004286	0.013464	0.004915
73	0.010621	0.004258	0.012942	0.004886
74	0.010264	0.004208	0.012479	0.004872
75	0.009924	0.004178	0.012295	0.004838
76	0.009575	0.004129	0.011913	0.004837
77	0.009319	0.004091	0.011465	0.004797
78	0.009112	0.004088	0.011313	0.004812
79	0.008823	0.004010	0.011047	0.004766
80	0.008630	0.004016	0.010752	0.004728
81	0.008405	0.003969	0.010769	0.004720
82	0.008218	0.003955	0.010880	0.004785
83	0.008077	0.003930	0.010368	0.004775
84	0.007870	0.003899	0.010122	0.004685
85	0.007727	0.003895	0.009765	0.004661
86	0.007568	0.003868	0.009912	0.004821
87	0.007402	0.003828	0.010004	0.004719
88	0.007296	0.003830	0.009613	0.004691
89	0.007163	0.003790	0.009419	0.004621
90	0.006986	0.003757	0.009244	0.004625
91	0.006925	0.003744	0.009095	0.004693
92	0.006789	0.003707	0.009008	0.004662
93	0.006684	0.003701	0.008999	0.004570
94	0.006597	0.003685	0.008941	0.004695
95	0.006541	0.003678	0.008596	0.004535
96	0.006393	0.003658	0.008618	0.004559
97	0.006320	0.003632	0.008638	0.004572
98	0.006226	0.003607	0.008889	0.004645
99	0.006175	0.003593	0.008510	0.004549
100	0.006082	0.003566	0.008722	0.004606
