#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.8375%
#Sample standard deviation for averaged success rate: 0.025241799410899136%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.695075	0.000904	0.693302	0.000119
2	0.693254	0.000025	0.693258	0.000065
3	0.693120	0.000048	0.693391	0.000105
4	0.692903	0.000097	0.692992	0.000203
5	0.692542	0.000228	0.692712	0.000387
6	0.691838	0.000539	0.691602	0.000808
7	0.690135	0.001371	0.689270	0.002119
8	0.686085	0.003387	0.683436	0.005204
9	0.677439	0.007606	0.671568	0.010819
10	0.661383	0.014824	0.651506	0.019774
11	0.637114	0.024589	0.623805	0.030144
12	0.607673	0.034543	0.593094	0.039545
13	0.576109	0.042882	0.560554	0.047086
14	0.542706	0.049394	0.525823	0.053043
15	0.507344	0.054829	0.489617	0.057937
16	0.472141	0.059070	0.455069	0.061356
17	0.438569	0.061785	0.422332	0.063336
18	0.407153	0.063272	0.392186	0.064592
19	0.378399	0.064337	0.365165	0.065454
20	0.353206	0.065276	0.342698	0.066411
21	0.332852	0.066177	0.324766	0.067345
22	0.316509	0.066983	0.310163	0.068004
23	0.302603	0.067522	0.297192	0.068352
24	0.289789	0.067558	0.284696	0.068202
25	0.276966	0.066911	0.271946	0.067069
26	0.263204	0.065224	0.257735	0.064798
27	0.247395	0.062217	0.240541	0.060907
28	0.228712	0.057859	0.220782	0.055991
29	0.207981	0.052592	0.199588	0.050371
30	0.187641	0.047790	0.180354	0.046258
31	0.169533	0.044297	0.164124	0.043508
32	0.154894	0.042197	0.151033	0.042023
33	0.143326	0.040983	0.141004	0.041028
34	0.133847	0.040179	0.132496	0.040378
35	0.125975	0.039533	0.125074	0.039630
36	0.118912	0.038890	0.118157	0.039006
37	0.112359	0.038137	0.112228	0.038243
38	0.106173	0.037277	0.105938	0.037178
39	0.100244	0.036230	0.100300	0.036120
40	0.094443	0.034897	0.094280	0.034583
41	0.088688	0.033321	0.088558	0.032656
42	0.082841	0.031354	0.082808	0.030563
43	0.076985	0.029067	0.076970	0.028199
44	0.070929	0.026346	0.070295	0.025014
45	0.064728	0.023259	0.064167	0.021613
46	0.058589	0.020001	0.058107	0.018206
47	0.052602	0.016721	0.052020	0.015126
48	0.046940	0.013636	0.046364	0.012184
49	0.041709	0.010868	0.041928	0.009821
50	0.037173	0.008659	0.037381	0.007955
51	0.033277	0.007004	0.033650	0.006593
52	0.029965	0.005844	0.030686	0.005783
53	0.027222	0.005096	0.028125	0.005287
54	0.024882	0.004617	0.025898	0.005051
55	0.022845	0.004332	0.023970	0.004823
56	0.021173	0.004165	0.022388	0.004807
57	0.019652	0.004051	0.020983	0.004748
58	0.018309	0.003976	0.019758	0.004720
59	0.017110	0.003892	0.018584	0.004656
60	0.016089	0.003831	0.017421	0.004507
61	0.015182	0.003753	0.016648	0.004461
62	0.014311	0.003682	0.015909	0.004382
63	0.013542	0.003599	0.015055	0.004303
64	0.012839	0.003507	0.014691	0.004317
65	0.012207	0.003450	0.013793	0.004102
66	0.011617	0.003326	0.013305	0.004108
67	0.011043	0.003235	0.012916	0.004060
68	0.010558	0.003169	0.012339	0.003948
69	0.010105	0.003068	0.012080	0.003883
70	0.009715	0.002995	0.011564	0.003825
71	0.009306	0.002906	0.011198	0.003678
72	0.008935	0.002841	0.010867	0.003639
73	0.008601	0.002776	0.010630	0.003549
74	0.008306	0.002704	0.010121	0.003525
75	0.008000	0.002635	0.009766	0.003381
76	0.007727	0.002582	0.009682	0.003368
77	0.007467	0.002510	0.009352	0.003313
78	0.007269	0.002462	0.008808	0.003165
79	0.007019	0.002401	0.008782	0.003142
80	0.006833	0.002349	0.008720	0.003169
81	0.006612	0.002300	0.008354	0.003112
82	0.006430	0.002251	0.008192	0.003020
83	0.006250	0.002205	0.008164	0.003080
84	0.006073	0.002156	0.007962	0.003006
85	0.005961	0.002121	0.007705	0.002930
86	0.005742	0.002083	0.007601	0.002920
87	0.005626	0.002042	0.007425	0.002900
88	0.005477	0.002005	0.007122	0.002778
89	0.005913	0.002093	0.007752	0.002962
90	0.005771	0.002055	0.007825	0.002945
91	0.005638	0.002033	0.007761	0.002895
92	0.005504	0.001999	0.007934	0.003010
93	0.006051	0.002078	0.008289	0.003073
94	0.005894	0.002051	0.008017	0.002933
95	0.006551	0.002152	0.008805	0.003109
96	0.006438	0.002138	0.008851	0.003114
97	0.007312	0.002179	0.010554	0.003453
98	0.007153	0.002168	0.009852	0.003146
99	0.007086	0.002134	0.009858	0.003265
100	0.006884	0.002108	0.010039	0.003368
