#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [8, 4, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.79249999999999%
#Sample standard deviation for averaged success rate: 0.029503998669681262%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.696218	0.001313	0.693483	0.000127
2	0.693216	0.000026	0.693243	0.000111
3	0.693109	0.000045	0.693330	0.000146
4	0.693019	0.000071	0.693302	0.000155
5	0.692856	0.000121	0.693180	0.000250
6	0.692631	0.000214	0.692650	0.000315
7	0.692241	0.000396	0.692310	0.000472
8	0.691570	0.000723	0.691304	0.000938
9	0.690415	0.001315	0.689862	0.001664
10	0.688500	0.002265	0.687646	0.002819
11	0.685394	0.003811	0.683907	0.004729
12	0.680498	0.006165	0.678104	0.007499
13	0.673257	0.009399	0.669863	0.011021
14	0.663385	0.013593	0.658798	0.015568
15	0.650834	0.018788	0.645374	0.021272
16	0.635598	0.025040	0.629339	0.027904
17	0.618043	0.032191	0.610605	0.035385
18	0.598804	0.039763	0.591285	0.042836
19	0.579824	0.046613	0.572746	0.049060
20	0.561523	0.052281	0.554646	0.054248
21	0.543577	0.056831	0.536467	0.058281
22	0.525762	0.060447	0.518267	0.061571
23	0.507441	0.063246	0.499569	0.063990
24	0.488107	0.065270	0.479473	0.065710
25	0.467336	0.066709	0.458038	0.066864
26	0.445557	0.067753	0.435855	0.067826
27	0.423590	0.068483	0.413853	0.068194
28	0.401710	0.068607	0.392185	0.067995
29	0.380017	0.068182	0.370603	0.067164
30	0.358642	0.067306	0.349250	0.066077
31	0.338200	0.066429	0.329845	0.065212
32	0.319800	0.065816	0.312481	0.064624
33	0.303869	0.065539	0.297955	0.064377
34	0.290392	0.065463	0.285387	0.064349
35	0.278548	0.065458	0.274192	0.064319
36	0.267929	0.065371	0.264111	0.064162
37	0.258022	0.065165	0.254586	0.063895
38	0.248427	0.064754	0.245114	0.063350
39	0.238876	0.064127	0.235602	0.062618
40	0.229297	0.063231	0.226230	0.061578
41	0.219612	0.062043	0.216424	0.060260
42	0.209812	0.060613	0.206651	0.058663
43	0.200157	0.059070	0.197373	0.057191
44	0.190762	0.057539	0.188177	0.055545
45	0.181703	0.056003	0.179239	0.054122
46	0.173058	0.054540	0.170995	0.052629
47	0.164815	0.053145	0.163040	0.051203
48	0.156675	0.051750	0.155136	0.049895
49	0.148712	0.050336	0.147480	0.048470
50	0.140998	0.048964	0.139848	0.047184
51	0.133514	0.047651	0.132663	0.045934
52	0.126408	0.046386	0.125899	0.044596
53	0.119585	0.045147	0.118788	0.043461
54	0.113075	0.043961	0.112460	0.042321
55	0.106895	0.042788	0.106298	0.041236
56	0.101066	0.041618	0.100682	0.040032
57	0.095701	0.040462	0.095230	0.038924
58	0.090595	0.039293	0.090216	0.037802
59	0.085741	0.038076	0.085545	0.036614
60	0.081244	0.036864	0.081389	0.035489
61	0.077036	0.035723	0.077001	0.034452
62	0.073175	0.034619	0.073160	0.033328
63	0.069631	0.033614	0.069593	0.032409
64	0.066374	0.032700	0.066298	0.031568
65	0.063481	0.031883	0.063573	0.030855
66	0.060828	0.031178	0.061088	0.030223
67	0.058365	0.030541	0.058646	0.029551
68	0.056137	0.029980	0.056358	0.029119
69	0.054086	0.029469	0.054390	0.028544
70	0.052088	0.028996	0.052355	0.028102
71	0.050231	0.028552	0.050377	0.027716
72	0.048509	0.028136	0.048585	0.027307
73	0.046806	0.027713	0.047132	0.026894
74	0.045197	0.027284	0.045356	0.026540
75	0.043593	0.026872	0.044160	0.026055
76	0.042090	0.026440	0.042448	0.025651
77	0.040540	0.025974	0.040871	0.025231
78	0.039026	0.025437	0.039695	0.024631
79	0.037551	0.024861	0.038226	0.024041
80	0.035928	0.024147	0.036510	0.023355
81	0.034337	0.023319	0.034866	0.022537
82	0.032623	0.022356	0.033175	0.021459
83	0.030810	0.021229	0.031326	0.020321
84	0.028905	0.019914	0.029696	0.018898
85	0.026846	0.018400	0.027269	0.017409
86	0.024623	0.016703	0.025446	0.015678
87	0.022389	0.014919	0.023056	0.013881
88	0.020118	0.013065	0.020694	0.012116
89	0.017919	0.011258	0.018881	0.010341
90	0.015923	0.009596	0.016672	0.008797
91	0.014148	0.008115	0.015007	0.007421
92	0.012627	0.006868	0.013600	0.006285
93	0.011380	0.005836	0.012463	0.005353
94	0.010278	0.005007	0.011628	0.004629
95	0.009425	0.004346	0.010745	0.003968
96	0.008691	0.003789	0.009860	0.003449
97	0.008044	0.003346	0.009318	0.003056
98	0.007542	0.002973	0.008742	0.002775
99	0.007079	0.002676	0.008510	0.002506
100	0.006722	0.002424	0.007881	0.002227
