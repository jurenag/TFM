#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 93.97000000000001%
#Sample standard deviation for averaged success rate: 0.12358104628137673%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.704354	0.003557	0.693439	0.000160
2	0.693219	0.000038	0.693155	0.000051
3	0.693080	0.000032	0.693174	0.000066
4	0.693001	0.000037	0.693087	0.000050
5	0.692857	0.000058	0.693064	0.000061
6	0.692699	0.000087	0.692917	0.000111
7	0.692456	0.000148	0.692551	0.000150
8	0.692061	0.000256	0.692121	0.000311
9	0.691449	0.000461	0.691384	0.000557
10	0.690465	0.000821	0.690282	0.000976
11	0.688929	0.001402	0.688547	0.001701
12	0.686644	0.002282	0.685926	0.002780
13	0.683190	0.003572	0.681996	0.004236
14	0.678286	0.005345	0.676386	0.006229
15	0.671443	0.007708	0.668877	0.008833
16	0.662497	0.010635	0.659034	0.011926
17	0.651355	0.014049	0.647288	0.015415
18	0.637917	0.017882	0.633239	0.019344
19	0.622448	0.021996	0.617186	0.023452
20	0.605329	0.026241	0.599995	0.027668
21	0.586977	0.030466	0.581654	0.031779
22	0.568023	0.034566	0.562924	0.035731
23	0.548572	0.038480	0.544057	0.039431
24	0.528779	0.042093	0.524561	0.042850
25	0.508753	0.045394	0.505102	0.046009
26	0.489104	0.048448	0.486023	0.048961
27	0.470231	0.051326	0.467793	0.051816
28	0.452266	0.054086	0.450596	0.054590
29	0.435491	0.056736	0.434422	0.057280
30	0.420092	0.059261	0.419581	0.059889
31	0.406035	0.061616	0.406083	0.062237
32	0.393153	0.063814	0.393454	0.064492
33	0.381477	0.065794	0.382309	0.066451
34	0.371027	0.067499	0.372295	0.068148
35	0.361535	0.068935	0.362989	0.069584
36	0.352816	0.070110	0.354508	0.070686
37	0.344663	0.071003	0.346400	0.071551
38	0.336828	0.071651	0.338775	0.072067
39	0.329185	0.072094	0.330969	0.072497
40	0.321707	0.072390	0.323288	0.072709
41	0.314335	0.072634	0.315949	0.073019
42	0.307018	0.072883	0.308661	0.073210
43	0.299787	0.073132	0.301368	0.073485
44	0.293094	0.073353	0.294614	0.073635
45	0.286869	0.073488	0.288589	0.073811
46	0.281142	0.073572	0.282817	0.073848
47	0.275744	0.073593	0.277523	0.073873
48	0.270835	0.073585	0.272459	0.073872
49	0.266296	0.073527	0.267865	0.073819
50	0.262080	0.073470	0.263661	0.073717
51	0.258110	0.073357	0.259664	0.073587
52	0.254335	0.073207	0.255971	0.073431
53	0.250786	0.073037	0.252330	0.073247
54	0.247318	0.072860	0.248798	0.072978
55	0.243992	0.072647	0.245405	0.072756
56	0.240725	0.072401	0.242118	0.072514
57	0.237523	0.072160	0.239022	0.072158
58	0.234398	0.071878	0.235893	0.071887
59	0.231367	0.071585	0.232656	0.071552
60	0.228413	0.071283	0.229562	0.071244
61	0.225475	0.070970	0.226772	0.070927
62	0.222623	0.070620	0.223818	0.070513
63	0.219744	0.070238	0.220884	0.070141
64	0.216882	0.069805	0.217980	0.069647
65	0.213975	0.069327	0.215368	0.069146
66	0.211042	0.068797	0.212329	0.068530
67	0.208052	0.068219	0.209311	0.067960
68	0.205044	0.067619	0.206287	0.067369
69	0.202069	0.067006	0.203504	0.066709
70	0.199095	0.066370	0.200420	0.066024
71	0.196171	0.065741	0.197700	0.065375
72	0.193361	0.065115	0.194893	0.064755
73	0.190621	0.064497	0.192245	0.064173
74	0.187940	0.063881	0.189671	0.063522
75	0.185408	0.063281	0.187083	0.062890
76	0.182931	0.062667	0.184751	0.062324
77	0.180542	0.062063	0.182516	0.061651
78	0.178228	0.061453	0.180511	0.061035
79	0.175969	0.060836	0.178274	0.060437
80	0.173757	0.060208	0.175856	0.059823
81	0.171635	0.059585	0.173801	0.059185
82	0.169559	0.058955	0.171703	0.058553
83	0.167480	0.058315	0.169638	0.057928
84	0.165398	0.057677	0.168025	0.057219
85	0.163434	0.057041	0.165888	0.056646
86	0.161520	0.056426	0.164243	0.056049
87	0.159691	0.055846	0.162303	0.055516
88	0.157890	0.055297	0.160682	0.054955
89	0.156192	0.054766	0.158999	0.054449
90	0.154560	0.054272	0.157255	0.053991
91	0.152971	0.053794	0.155913	0.053496
92	0.151437	0.053351	0.154220	0.053070
93	0.149893	0.052919	0.152941	0.052635
94	0.148424	0.052511	0.151401	0.052287
95	0.146994	0.052108	0.149842	0.051854
96	0.145518	0.051715	0.148469	0.051476
97	0.144137	0.051337	0.146961	0.051139
98	0.142699	0.050978	0.145598	0.050756
99	0.141248	0.050614	0.144274	0.050340
100	0.139821	0.050248	0.142614	0.050049
