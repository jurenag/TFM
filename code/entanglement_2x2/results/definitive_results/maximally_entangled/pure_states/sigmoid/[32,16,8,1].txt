#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.6875%
#Sample standard deviation for averaged success rate: 0.0332691439249719%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.697402	0.002687	0.693163	0.000075
2	0.693387	0.000037	0.693538	0.000223
3	0.693293	0.000034	0.693307	0.000143
4	0.693093	0.000110	0.692943	0.000200
5	0.692235	0.000485	0.691765	0.000961
6	0.687145	0.003719	0.680912	0.008000
7	0.665395	0.016986	0.649551	0.026021
8	0.628156	0.034222	0.608639	0.041438
9	0.576391	0.048267	0.545105	0.055383
10	0.505139	0.061554	0.471718	0.066541
11	0.431817	0.069188	0.400285	0.071175
12	0.367119	0.073506	0.347036	0.074883
13	0.324794	0.076822	0.315440	0.076957
14	0.296958	0.077834	0.290957	0.076659
15	0.271588	0.075588	0.263861	0.072976
16	0.241414	0.070204	0.230839	0.066491
17	0.207021	0.063615	0.198202	0.059773
18	0.177344	0.058140	0.174387	0.056222
19	0.157772	0.055710	0.158598	0.054349
20	0.144042	0.053548	0.145208	0.051509
21	0.131835	0.049912	0.134795	0.047139
22	0.121202	0.046271	0.125413	0.044279
23	0.113693	0.044679	0.117803	0.043702
24	0.108095	0.044209	0.114338	0.043362
25	0.103484	0.044117	0.109307	0.043437
26	0.099385	0.044114	0.105574	0.043463
27	0.095653	0.044147	0.100883	0.043750
28	0.092082	0.044161	0.099635	0.043912
29	0.088868	0.044135	0.095432	0.044003
30	0.085934	0.043984	0.092350	0.043665
31	0.082981	0.043731	0.090954	0.043234
32	0.080301	0.043310	0.086405	0.042880
33	0.077570	0.042635	0.084514	0.042228
34	0.074890	0.041704	0.081417	0.041086
35	0.071561	0.040304	0.078501	0.039559
36	0.067978	0.038294	0.073551	0.037270
37	0.063595	0.035434	0.070324	0.033967
38	0.058600	0.031775	0.064318	0.029835
39	0.053065	0.027601	0.058438	0.025468
40	0.047463	0.023315	0.053305	0.021110
41	0.042172	0.019223	0.048360	0.017085
42	0.037560	0.015567	0.044072	0.013586
43	0.033361	0.012375	0.039999	0.010625
44	0.029656	0.009607	0.036460	0.008153
45	0.026412	0.007341	0.034235	0.006467
46	0.023792	0.005624	0.030678	0.004962
47	0.021656	0.004286	0.028019	0.004205
48	0.019787	0.003419	0.026961	0.004146
49	0.018263	0.002909	0.024704	0.003794
50	0.017098	0.002701	0.023267	0.003812
51	0.016101	0.002638	0.024292	0.004059
52	0.015239	0.002674	0.022861	0.004576
53	0.014551	0.002705	0.023122	0.004101
54	0.014026	0.002746	0.020776	0.004210
55	0.013557	0.002737	0.020438	0.004159
56	0.012963	0.002722	0.020158	0.004378
57	0.012636	0.002734	0.020601	0.004387
58	0.012181	0.002712	0.019511	0.004739
59	0.012003	0.002773	0.018660	0.004039
60	0.011593	0.002744	0.018965	0.004368
61	0.011289	0.002761	0.018498	0.004481
62	0.011001	0.002713	0.019248	0.004527
63	0.010683	0.002702	0.018702	0.005094
64	0.010544	0.002718	0.016642	0.004184
65	0.010326	0.002710	0.016863	0.003981
66	0.010040	0.002655	0.017057	0.004083
67	0.009784	0.002634	0.018598	0.004982
68	0.009628	0.002605	0.015615	0.004003
69	0.009426	0.002598	0.016843	0.004387
70	0.009175	0.002562	0.017012	0.004114
71	0.009101	0.002531	0.015501	0.004107
72	0.008988	0.002544	0.016063	0.004461
73	0.008790	0.002466	0.015137	0.004182
74	0.008576	0.002476	0.015872	0.004578
75	0.008581	0.002497	0.016196	0.004579
76	0.008333	0.002460	0.014428	0.003982
77	0.008311	0.002468	0.014977	0.004410
78	0.008171	0.002439	0.014672	0.004222
79	0.008002	0.002406	0.014717	0.004323
80	0.007878	0.002376	0.013572	0.004010
81	0.007711	0.002349	0.014606	0.004119
82	0.007609	0.002365	0.013364	0.003844
83	0.008438	0.002437	0.015068	0.004735
84	0.008271	0.002436	0.015250	0.004127
85	0.008191	0.002440	0.014511	0.004109
86	0.008077	0.002356	0.013651	0.003905
87	0.007986	0.002394	0.014495	0.004039
88	0.007788	0.002340	0.014443	0.004191
89	0.007732	0.002336	0.014753	0.004210
90	0.007686	0.002335	0.013552	0.003738
91	0.007450	0.002281	0.014781	0.004402
92	0.007395	0.002274	0.013449	0.003800
93	0.007320	0.002283	0.014670	0.003958
94	0.008156	0.002360	0.015628	0.003663
95	0.008001	0.002308	0.014185	0.003963
96	0.007904	0.002303	0.015014	0.003965
97	0.008864	0.002354	0.015793	0.003966
98	0.009144	0.002641	0.015622	0.004337
99	0.009050	0.002670	0.018020	0.005022
100	0.008865	0.002601	0.017019	0.005703
