#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7425%
#Sample standard deviation for averaged success rate: 0.030036202115115058%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.710775	0.008557	0.693272	0.000108
2	0.693236	0.000015	0.693304	0.000108
3	0.693186	0.000021	0.693067	0.000079
4	0.693085	0.000048	0.693321	0.000109
5	0.692884	0.000099	0.692676	0.000153
6	0.692423	0.000258	0.692263	0.000387
7	0.691111	0.000772	0.690140	0.001235
8	0.686956	0.002388	0.683428	0.003858
9	0.675723	0.006815	0.667478	0.010043
10	0.653870	0.014933	0.640942	0.019690
11	0.622586	0.025943	0.605866	0.031547
12	0.581428	0.038122	0.560814	0.043492
13	0.533709	0.049650	0.513856	0.053520
14	0.486698	0.058386	0.468525	0.060873
15	0.442927	0.065151	0.429018	0.066674
16	0.404611	0.070432	0.393785	0.070928
17	0.372337	0.074177	0.365853	0.073965
18	0.347041	0.076451	0.343658	0.075366
19	0.325712	0.077334	0.323885	0.075454
20	0.305371	0.076981	0.302956	0.074522
21	0.284146	0.076027	0.282375	0.073597
22	0.265149	0.075435	0.265147	0.073242
23	0.250305	0.075035	0.251794	0.072807
24	0.238606	0.074489	0.240998	0.072330
25	0.228798	0.073714	0.231679	0.071575
26	0.220531	0.072971	0.224982	0.071125
27	0.214084	0.072565	0.219548	0.070934
28	0.208992	0.072437	0.214627	0.071106
29	0.204818	0.072448	0.210833	0.071131
30	0.201259	0.072443	0.207674	0.071154
31	0.198126	0.072391	0.204570	0.071180
32	0.195156	0.072205	0.201635	0.070873
33	0.192092	0.071885	0.198081	0.070581
34	0.188872	0.071369	0.195981	0.070163
35	0.185358	0.070654	0.190519	0.069176
36	0.181241	0.069641	0.186434	0.067967
37	0.176514	0.068327	0.181065	0.066486
38	0.170988	0.066661	0.174827	0.064685
39	0.164990	0.064711	0.168949	0.062494
40	0.158455	0.062615	0.161825	0.060422
41	0.152061	0.060441	0.155424	0.058138
42	0.145768	0.058363	0.148609	0.056327
43	0.139630	0.056484	0.143847	0.054454
44	0.133654	0.054584	0.136680	0.052462
45	0.127061	0.052482	0.129680	0.050230
46	0.119433	0.050022	0.121756	0.047457
47	0.111080	0.047585	0.113141	0.045292
48	0.103013	0.045712	0.105948	0.043728
49	0.095785	0.044506	0.098592	0.042776
50	0.089909	0.043852	0.093361	0.042208
51	0.085144	0.043493	0.089417	0.041879
52	0.081264	0.043270	0.085536	0.041806
53	0.077970	0.043107	0.082059	0.041792
54	0.075020	0.042986	0.079584	0.041650
55	0.072426	0.042856	0.076129	0.041631
56	0.069933	0.042716	0.073524	0.041609
57	0.067853	0.042522	0.071650	0.041429
58	0.065933	0.042341	0.070088	0.041289
59	0.064132	0.042120	0.068109	0.041084
60	0.062511	0.041842	0.066078	0.040905
61	0.060864	0.041484	0.064868	0.040494
62	0.059359	0.041090	0.063008	0.040156
63	0.057955	0.040628	0.062080	0.039684
64	0.056555	0.040120	0.061589	0.039449
65	0.055312	0.039621	0.059232	0.038695
66	0.053948	0.038964	0.058124	0.038155
67	0.052453	0.038079	0.056367	0.037108
68	0.050392	0.036715	0.054656	0.035857
69	0.047558	0.034496	0.053369	0.034298
70	0.043264	0.030747	0.047064	0.029564
71	0.038773	0.026934	0.043097	0.025943
72	0.034252	0.023036	0.038735	0.021479
73	0.028032	0.017471	0.029498	0.014202
74	0.019169	0.009367	0.020129	0.006247
75	0.013152	0.004218	0.016856	0.004075
76	0.010874	0.002681	0.016280	0.003425
77	0.009725	0.002163	0.014919	0.003454
78	0.009092	0.002004	0.013200	0.002955
79	0.008610	0.001902	0.013960	0.003131
80	0.008255	0.001872	0.012884	0.003161
81	0.007866	0.002049	0.012923	0.003495
82	0.007556	0.002039	0.012282	0.003462
83	0.007310	0.001995	0.012033	0.003344
84	0.007106	0.001964	0.012146	0.003711
85	0.006949	0.001953	0.011611	0.003330
86	0.006742	0.001912	0.011057	0.003346
87	0.006550	0.001916	0.011361	0.003489
88	0.006304	0.001890	0.011260	0.003255
89	0.006210	0.001886	0.011243	0.003422
90	0.006066	0.001839	0.010972	0.003400
91	0.005946	0.001849	0.010867	0.003335
92	0.006489	0.001878	0.012095	0.003408
93	0.006465	0.001911	0.011294	0.003424
94	0.007148	0.001900	0.013705	0.003963
95	0.007015	0.001976	0.013136	0.003645
96	0.006896	0.001910	0.012646	0.003586
97	0.006791	0.001946	0.012347	0.003318
98	0.006639	0.001962	0.012925	0.003657
99	0.006386	0.001930	0.012932	0.003774
100	0.006309	0.001916	0.012052	0.003641
