#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.08000000000001%
#Sample standard deviation for averaged success rate: 0.06464054455215759%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.673058	0.004171	0.641656	0.010108
2	0.597821	0.015831	0.551753	0.020992
3	0.505569	0.024680	0.461387	0.028241
4	0.418940	0.030535	0.381263	0.032568
5	0.346437	0.033903	0.317008	0.035051
6	0.288575	0.035887	0.264056	0.036479
7	0.242858	0.037183	0.224729	0.037451
8	0.209494	0.037663	0.197040	0.037769
9	0.185133	0.037684	0.177037	0.037662
10	0.166809	0.037430	0.161349	0.037480
11	0.152325	0.037092	0.149198	0.037178
12	0.140729	0.036664	0.139234	0.036914
13	0.131217	0.036261	0.130798	0.036693
14	0.123092	0.035885	0.123784	0.036522
15	0.115950	0.035572	0.117100	0.036223
16	0.109609	0.035300	0.110663	0.036095
17	0.103936	0.035045	0.104950	0.035627
18	0.098738	0.034756	0.099866	0.035333
19	0.093951	0.034474	0.095893	0.035033
20	0.089289	0.034117	0.091144	0.034794
21	0.085079	0.033844	0.086504	0.034142
22	0.080862	0.033449	0.082557	0.033594
23	0.077309	0.033050	0.079287	0.033363
24	0.074309	0.032759	0.076312	0.033098
25	0.071547	0.032470	0.074208	0.032944
26	0.069319	0.032262	0.071927	0.032824
27	0.067171	0.032044	0.069743	0.032676
28	0.065263	0.031895	0.067949	0.032452
29	0.063681	0.031760	0.066387	0.032283
30	0.062204	0.031648	0.065203	0.032123
31	0.060865	0.031545	0.064419	0.032190
32	0.059504	0.031424	0.063111	0.031928
33	0.058305	0.031264	0.061605	0.031949
34	0.057207	0.031162	0.060719	0.031895
35	0.056181	0.030996	0.060203	0.031999
36	0.055408	0.030975	0.059682	0.031837
37	0.060375	0.033736	0.064849	0.034941
38	0.059552	0.033648	0.063930	0.035023
39	0.058795	0.033599	0.063087	0.035153
40	0.058103	0.033584	0.062262	0.035170
41	0.057519	0.033579	0.061660	0.035221
42	0.056926	0.033490	0.061790	0.035312
43	0.056332	0.033440	0.061808	0.035371
44	0.062690	0.036872	0.067489	0.038971
45	0.023319	0.006387	0.025404	0.006349
46	0.022783	0.006296	0.025539	0.006146
47	0.022459	0.006197	0.024543	0.005898
48	0.022062	0.006071	0.024517	0.005920
49	0.021541	0.005993	0.024467	0.005626
50	0.021233	0.005882	0.023289	0.005522
51	0.020872	0.005815	0.024138	0.005804
52	0.020561	0.005761	0.023075	0.005556
53	0.023225	0.005724	0.026036	0.005732
54	0.022915	0.005687	0.025159	0.005623
55	0.022494	0.005649	0.025071	0.005398
56	0.022181	0.005541	0.024267	0.005521
57	0.021897	0.005481	0.024080	0.005492
58	0.021487	0.005363	0.024129	0.005524
59	0.021235	0.005304	0.023958	0.005358
60	0.024812	0.004636	0.027867	0.004091
61	0.024564	0.004496	0.027885	0.004831
62	0.024270	0.004503	0.027216	0.004440
63	0.023987	0.004472	0.026374	0.004360
64	0.023746	0.004367	0.025871	0.003905
65	0.023465	0.004385	0.027176	0.003681
66	0.023386	0.004242	0.025427	0.003718
67	0.022939	0.004237	0.025555	0.003941
68	0.022806	0.004158	0.024729	0.004014
69	0.025525	0.003931	0.028463	0.003551
70	0.025276	0.003876	0.028346	0.003514
71	0.025157	0.003891	0.028296	0.003471
72	0.028438	0.002870	0.031861	0.002244
73	0.028230	0.002925	0.029747	0.002430
74	0.027936	0.002879	0.030355	0.002486
75	0.027742	0.002893	0.029370	0.002239
76	0.027270	0.002913	0.030322	0.001118
77	0.027138	0.002750	0.030079	0.002043
78	0.027090	0.002861	0.029431	0.001766
79	0.026417	0.002884	0.029761	0.002536
80	0.026358	0.002942	0.028467	0.001660
81	0.025972	0.002783	0.029060	0.001264
82	0.025494	0.002619	0.031054	0.003007
83	0.025221	0.002626	0.029120	0.001077
84	0.027446	0.002165	0.029168	0.002014
85	0.026907	0.001981	0.030247	0.000538
86	0.027059	0.001895	0.028115	0.001389
87	0.026338	0.001772	0.027027	0.001403
88	0.026394	0.001654	0.027009	0.001227
89	0.025982	0.001738	0.026798	0.000748
90	0.025749	0.001462	0.027524	0.000332
91	0.028067	0.000000	0.028623	0.000000
92	0.027461	0.000000	0.029143	0.000000
93	0.027095	0.000000	0.028840	0.000000
94	0.026961	0.000000	0.027676	0.000000
95	0.026373	0.000000	0.028226	0.000000
96	0.026124	0.000000	0.031079	0.000000
97	0.025975	0.000000	0.028579	0.000000
98	0.025855	0.000000	0.027512	0.000000
99	0.025406	0.000000	0.027420	0.000000
