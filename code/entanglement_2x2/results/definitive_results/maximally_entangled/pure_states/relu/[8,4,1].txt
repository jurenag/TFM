#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#separable DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; maximally entangled DMs were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [8, 4, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.97%
#Sample standard deviation for averaged success rate: 0.008647253899355325%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.636437	0.008420	0.557389	0.015989
2	0.448119	0.024059	0.342508	0.029929
3	0.253953	0.031352	0.187266	0.029884
4	0.140912	0.025981	0.107486	0.021528
5	0.079817	0.017944	0.061533	0.014719
6	0.047075	0.011618	0.037567	0.009326
7	0.028825	0.007475	0.023556	0.006119
8	0.018063	0.005025	0.015098	0.004137
9	0.011580	0.003458	0.010045	0.002847
10	0.007674	0.002387	0.007046	0.001950
11	0.005242	0.001661	0.005020	0.001345
12	0.003662	0.001158	0.003729	0.000962
13	0.002618	0.000826	0.002913	0.000712
14	0.001935	0.000603	0.002326	0.000542
15	0.001449	0.000448	0.002056	0.000476
16	0.001153	0.000366	0.001759	0.000411
17	0.000915	0.000299	0.001506	0.000395
18	0.000828	0.000272	0.001340	0.000340
19	0.000682	0.000243	0.001345	0.000360
20	0.000581	0.000221	0.001240	0.000334
21	0.000551	0.000207	0.001114	0.000292
22	0.000500	0.000196	0.001164	0.000308
23	0.000456	0.000184	0.001013	0.000284
24	0.000389	0.000164	0.001079	0.000323
25	0.000355	0.000154	0.001025	0.000301
26	0.000358	0.000152	0.001133	0.000322
27	0.000307	0.000133	0.001079	0.000284
28	0.000399	0.000158	0.001499	0.000435
29	0.000340	0.000137	0.001170	0.000415
30	0.000373	0.000160	0.000823	0.000249
31	0.000361	0.000155	0.000642	0.000264
32	0.000419	0.000169	0.000666	0.000287
33	0.000603	0.000024	0.001060	0.000112
34	0.000614	0.000054	0.000923	0.000213
35	0.000560	0.000047	0.001141	0.000380
36	0.000570	0.000038	0.000658	0.000105
37	0.000561	0.000000	0.000541	0.000000
38	0.000484	0.000000	0.000574	0.000000
39	0.000547	0.000000	0.000464	0.000000
