#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [64, 32, 16, 8, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.745%
#Sample standard deviation for averaged success rate: 0.02951683841470314%
#Same average success rate for supplementary tests: [0.501015, 0.50174, 0.507455, 0.5382899999999999, 0.624815, 0.576815, 0.5860299999999999, 0.612725, 0.6418400000000001, 0.68217]%
#Sample STD for averaged success rate in supplementary tests: [0.003500710426292069, 0.0034963407471240555, 0.0034611119815385924, 0.0032670160383750793, 0.002742751499634993, 0.00306859866857007, 0.0030247713888821425, 0.0028570927039124933, 0.002660943577004217, 0.0024117741509104853]%
#Epoch	Loss	Sample STD
1	0.703184	0.005069
2	0.693407	0.000034
3	0.693409	0.000039
4	0.693352	0.000032
5	0.693301	0.000051
6	0.691889	0.000671
7	0.672255	0.010461
8	0.617433	0.038882
9	0.554677	0.058270
10	0.473226	0.067783
11	0.394165	0.078109
12	0.353645	0.081787
13	0.313021	0.078867
14	0.282723	0.078233
15	0.265905	0.078443
16	0.253042	0.078295
17	0.242132	0.077269
18	0.234018	0.076780
19	0.227925	0.076560
20	0.223041	0.076246
21	0.218547	0.076019
22	0.214496	0.075885
23	0.211336	0.075851
24	0.208810	0.075855
25	0.206536	0.075905
26	0.204117	0.075960
27	0.202125	0.075884
28	0.199559	0.075774
29	0.196875	0.075167
30	0.191930	0.073265
31	0.184869	0.070779
32	0.181586	0.069851
33	0.179445	0.069495
34	0.177211	0.069149
35	0.175140	0.068560
36	0.172138	0.067643
37	0.167316	0.066188
38	0.161083	0.064556
39	0.155909	0.063383
40	0.151874	0.062697
41	0.148529	0.062088
42	0.145459	0.061691
43	0.142433	0.061298
44	0.139054	0.060965
45	0.136761	0.060648
46	0.133511	0.060524
47	0.130449	0.060333
48	0.127542	0.060299
49	0.126045	0.060040
50	0.124075	0.059884
51	0.121943	0.059726
52	0.120024	0.059549
53	0.118361	0.058925
54	0.115331	0.057968
55	0.103693	0.052739
56	0.090605	0.048816
57	0.084069	0.047683
58	0.078872	0.047203
59	0.075778	0.046781
60	0.072791	0.045790
61	0.068708	0.043753
62	0.064212	0.040885
63	0.060899	0.038629
64	0.058847	0.036741
65	0.056023	0.035256
66	0.053978	0.033662
67	0.051529	0.031872
68	0.049103	0.029661
69	0.045292	0.026951
70	0.041425	0.023594
71	0.037245	0.019868
72	0.032802	0.015968
73	0.028936	0.012564
74	0.025713	0.009614
75	0.023010	0.007069
76	0.019860	0.005038
77	0.018443	0.003760
78	0.016875	0.002650
79	0.016225	0.002395
80	0.014823	0.002169
81	0.014307	0.002128
82	0.013812	0.002114
83	0.014010	0.002637
84	0.013419	0.002418
85	0.013037	0.002273
86	0.012950	0.002442
87	0.012459	0.002362
88	0.012856	0.002663
89	0.012344	0.002448
90	0.012428	0.002523
91	0.012182	0.002446
92	0.012172	0.002626
93	0.012343	0.002769
94	0.012415	0.002772
95	0.011558	0.002507
96	0.011760	0.002495
97	0.011669	0.002687
98	0.012081	0.002775
99	0.011715	0.002748
100	0.011310	0.002566
101	0.011567	0.002554
102	0.011527	0.002808
103	0.010966	0.002531
104	0.011176	0.002536
105	0.011256	0.002872
106	0.011394	0.002788
107	0.010353	0.002335
108	0.010984	0.002634
109	0.010944	0.002558
110	0.011110	0.002735
111	0.011240	0.002860
112	0.010759	0.002714
113	0.010592	0.002567
114	0.010510	0.002595
115	0.010630	0.002619
116	0.010211	0.002511
117	0.011108	0.003037
118	0.009968	0.002456
119	0.010177	0.002510
120	0.010633	0.002615
121	0.010336	0.002576
122	0.010280	0.002421
123	0.010438	0.002565
124	0.009719	0.002339
125	0.009560	0.002430
126	0.010116	0.002675
127	0.010386	0.002767
128	0.009973	0.002715
129	0.009867	0.002602
130	0.009698	0.002543
131	0.009549	0.002473
132	0.010125	0.002779
133	0.009516	0.002476
134	0.009535	0.002500
135	0.009580	0.002614
136	0.009897	0.002800
137	0.009552	0.002508
138	0.009617	0.002456
139	0.010234	0.003255
140	0.008988	0.002322
141	0.009670	0.002835
142	0.009165	0.002460
143	0.009262	0.002575
144	0.009349	0.002600
145	0.009387	0.002627
146	0.009309	0.002635
147	0.008912	0.002502
148	0.008926	0.002349
149	0.009163	0.002516
150	0.008895	0.002434
151	0.008666	0.002554
