#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.895%
#Sample standard deviation for averaged success rate: 0.01724075839399532%
#Same average success rate for supplementary tests: [0.50047, 0.501125, 0.5101499999999999, 0.5823, 0.7623049999999999, 0.57302, 0.6131449999999999, 0.6824349999999999, 0.76826, 0.8441700000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.0035214966924590457, 0.0035174865626964374, 0.003462058473654078, 0.0030390681960100864, 0.001983199271064309, 0.0031420461454281672, 0.0029442037036777878, 0.002586106830498308, 0.0020280972905657156, 0.0014212144648152126]%
#Epoch	Loss	Sample STD
1	0.705613	0.005456
2	0.693234	0.000019
3	0.693211	0.000024
4	0.693184	0.000018
5	0.693089	0.000049
6	0.692879	0.000115
7	0.692423	0.000347
8	0.690798	0.001208
9	0.685447	0.004221
10	0.671651	0.011910
11	0.647139	0.024636
12	0.617687	0.038755
13	0.591846	0.050210
14	0.572744	0.057995
15	0.558810	0.062584
16	0.547798	0.065021
17	0.538001	0.066348
18	0.528299	0.066874
19	0.517428	0.066794
20	0.504876	0.066146
21	0.490040	0.064989
22	0.472618	0.063872
23	0.453814	0.063930
24	0.437455	0.065104
25	0.424955	0.066440
26	0.414947	0.067380
27	0.406199	0.067998
28	0.397978	0.068352
29	0.390058	0.068389
30	0.382600	0.068265
31	0.375673	0.067965
32	0.369007	0.067546
33	0.362275	0.066963
34	0.355287	0.066197
35	0.347555	0.065230
36	0.338514	0.063895
37	0.328312	0.062382
38	0.317598	0.060817
39	0.306916	0.059371
40	0.295989	0.057922
41	0.284904	0.056518
42	0.273571	0.055190
43	0.261629	0.053919
44	0.249023	0.052746
45	0.236145	0.051740
46	0.223724	0.050953
47	0.211794	0.050288
48	0.200054	0.049644
49	0.188440	0.048971
50	0.176985	0.048213
51	0.165595	0.047234
52	0.154328	0.045978
53	0.143217	0.044565
54	0.132684	0.043149
55	0.123048	0.041870
56	0.114493	0.040807
57	0.106609	0.039839
58	0.099280	0.038749
59	0.092175	0.037416
60	0.084396	0.035132
61	0.075400	0.031343
62	0.064689	0.025810
63	0.056822	0.022412
64	0.050063	0.019736
65	0.043718	0.017076
66	0.036914	0.013767
67	0.028054	0.008348
68	0.022712	0.005877
69	0.019572	0.004849
70	0.017046	0.004169
71	0.015120	0.003687
72	0.013517	0.003354
73	0.012233	0.003106
74	0.011134	0.002904
75	0.010238	0.002750
76	0.009457	0.002645
77	0.008840	0.002547
78	0.008259	0.002481
79	0.007699	0.002398
80	0.007256	0.002346
81	0.006863	0.002315
82	0.006522	0.002319
83	0.006222	0.002279
84	0.005908	0.002245
85	0.005650	0.002216
86	0.005430	0.002228
87	0.005156	0.002197
88	0.004968	0.002161
89	0.004821	0.002159
90	0.004647	0.002141
91	0.004527	0.002134
92	0.004338	0.002122
93	0.004262	0.002100
94	0.004107	0.002078
95	0.004044	0.002083
96	0.003955	0.002075
97	0.003853	0.002061
98	0.003732	0.002031
99	0.003693	0.002036
100	0.003588	0.001984
101	0.003532	0.002000
102	0.003464	0.001983
103	0.003407	0.001968
104	0.003373	0.001955
105	0.003330	0.001969
106	0.003209	0.001928
107	0.003203	0.001925
108	0.003136	0.001888
109	0.003081	0.001890
110	0.003027	0.001854
111	0.003008	0.001861
112	0.002974	0.001871
113	0.002958	0.001835
114	0.002952	0.001850
115	0.002864	0.001831
116	0.002920	0.001848
117	0.002860	0.001812
118	0.002853	0.001803
119	0.002787	0.001787
120	0.002765	0.001769
121	0.002726	0.001779
122	0.002658	0.001745
123	0.002676	0.001776
124	0.002719	0.001776
125	0.002578	0.001728
126	0.002629	0.001743
127	0.002569	0.001718
128	0.002585	0.001709
129	0.002513	0.001682
130	0.002488	0.001661
131	0.002528	0.001690
132	0.002486	0.001675
133	0.002448	0.001652
134	0.002441	0.001681
135	0.002386	0.001656
136	0.002435	0.001667
137	0.002400	0.001616
138	0.002383	0.001639
139	0.002390	0.001605
140	0.002342	0.001612
141	0.002327	0.001597
142	0.002309	0.001590
143	0.002305	0.001599
144	0.002254	0.001559
145	0.002316	0.001585
146	0.002268	0.001568
147	0.002215	0.001547
148	0.002237	0.001577
149	0.002248	0.001549
150	0.002230	0.001554
151	0.002185	0.001516
