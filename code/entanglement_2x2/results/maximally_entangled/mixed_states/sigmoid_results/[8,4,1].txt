#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [8, 4, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.6825%
#Sample standard deviation for averaged success rate: 0.029471145846396287%
#Same average success rate for supplementary tests: [0.5012800000000001, 0.50234, 0.51139, 0.56909, 0.7285, 0.565215, 0.5768300000000001, 0.615525, 0.7119949999999999, 0.816585]%
#Sample STD for averaged success rate in supplementary tests: [0.0034895870930527007, 0.0034830484090807587, 0.0034280845081473706, 0.0030754395775238388, 0.0022121228492106835, 0.003139542194134361, 0.0030971692809725455, 0.002914699593225689, 0.0023896089635649606, 0.0016355035581587715]%
#Epoch	Loss	Sample STD
1	0.706823	0.009002
2	0.693203	0.000032
3	0.693141	0.000045
4	0.693035	0.000078
5	0.692813	0.000152
6	0.692447	0.000303
7	0.691674	0.000636
8	0.690101	0.001348
9	0.686716	0.002871
10	0.679990	0.005820
11	0.668468	0.010828
12	0.651336	0.018260
13	0.629142	0.027452
14	0.604281	0.036942
15	0.579487	0.045725
16	0.556837	0.053172
17	0.536382	0.059423
18	0.517858	0.064636
19	0.501017	0.068776
20	0.484904	0.071858
21	0.468614	0.073857
22	0.451516	0.074926
23	0.433268	0.075229
24	0.414063	0.075045
25	0.393964	0.074457
26	0.373982	0.073544
27	0.354424	0.072313
28	0.335255	0.070734
29	0.316654	0.068844
30	0.298836	0.066728
31	0.281873	0.064423
32	0.265993	0.062110
33	0.251072	0.059860
34	0.237124	0.057563
35	0.223685	0.055203
36	0.211001	0.052874
37	0.198850	0.050598
38	0.187269	0.048489
39	0.176430	0.046581
40	0.166461	0.044941
41	0.157476	0.043572
42	0.149150	0.042389
43	0.141432	0.041349
44	0.134418	0.040377
45	0.127986	0.039468
46	0.122034	0.038609
47	0.116399	0.037720
48	0.111091	0.036853
49	0.106134	0.035929
50	0.101424	0.035010
51	0.096956	0.034072
52	0.092668	0.033164
53	0.088663	0.032243
54	0.084799	0.031344
55	0.081198	0.030486
56	0.077748	0.029605
57	0.074453	0.028770
58	0.071348	0.027910
59	0.068377	0.027111
60	0.065594	0.026301
61	0.062928	0.025478
62	0.060320	0.024669
63	0.057905	0.023868
64	0.055627	0.023056
65	0.053436	0.022234
66	0.051354	0.021403
67	0.049356	0.020557
68	0.047345	0.019666
69	0.045401	0.018761
70	0.043542	0.017863
71	0.041725	0.016939
72	0.039919	0.016004
73	0.038188	0.015055
74	0.036471	0.014121
75	0.034754	0.013175
76	0.033132	0.012271
77	0.031597	0.011411
78	0.030063	0.010580
79	0.028592	0.009763
80	0.027151	0.009006
81	0.025830	0.008337
82	0.024607	0.007762
83	0.023440	0.007267
84	0.022392	0.006856
85	0.021466	0.006526
86	0.020604	0.006275
87	0.019805	0.006058
88	0.019076	0.005902
89	0.018417	0.005772
90	0.017783	0.005655
91	0.017185	0.005586
92	0.016698	0.005521
93	0.016182	0.005477
94	0.015745	0.005461
95	0.015312	0.005423
96	0.014893	0.005412
97	0.014601	0.005406
98	0.014223	0.005392
99	0.013936	0.005395
100	0.013644	0.005384
101	0.013360	0.005364
102	0.013152	0.005378
103	0.012923	0.005369
104	0.012724	0.005357
105	0.012512	0.005338
106	0.012367	0.005359
107	0.012161	0.005328
108	0.012060	0.005333
109	0.011867	0.005301
110	0.011752	0.005296
111	0.011610	0.005273
112	0.011496	0.005271
113	0.011353	0.005239
114	0.011284	0.005240
115	0.011167	0.005217
116	0.011060	0.005217
117	0.010954	0.005190
118	0.010856	0.005163
119	0.010744	0.005141
120	0.010699	0.005143
121	0.010601	0.005118
122	0.010542	0.005102
123	0.010461	0.005078
124	0.010385	0.005066
125	0.010307	0.005048
126	0.010215	0.005017
127	0.010135	0.005008
128	0.010102	0.005002
129	0.010036	0.004968
130	0.009930	0.004943
131	0.009902	0.004954
132	0.009860	0.004921
133	0.009780	0.004908
134	0.009755	0.004893
135	0.009673	0.004873
136	0.009570	0.004835
137	0.009531	0.004821
138	0.009487	0.004818
139	0.009460	0.004792
140	0.009373	0.004765
141	0.009354	0.004780
142	0.009278	0.004747
143	0.009269	0.004747
144	0.009202	0.004715
145	0.009139	0.004688
146	0.009108	0.004689
147	0.009021	0.004666
148	0.009012	0.004656
149	0.008922	0.004634
150	0.008944	0.004628
151	0.008879	0.004613
