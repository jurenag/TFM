#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.8025%
#Sample standard deviation for averaged success rate: 0.02591328689109713%
#Same average success rate for supplementary tests: [0.50075, 0.50173, 0.503765, 0.512365, 0.5775650000000001, 0.565025, 0.566615, 0.577535, 0.60015, 0.64787]%
#Sample STD for averaged success rate in supplementary tests: [0.0035133775594148715, 0.0035074777768362266, 0.0034956639195938156, 0.0034493884006806183, 0.0031659530221956854, 0.0031969849028029526, 0.003198381948540543, 0.0031505937835192275, 0.0030501063710959334, 0.002843118561544701]%
#Epoch	Loss	Sample STD
1	0.695851	0.001667
2	0.693414	0.000036
3	0.693323	0.000032
4	0.693200	0.000064
5	0.692883	0.000158
6	0.691378	0.000671
7	0.683199	0.003537
8	0.649364	0.014634
9	0.579591	0.035517
10	0.500874	0.055300
11	0.430823	0.068350
12	0.379496	0.076149
13	0.338886	0.076543
14	0.294510	0.071595
15	0.249881	0.066266
16	0.214353	0.058370
17	0.175603	0.043387
18	0.136233	0.025426
19	0.113301	0.016327
20	0.100901	0.012670
21	0.092314	0.010692
22	0.085531	0.009400
23	0.080079	0.008498
24	0.075229	0.007784
25	0.070838	0.007284
26	0.067142	0.006815
27	0.063443	0.006373
28	0.060160	0.005967
29	0.057064	0.005612
30	0.054000	0.005260
31	0.051373	0.004966
32	0.048565	0.004678
33	0.045805	0.004516
34	0.043240	0.004363
35	0.041079	0.004285
36	0.038933	0.004235
37	0.036859	0.004210
38	0.035339	0.004140
39	0.033743	0.004131
40	0.032267	0.004131
41	0.030871	0.004085
42	0.029728	0.004085
43	0.028679	0.004039
44	0.027572	0.003984
45	0.026591	0.003939
46	0.025649	0.003917
47	0.024817	0.003863
48	0.023969	0.003794
49	0.023240	0.003739
50	0.022487	0.003647
51	0.021736	0.003535
52	0.021130	0.003471
53	0.020478	0.003394
54	0.019752	0.003263
55	0.019042	0.003126
56	0.018470	0.003056
57	0.017835	0.002881
58	0.017348	0.002818
59	0.016691	0.002651
60	0.016074	0.002514
61	0.015647	0.002477
62	0.015087	0.002360
63	0.014631	0.002284
64	0.014088	0.002214
65	0.013721	0.002136
66	0.013365	0.002080
67	0.012938	0.002056
68	0.012612	0.002011
69	0.012241	0.001948
70	0.012072	0.001929
71	0.011644	0.001899
72	0.011469	0.001899
73	0.011101	0.001814
74	0.010834	0.001810
75	0.010520	0.001739
76	0.010443	0.001793
77	0.010060	0.001771
78	0.009701	0.001678
79	0.009766	0.001759
80	0.009391	0.001749
81	0.009071	0.001712
82	0.009048	0.001658
83	0.008802	0.001652
84	0.008564	0.001672
85	0.008429	0.001663
86	0.008269	0.001650
87	0.008156	0.001680
88	0.008046	0.001660
89	0.007712	0.001655
90	0.007689	0.001644
91	0.007546	0.001657
92	0.007339	0.001631
93	0.007197	0.001609
94	0.007091	0.001587
95	0.006965	0.001599
96	0.006844	0.001601
97	0.006729	0.001615
98	0.006647	0.001578
99	0.006553	0.001640
100	0.006321	0.001533
101	0.006323	0.001575
102	0.006136	0.001553
103	0.006117	0.001576
104	0.005890	0.001529
105	0.005777	0.001509
106	0.005765	0.001518
107	0.005762	0.001565
108	0.005657	0.001570
109	0.005512	0.001522
110	0.005390	0.001487
111	0.005272	0.001468
112	0.005322	0.001506
113	0.005170	0.001481
114	0.005046	0.001443
115	0.005001	0.001502
116	0.004917	0.001453
117	0.004880	0.001448
118	0.004880	0.001407
119	0.004752	0.001421
120	0.004678	0.001380
121	0.004617	0.001406
122	0.004592	0.001405
123	0.004402	0.001333
124	0.004365	0.001372
125	0.004372	0.001410
126	0.004318	0.001368
127	0.004162	0.001356
128	0.004117	0.001304
129	0.003963	0.001274
130	0.004087	0.001341
131	0.003967	0.001323
132	0.003917	0.001248
133	0.003969	0.001323
134	0.003804	0.001276
135	0.003785	0.001307
136	0.003795	0.001321
137	0.003648	0.001291
138	0.003522	0.001256
139	0.003680	0.001283
140	0.003582	0.001286
141	0.003463	0.001196
142	0.003485	0.001289
143	0.003461	0.001287
144	0.003415	0.001227
145	0.003398	0.001257
146	0.003409	0.001258
147	0.003278	0.001221
148	0.003305	0.001250
149	0.003214	0.001225
150	0.003212	0.001239
151	0.003141	0.001165
