#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 97.48500000000001%
#Sample standard deviation for averaged success rate: 0.09820231030887044%
#Same average success rate for supplementary tests: [0.5029950000000001, 0.51521, 0.5396850000000001, 0.589565, 0.71541, 0.55535, 0.5760649999999999, 0.615675, 0.6843299999999999, 0.7906150000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.0033025636252387324, 0.0032310884845512975, 0.0030837688043609873, 0.0027926565379133178, 0.002237906744035596, 0.003063987414301828, 0.002965135880318135, 0.0027701605943970106, 0.0024396357422779356, 0.0019168518692768096]%
#Epoch	Loss	Sample STD
1	0.701385	0.003384
2	0.693262	0.000050
3	0.693146	0.000036
4	0.693115	0.000034
5	0.693015	0.000044
6	0.692974	0.000052
7	0.692887	0.000069
8	0.692798	0.000091
9	0.692667	0.000121
10	0.692488	0.000172
11	0.692249	0.000245
12	0.691895	0.000372
13	0.691406	0.000558
14	0.690697	0.000835
15	0.689724	0.001230
16	0.688469	0.001748
17	0.686749	0.002416
18	0.684577	0.003213
19	0.681893	0.004153
20	0.678533	0.005193
21	0.674489	0.006311
22	0.669608	0.007503
23	0.663873	0.008729
24	0.657046	0.010064
25	0.648976	0.011586
26	0.639446	0.013386
27	0.628447	0.015488
28	0.615919	0.017908
29	0.601247	0.020781
30	0.584383	0.024079
31	0.565366	0.027831
32	0.544469	0.031961
33	0.522742	0.036010
34	0.500609	0.039734
35	0.478835	0.042766
36	0.457436	0.045091
37	0.435938	0.046715
38	0.414162	0.047719
39	0.392279	0.048182
40	0.370628	0.048316
41	0.349947	0.048157
42	0.330405	0.047726
43	0.312380	0.047051
44	0.295716	0.046200
45	0.280501	0.045281
46	0.266875	0.044394
47	0.254585	0.043414
48	0.243294	0.042402
49	0.233001	0.041350
50	0.223602	0.040331
51	0.214936	0.039347
52	0.206918	0.038363
53	0.199540	0.037463
54	0.192767	0.036664
55	0.186538	0.035954
56	0.180816	0.035350
57	0.175605	0.034839
58	0.170775	0.034384
59	0.166230	0.033993
60	0.162011	0.033647
61	0.158016	0.033345
62	0.154250	0.033059
63	0.150799	0.032816
64	0.147519	0.032575
65	0.144402	0.032354
66	0.141446	0.032124
67	0.138695	0.031916
68	0.136069	0.031712
69	0.133599	0.031518
70	0.131259	0.031321
71	0.128949	0.031132
72	0.126839	0.030934
73	0.124750	0.030740
74	0.122822	0.030538
75	0.120933	0.030336
76	0.119164	0.030150
77	0.117400	0.029942
78	0.115767	0.029771
79	0.114108	0.029560
80	0.112575	0.029393
81	0.111072	0.029180
82	0.109617	0.028980
83	0.108218	0.028780
84	0.106820	0.028585
85	0.105433	0.028354
86	0.104205	0.028204
87	0.102905	0.027993
88	0.101630	0.027785
89	0.100455	0.027580
90	0.099223	0.027381
91	0.098036	0.027183
92	0.096962	0.026982
93	0.095783	0.026793
94	0.094781	0.026610
95	0.093726	0.026435
96	0.092667	0.026222
97	0.091725	0.026069
98	0.090733	0.025896
99	0.089782	0.025709
100	0.088795	0.025542
101	0.087894	0.025383
102	0.087001	0.025222
103	0.086126	0.025060
104	0.085288	0.024924
105	0.084478	0.024771
106	0.083624	0.024627
107	0.082842	0.024496
108	0.082093	0.024377
109	0.081399	0.024250
110	0.080608	0.024132
111	0.080035	0.024035
112	0.079303	0.023929
113	0.078650	0.023812
114	0.078043	0.023740
115	0.077450	0.023648
116	0.076863	0.023555
117	0.076303	0.023484
118	0.075742	0.023419
119	0.075238	0.023350
120	0.074758	0.023262
121	0.074219	0.023217
122	0.073747	0.023151
123	0.073332	0.023096
124	0.072878	0.023053
125	0.072445	0.022997
126	0.072000	0.022927
127	0.071606	0.022888
128	0.071186	0.022838
129	0.070817	0.022794
130	0.070433	0.022748
131	0.069986	0.022708
132	0.069730	0.022648
133	0.069367	0.022616
134	0.069004	0.022563
135	0.068639	0.022524
136	0.068314	0.022482
137	0.068007	0.022451
138	0.067586	0.022389
139	0.067291	0.022365
140	0.066978	0.022305
141	0.066638	0.022274
142	0.066320	0.022241
143	0.066028	0.022202
144	0.065670	0.022150
145	0.065351	0.022095
146	0.065029	0.022070
147	0.064777	0.022027
148	0.064492	0.021976
149	0.064109	0.021923
150	0.063842	0.021900
151	0.063514	0.021843
