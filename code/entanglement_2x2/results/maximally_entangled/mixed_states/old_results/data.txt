#Tensor product hilbert space dimension: 4; Number of simulations: 2;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglemed_2x2_merge/input_data/received_from_DM/mixed_states/mixed_separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglemed_2x2_merge/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.6125%
#Sample standard deviation for averaged success rate: 0.3844968749999955%
#Epoch	Loss	Sample STD
1	0.697393	0.003760
2	0.693216	0.000019
3	0.693190	0.000068
4	0.693127	0.000144
5	0.692912	0.000333
6	0.692312	0.001135
7	0.689738	0.004542
8	0.678649	0.019998
9	0.646123	0.065241
10	0.591756	0.140819
11	0.534809	0.218154
12	0.487653	0.277552
13	0.450297	0.315845
14	0.419489	0.337291
15	0.395026	0.349585
16	0.376838	0.356853
17	0.361189	0.358183
18	0.343672	0.351371
19	0.324746	0.337794
20	0.308588	0.324663
21	0.297001	0.315887
22	0.289639	0.312252
23	0.284681	0.310993
24	0.281392	0.310583
25	0.278330	0.310694
26	0.275993	0.311015
27	0.273730	0.310888
28	0.271324	0.310526
29	0.269273	0.309955
30	0.266685	0.309124
31	0.264215	0.307344
32	0.261240	0.304993
33	0.257768	0.302086
34	0.253659	0.297773
35	0.248650	0.291825
36	0.242021	0.284195
37	0.233644	0.273320
38	0.223368	0.259772
39	0.210826	0.243047
40	0.197038	0.224625
41	0.182130	0.204585
42	0.168281	0.185697
43	0.154874	0.167857
44	0.142564	0.151027
45	0.131006	0.136065
46	0.120369	0.121594
47	0.110008	0.107530
48	0.099487	0.093340
49	0.089434	0.079885
50	0.079610	0.067335
51	0.070551	0.054432
52	0.061940	0.043158
53	0.054332	0.032935
54	0.047179	0.023655
55	0.041507	0.015046
56	0.035837	0.008040
57	0.031100	0.001973
58	0.027158	0.002969
59	0.024120	0.007563
60	0.021323	0.010652
61	0.019640	0.013223
62	0.018094	0.014875
63	0.016822	0.015798
64	0.016176	0.016755
65	0.015166	0.016851
66	0.015012	0.017592
67	0.014398	0.017546
68	0.014438	0.018268
69	0.013848	0.017918
70	0.013583	0.017826
71	0.013580	0.018142
72	0.012946	0.017460
73	0.013100	0.017837
74	0.012975	0.017804
75	0.012661	0.017479
76	0.012650	0.017545
77	0.012580	0.017522
78	0.012326	0.017220
79	0.012274	0.017197
80	0.012248	0.017204
81	0.012210	0.017178
82	0.011998	0.016897
83	0.011861	0.016724
84	0.011564	0.016318
85	0.011668	0.016472
86	0.011667	0.016477
87	0.011479	0.016218
88	0.011075	0.015651
89	0.011213	0.015849
90	0.011190	0.015818
91	0.010933	0.015458
92	0.011018	0.015578
93	0.010988	0.015537
94	0.010743	0.015192
95	0.010893	0.015404
96	0.010731	0.015175
97	0.010504	0.014855
98	0.010440	0.014763
99	0.010550	0.014920
100	0.010303	0.014570
101	0.010366	0.014659
102	0.010108	0.014294
103	0.009971	0.014101
104	0.009842	0.013918
105	0.010106	0.014292
106	0.009838	0.013913
107	0.009894	0.013992
108	0.009809	0.013873
109	0.009970	0.014099
110	0.010062	0.014230
111	0.009726	0.013755
112	0.009799	0.013857
113	0.009674	0.013681
114	0.009417	0.013318
115	0.009478	0.013404
116	0.009512	0.013451
117	0.009449	0.013363
118	0.009383	0.013269
119	0.009559	0.013519
120	0.009349	0.013222
121	0.009129	0.012910
122	0.009147	0.012935
123	0.009119	0.012896
124	0.009033	0.012774
125	0.008983	0.012703
126	0.008917	0.012610
127	0.009160	0.012955
128	0.008953	0.012661
129	0.008910	0.012601
130	0.008949	0.012656
131	0.008721	0.012334
132	0.008870	0.012544
133	0.008600	0.012162
134	0.008734	0.012352
135	0.008704	0.012310
136	0.008731	0.012348
137	0.008675	0.012268
138	0.008534	0.012068
139	0.008546	0.012087
140	0.008679	0.012274
141	0.007948	0.011240
142	0.008389	0.011864
143	0.008245	0.011660
144	0.008362	0.011826
145	0.008458	0.011962
146	0.008566	0.012115
147	0.008242	0.011656
148	0.008216	0.011619
149	0.008217	0.011621
150	0.008120	0.011483
151	0.008249	0.011666
152	0.008125	0.011491
153	0.008235	0.011646
154	0.008062	0.011402
155	0.007984	0.011290
156	0.008012	0.011330
157	0.008169	0.011553
158	0.008218	0.011623
159	0.007837	0.011083
160	0.007938	0.011226
161	0.007800	0.011032
162	0.007700	0.010890
163	0.007751	0.010962
164	0.007690	0.010876
165	0.007700	0.010889
166	0.007983	0.011290
167	0.007922	0.011203
168	0.007759	0.010974
169	0.007962	0.011260
170	0.007785	0.011010
171	0.007684	0.010866
172	0.007733	0.010936
173	0.007762	0.010977
174	0.007934	0.011221
175	0.007678	0.010858
176	0.007564	0.010697
177	0.007665	0.010841
178	0.007613	0.010766
179	0.007661	0.010834
180	0.007497	0.010603
181	0.007507	0.010617
182	0.007141	0.010098
183	0.007495	0.010600
184	0.007696	0.010883
185	0.007213	0.010201
186	0.007493	0.010597
187	0.007378	0.010434
188	0.007411	0.010481
189	0.007219	0.010209
190	0.007090	0.010027
191	0.007348	0.010391
192	0.007654	0.010825
193	0.007194	0.010173
194	0.007361	0.010410
195	0.007332	0.010369
196	0.007323	0.010357
197	0.007151	0.010113
198	0.007153	0.010116
199	0.006984	0.009877
200	0.007325	0.010359
