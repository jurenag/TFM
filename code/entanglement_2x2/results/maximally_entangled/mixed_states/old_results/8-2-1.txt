#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_separable.txt; Entangled DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [8, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.16400000000002%
#Sample standard deviation for averaged success rate: 0.012410141414105751%
#Epoch	Loss	Sample STD
1	0.702024	0.013933
2	0.693196	0.000094
3	0.693097	0.000126
4	0.692993	0.000221
5	0.692824	0.000409
6	0.692527	0.000838
7	0.691992	0.001756
8	0.690925	0.003723
9	0.688881	0.007731
10	0.685193	0.014846
11	0.679494	0.025076
12	0.671803	0.037361
13	0.662478	0.050665
14	0.651802	0.064656
15	0.639903	0.078953
16	0.626927	0.093032
17	0.612961	0.106487
18	0.598194	0.119048
19	0.582856	0.130500
20	0.567206	0.140741
21	0.551426	0.149694
22	0.535530	0.157589
23	0.519709	0.164479
24	0.503845	0.170439
25	0.487878	0.175598
26	0.471836	0.180013
27	0.455699	0.183740
28	0.439511	0.186889
29	0.423363	0.189595
30	0.407401	0.192012
31	0.391828	0.194377
32	0.376939	0.196671
33	0.362858	0.198753
34	0.349548	0.200441
35	0.336949	0.201727
36	0.325115	0.202495
37	0.313952	0.202656
38	0.303342	0.202294
39	0.293201	0.201428
40	0.283366	0.200130
41	0.273827	0.198501
42	0.264589	0.196704
43	0.255697	0.194907
44	0.247192	0.193155
45	0.239047	0.191474
46	0.231323	0.189801
47	0.223936	0.188101
48	0.216883	0.186343
49	0.210110	0.184496
50	0.203562	0.182537
51	0.197232	0.180514
52	0.191057	0.178404
53	0.184997	0.176190
54	0.178984	0.173854
55	0.173064	0.171546
56	0.167296	0.169323
57	0.161807	0.167327
58	0.156565	0.165451
59	0.151530	0.163666
60	0.146692	0.161898
61	0.142001	0.160128
62	0.137429	0.158325
63	0.132994	0.156533
64	0.128680	0.154767
65	0.124506	0.153070
66	0.120532	0.151491
67	0.116748	0.149983
68	0.113114	0.148522
69	0.109640	0.147093
70	0.106271	0.145629
71	0.103013	0.144153
72	0.099876	0.142646
73	0.096792	0.141047
74	0.093741	0.139293
75	0.090600	0.137239
76	0.087372	0.134908
77	0.084049	0.132452
78	0.080813	0.130284
79	0.077874	0.128437
80	0.075112	0.126590
81	0.072466	0.124621
82	0.069955	0.122644
83	0.067576	0.120703
84	0.065270	0.118679
85	0.062970	0.116481
86	0.060589	0.114031
87	0.058109	0.111445
88	0.055568	0.108986
89	0.053155	0.106933
90	0.051020	0.105231
91	0.049175	0.103623
92	0.047483	0.101909
93	0.045924	0.100047
94	0.044396	0.097987
95	0.042915	0.095855
96	0.041510	0.093763
97	0.040167	0.091761
98	0.038888	0.089886
99	0.037694	0.088101
100	0.036492	0.086207
101	0.035276	0.084108
102	0.034070	0.082049
103	0.032911	0.079920
104	0.031683	0.077624
105	0.030373	0.075389
106	0.029153	0.073687
107	0.028103	0.072538
108	0.027165	0.071732
109	0.026351	0.071122
110	0.025622	0.070630
111	0.024949	0.070226
112	0.024331	0.069881
113	0.023738	0.069590
114	0.023174	0.069331
115	0.022645	0.069104
116	0.022124	0.068920
117	0.021656	0.068775
118	0.021220	0.068666
119	0.020806	0.068577
120	0.020443	0.068516
121	0.020128	0.068467
122	0.019815	0.068425
123	0.019530	0.068404
124	0.019258	0.068382
125	0.019005	0.068366
126	0.018796	0.068344
127	0.018572	0.068341
128	0.018379	0.068327
129	0.018193	0.068320
130	0.018024	0.068311
131	0.017848	0.068300
132	0.017699	0.068293
133	0.017559	0.068288
134	0.017417	0.068281
135	0.017295	0.068272
136	0.017158	0.068267
137	0.017038	0.068254
138	0.016928	0.068253
139	0.016810	0.068243
140	0.016698	0.068236
141	0.016601	0.068232
142	0.016498	0.068221
143	0.016393	0.068219
144	0.016293	0.068209
145	0.016206	0.068205
146	0.016117	0.068197
147	0.016024	0.068191
148	0.015943	0.068177
149	0.015849	0.068174
150	0.015768	0.068165
151	0.015693	0.068162
152	0.015607	0.068152
153	0.015543	0.068136
154	0.015465	0.068141
155	0.015393	0.068129
156	0.015321	0.068128
157	0.015252	0.068117
158	0.015197	0.068111
159	0.015131	0.068105
160	0.015058	0.068094
161	0.015005	0.068091
162	0.014940	0.068077
163	0.014883	0.068079
164	0.014815	0.068067
165	0.014759	0.068062
166	0.014700	0.068052
167	0.014646	0.068047
168	0.014600	0.068035
169	0.014536	0.068042
170	0.014492	0.068027
171	0.014441	0.068021
172	0.014393	0.068016
173	0.014351	0.068011
174	0.014298	0.068003
175	0.014266	0.067993
176	0.014208	0.067992
177	0.014176	0.067987
178	0.014118	0.067975
179	0.014089	0.067970
180	0.014036	0.067969
181	0.013989	0.067961
182	0.013953	0.067953
183	0.013918	0.067941
184	0.013881	0.067942
185	0.013841	0.067934
186	0.013799	0.067921
187	0.013777	0.067926
188	0.013721	0.067915
189	0.013700	0.067907
190	0.013654	0.067907
191	0.013627	0.067895
192	0.013600	0.067888
193	0.013560	0.067880
194	0.013527	0.067871
195	0.013497	0.067864
196	0.013479	0.067860
197	0.013442	0.067856
198	0.013422	0.067846
199	0.013390	0.067841
200	0.013357	0.067831
