#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_separable.txt; Entangled DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 87.054%
#Sample standard deviation for averaged success rate: 0.2666271111111104%
#Epoch	Loss	Sample STD
1	0.701386	0.012965
2	0.693232	0.000154
3	0.693117	0.000121
4	0.693042	0.000208
5	0.692934	0.000357
6	0.692765	0.000657
7	0.692508	0.001204
8	0.692062	0.002206
9	0.691341	0.003851
10	0.690183	0.006299
11	0.688396	0.009763
12	0.685736	0.014325
13	0.681962	0.020160
14	0.676931	0.027353
15	0.670515	0.035880
16	0.662674	0.045544
17	0.653367	0.056174
18	0.642580	0.067644
19	0.630336	0.079799
20	0.616749	0.092489
21	0.602132	0.105275
22	0.586777	0.117784
23	0.570939	0.129703
24	0.554840	0.140920
25	0.538573	0.151434
26	0.522235	0.161063
27	0.505844	0.169754
28	0.489431	0.177430
29	0.473133	0.184066
30	0.457016	0.189790
31	0.441323	0.194671
32	0.426156	0.198822
33	0.411518	0.202345
34	0.397545	0.205349
35	0.384262	0.207856
36	0.371777	0.209879
37	0.360061	0.211499
38	0.349211	0.212824
39	0.339166	0.213944
40	0.329972	0.214809
41	0.321490	0.215450
42	0.313684	0.215878
43	0.306505	0.216088
44	0.299816	0.216111
45	0.293601	0.215996
46	0.287793	0.215738
47	0.282315	0.215340
48	0.277132	0.214834
49	0.272186	0.214243
50	0.267442	0.213559
51	0.262885	0.212809
52	0.258495	0.211981
53	0.254246	0.211112
54	0.250126	0.210188
55	0.246113	0.209205
56	0.242187	0.208167
57	0.238337	0.207058
58	0.234614	0.205907
59	0.231046	0.204761
60	0.227614	0.203616
61	0.224291	0.202446
62	0.221089	0.201265
63	0.217967	0.200032
64	0.214893	0.198731
65	0.211858	0.197310
66	0.208841	0.195814
67	0.205879	0.194264
68	0.202980	0.192718
69	0.200124	0.191175
70	0.197342	0.189624
71	0.194618	0.188107
72	0.191971	0.186607
73	0.189363	0.185119
74	0.186775	0.183643
75	0.184287	0.182217
76	0.181874	0.180845
77	0.179583	0.179484
78	0.177348	0.178151
79	0.175181	0.176825
80	0.173060	0.175500
81	0.170979	0.174162
82	0.168954	0.172827
83	0.166941	0.171456
84	0.164964	0.170044
85	0.162990	0.168610
86	0.160997	0.167141
87	0.159019	0.165600
88	0.157005	0.164026
89	0.155005	0.162445
90	0.153023	0.160855
91	0.151062	0.159293
92	0.149117	0.157742
93	0.147196	0.156238
94	0.145299	0.154784
95	0.143426	0.153344
96	0.141512	0.151925
97	0.139558	0.150532
98	0.137516	0.149194
99	0.135495	0.147970
100	0.133509	0.146843
101	0.131636	0.145796
102	0.129837	0.144798
103	0.128106	0.143806
104	0.126433	0.142795
105	0.124849	0.141726
106	0.123308	0.140640
107	0.121793	0.139557
108	0.120301	0.138505
109	0.118848	0.137478
110	0.117408	0.136511
111	0.115970	0.135576
112	0.114532	0.134685
113	0.113098	0.133865
114	0.111695	0.133142
115	0.110366	0.132528
116	0.109135	0.132021
117	0.107972	0.131582
118	0.106901	0.131188
119	0.105881	0.130851
120	0.104927	0.130510
121	0.104007	0.130180
122	0.103116	0.129892
123	0.102264	0.129591
124	0.101438	0.129312
125	0.100622	0.129038
126	0.099837	0.128768
127	0.099069	0.128495
128	0.098330	0.128255
129	0.097597	0.128001
130	0.096881	0.127746
131	0.096199	0.127511
132	0.095532	0.127251
133	0.094856	0.126999
134	0.094213	0.126731
135	0.093576	0.126472
136	0.092956	0.126177
137	0.092331	0.125897
138	0.091738	0.125604
139	0.091152	0.125298
140	0.090581	0.124974
141	0.089989	0.124649
142	0.089417	0.124291
143	0.088856	0.123914
144	0.088286	0.123492
145	0.087690	0.122998
146	0.087102	0.122461
147	0.086510	0.121895
148	0.085905	0.121305
149	0.085291	0.120665
150	0.084674	0.120010
151	0.084046	0.119328
152	0.083417	0.118656
153	0.082802	0.118012
154	0.082177	0.117372
155	0.081511	0.116741
156	0.080827	0.116083
157	0.080090	0.115446
158	0.079355	0.114873
159	0.078685	0.114419
160	0.078030	0.114023
161	0.077411	0.113562
162	0.076671	0.112800
163	0.075732	0.111691
164	0.074745	0.110786
165	0.074009	0.110314
166	0.073416	0.110074
167	0.072919	0.109885
168	0.072448	0.109742
169	0.072003	0.109622
170	0.071602	0.109517
171	0.071204	0.109404
172	0.070838	0.109318
173	0.070508	0.109219
174	0.070145	0.109150
175	0.069848	0.109065
176	0.069520	0.108981
177	0.069234	0.108894
178	0.068935	0.108826
179	0.068653	0.108744
180	0.068383	0.108672
181	0.068128	0.108583
182	0.067872	0.108519
183	0.067624	0.108441
184	0.067360	0.108361
185	0.067134	0.108281
186	0.066894	0.108222
187	0.066666	0.108130
188	0.066444	0.108074
189	0.066247	0.107996
190	0.066020	0.107926
191	0.065804	0.107855
192	0.065598	0.107787
193	0.065394	0.107709
194	0.065201	0.107637
195	0.064990	0.107578
196	0.064806	0.107499
197	0.064610	0.107419
198	0.064413	0.107351
199	0.064239	0.107288
200	0.064045	0.107211
