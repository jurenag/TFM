#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_separable.txt; Entangled DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [8, 4, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 96.64800000000001%
#Sample standard deviation for averaged success rate: 0.02300602020199658%
#Epoch	Loss	Sample STD
1	0.702888	0.017964
2	0.693279	0.000372
3	0.693182	0.000056
4	0.693136	0.000120
5	0.693050	0.000287
6	0.692868	0.000760
7	0.692423	0.002001
8	0.691364	0.004746
9	0.688945	0.009927
10	0.684007	0.019003
11	0.675653	0.033114
12	0.664041	0.051330
13	0.649922	0.071668
14	0.633990	0.092617
15	0.617002	0.113141
16	0.599508	0.131864
17	0.581568	0.147469
18	0.562982	0.160177
19	0.543839	0.170669
20	0.524264	0.179206
21	0.504141	0.186498
22	0.483596	0.193687
23	0.463357	0.201021
24	0.444792	0.207692
25	0.427896	0.212967
26	0.412209	0.216604
27	0.397248	0.218843
28	0.383126	0.220413
29	0.369893	0.221631
30	0.357532	0.222639
31	0.346085	0.223484
32	0.335603	0.224401
33	0.326041	0.225426
34	0.317255	0.226599
35	0.309283	0.227906
36	0.302093	0.229312
37	0.295580	0.230587
38	0.289443	0.231514
39	0.283040	0.232160
40	0.276841	0.232523
41	0.270574	0.231770
42	0.263488	0.230604
43	0.257085	0.229866
44	0.250759	0.228994
45	0.244644	0.228155
46	0.238794	0.227302
47	0.233400	0.226195
48	0.228247	0.224767
49	0.223192	0.222904
50	0.217917	0.220675
51	0.212396	0.218391
52	0.206924	0.216169
53	0.201581	0.213736
54	0.196445	0.211284
55	0.191554	0.208956
56	0.186848	0.206634
57	0.182361	0.204494
58	0.178193	0.202555
59	0.174241	0.200708
60	0.170405	0.198800
61	0.166440	0.196593
62	0.162370	0.194292
63	0.158190	0.192009
64	0.153753	0.189938
65	0.149446	0.188202
66	0.145433	0.186609
67	0.141640	0.185110
68	0.137347	0.183143
69	0.132922	0.181480
70	0.129417	0.180001
71	0.126101	0.178412
72	0.122874	0.176804
73	0.119873	0.175299
74	0.117063	0.173844
75	0.114319	0.172318
76	0.111540	0.170750
77	0.108214	0.168755
78	0.104663	0.167012
79	0.101761	0.165505
80	0.098983	0.163900
81	0.096195	0.162179
82	0.093487	0.160507
83	0.090867	0.158986
84	0.088398	0.157559
85	0.086063	0.156040
86	0.083629	0.154373
87	0.081040	0.152477
88	0.078590	0.150805
89	0.076351	0.149494
90	0.074427	0.148400
91	0.072733	0.147169
92	0.071149	0.145903
93	0.069712	0.144682
94	0.068289	0.143364
95	0.066825	0.141902
96	0.065302	0.140292
97	0.063525	0.138386
98	0.061346	0.136193
99	0.059473	0.134394
100	0.057628	0.132517
101	0.055596	0.130544
102	0.053415	0.128653
103	0.051183	0.127131
104	0.048994	0.125944
105	0.047504	0.125484
106	0.046579	0.125204
107	0.045806	0.124943
108	0.045157	0.124650
109	0.044585	0.124285
110	0.044001	0.123805
111	0.043370	0.123119
112	0.042715	0.122354
113	0.041968	0.121537
114	0.041249	0.120888
115	0.040477	0.120390
116	0.039816	0.120116
117	0.039422	0.119990
118	0.039102	0.119826
119	0.038781	0.119461
120	0.038347	0.118592
121	0.038006	0.118071
122	0.037757	0.117774
123	0.037528	0.117552
124	0.037375	0.117387
125	0.037197	0.117217
126	0.037023	0.117062
127	0.036859	0.116920
128	0.036720	0.116802
129	0.036628	0.116693
130	0.036489	0.116568
131	0.036366	0.116446
132	0.036248	0.116305
133	0.036110	0.116178
134	0.035975	0.116010
135	0.035846	0.115821
136	0.035710	0.115616
137	0.035551	0.115392
138	0.035399	0.115152
139	0.035275	0.114946
140	0.035138	0.114718
141	0.034980	0.114438
142	0.034764	0.114033
143	0.034501	0.113508
144	0.034072	0.112673
145	0.033625	0.111817
146	0.033195	0.110869
147	0.032636	0.109673
148	0.032014	0.108359
149	0.031144	0.106645
150	0.030079	0.104997
151	0.028640	0.103621
152	0.028151	0.103445
153	0.027870	0.103379
154	0.027680	0.103324
155	0.027535	0.103267
156	0.027438	0.103179
157	0.027307	0.103051
158	0.027186	0.102882
159	0.027084	0.102720
160	0.026969	0.102472
161	0.026795	0.101950
162	0.026606	0.101513
163	0.026524	0.101266
164	0.026409	0.101090
165	0.026317	0.100930
166	0.026254	0.100802
167	0.026175	0.100688
168	0.026110	0.100562
169	0.026042	0.100458
170	0.025991	0.100343
171	0.025927	0.100254
172	0.025860	0.100181
173	0.025793	0.100099
174	0.025756	0.100012
175	0.025690	0.099921
176	0.025661	0.099818
177	0.025568	0.099651
178	0.025486	0.099471
179	0.025418	0.099342
180	0.025385	0.099205
181	0.025296	0.099069
182	0.025199	0.098879
183	0.025155	0.098700
184	0.025054	0.098555
185	0.025003	0.098424
186	0.024913	0.098259
187	0.024812	0.098055
188	0.024745	0.097873
189	0.024676	0.097667
190	0.024547	0.097437
191	0.024389	0.097100
192	0.024145	0.096530
193	0.023892	0.096062
194	0.023768	0.095782
195	0.023609	0.095575
196	0.023497	0.095365
197	0.023350	0.095127
198	0.023130	0.094811
199	0.022796	0.094469
200	0.022429	0.094139
