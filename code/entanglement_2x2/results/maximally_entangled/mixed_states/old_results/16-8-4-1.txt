#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_separable.txt; Entangled DMs were read from: /content/drive/MyDrive/Fisymat/TFM/code/entanglement_2x2/input_data/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.85000000000001%
#Sample standard deviation for averaged success rate: 0.005833333333330056%
#Epoch	Loss	Sample STD
1	0.701497	0.014291
2	0.693246	0.000077
3	0.693218	0.000090
4	0.693121	0.000222
5	0.692916	0.000755
6	0.692195	0.002880
7	0.689847	0.009906
8	0.683726	0.025296
9	0.671879	0.049779
10	0.653988	0.079651
11	0.630766	0.110119
12	0.602725	0.139651
13	0.571072	0.167578
14	0.538946	0.191222
15	0.509051	0.209359
16	0.481689	0.222217
17	0.456706	0.231253
18	0.434374	0.237000
19	0.413740	0.239357
20	0.393684	0.239457
21	0.374235	0.238460
22	0.356177	0.237231
23	0.340514	0.235808
24	0.326799	0.234186
25	0.314631	0.232859
26	0.303755	0.231475
27	0.293750	0.229693
28	0.284275	0.227443
29	0.274950	0.224468
30	0.264994	0.220571
31	0.254348	0.216833
32	0.244438	0.213736
33	0.235173	0.210442
34	0.226218	0.206719
35	0.217629	0.202690
36	0.209227	0.198405
37	0.200957	0.193894
38	0.192682	0.189103
39	0.184346	0.184022
40	0.176143	0.179095
41	0.168266	0.174309
42	0.160440	0.169554
43	0.152596	0.164543
44	0.144515	0.159297
45	0.136398	0.153995
46	0.128401	0.148870
47	0.120726	0.143971
48	0.113367	0.139294
49	0.106271	0.134692
50	0.099415	0.130069
51	0.092646	0.125327
52	0.085925	0.120214
53	0.079489	0.115063
54	0.073499	0.110473
55	0.067897	0.106164
56	0.062676	0.101983
57	0.057751	0.097803
58	0.052993	0.093306
59	0.048147	0.088026
60	0.043372	0.082734
61	0.038950	0.078094
62	0.035389	0.074773
63	0.032359	0.072339
64	0.029724	0.070280
65	0.027214	0.068135
66	0.024884	0.066723
67	0.023401	0.066405
68	0.022242	0.066293
69	0.021307	0.066243
70	0.020536	0.066217
71	0.019876	0.066191
72	0.019343	0.066175
73	0.018877	0.066169
74	0.018463	0.066134
75	0.018102	0.066085
76	0.017795	0.066015
77	0.017493	0.065943
78	0.017231	0.065874
79	0.016999	0.065812
80	0.016766	0.065726
81	0.016535	0.065620
82	0.016336	0.065496
83	0.016132	0.065344
84	0.015947	0.065165
85	0.015740	0.064897
86	0.015595	0.064560
87	0.015359	0.064110
88	0.015121	0.063513
89	0.014886	0.062779
90	0.014653	0.062102
91	0.014438	0.061524
92	0.014268	0.061114
93	0.014093	0.060725
94	0.013941	0.060366
95	0.013762	0.059929
96	0.013611	0.059525
97	0.013428	0.059021
98	0.013254	0.058397
99	0.013061	0.057784
100	0.012865	0.057313
101	0.012712	0.056705
102	0.012530	0.056116
103	0.012334	0.055479
104	0.012147	0.054805
105	0.011907	0.053998
106	0.011567	0.052726
107	0.010596	0.049249
108	0.009680	0.047321
109	0.009307	0.046980
110	0.009122	0.046832
111	0.008980	0.046724
112	0.008907	0.046613
113	0.008821	0.046502
114	0.008748	0.046411
115	0.008665	0.046343
116	0.008601	0.046281
117	0.008527	0.046167
118	0.008474	0.046030
119	0.008396	0.045833
120	0.008337	0.045630
121	0.008270	0.045442
122	0.008210	0.045202
123	0.008127	0.044916
124	0.008022	0.044522
125	0.007933	0.043883
126	0.007753	0.042459
127	0.007588	0.041247
128	0.007472	0.040632
129	0.007371	0.040069
130	0.007286	0.039474
131	0.007181	0.038943
132	0.007119	0.038477
133	0.007021	0.037947
134	0.006911	0.037253
135	0.006803	0.036502
136	0.006710	0.035863
137	0.006598	0.034988
138	0.006409	0.033733
139	0.006117	0.031069
140	0.004562	0.016162
141	0.003963	0.010934
142	0.003684	0.008731
143	0.003454	0.007194
144	0.003298	0.006222
145	0.003179	0.005600
146	0.003090	0.005165
147	0.003011	0.004920
148	0.002944	0.004677
149	0.002901	0.004575
150	0.002814	0.004425
151	0.002780	0.004364
152	0.002731	0.004289
153	0.002680	0.004205
154	0.002643	0.004154
155	0.002621	0.004099
156	0.002588	0.004091
157	0.002550	0.004016
158	0.002508	0.004010
159	0.002486	0.003971
160	0.002464	0.003954
161	0.002432	0.003926
162	0.002402	0.003874
163	0.002374	0.003852
164	0.002366	0.003848
165	0.002312	0.003772
166	0.002290	0.003773
167	0.002260	0.003730
168	0.002240	0.003705
169	0.002237	0.003714
170	0.002193	0.003671
171	0.002168	0.003632
172	0.002164	0.003625
173	0.002153	0.003646
174	0.002140	0.003606
175	0.002084	0.003547
176	0.002090	0.003532
177	0.002067	0.003529
178	0.002025	0.003468
179	0.002000	0.003442
180	0.001984	0.003410
181	0.001976	0.003399
182	0.001955	0.003385
183	0.001945	0.003399
184	0.001921	0.003363
185	0.001913	0.003336
186	0.001887	0.003327
187	0.001886	0.003313
188	0.001841	0.003248
189	0.001859	0.003296
190	0.001820	0.003214
191	0.001828	0.003285
192	0.001807	0.003248
193	0.001768	0.003165
194	0.001785	0.003230
195	0.001765	0.003190
196	0.001737	0.003141
197	0.001725	0.003130
198	0.001709	0.003140
199	0.001713	0.003129
200	0.001675	0.003066
