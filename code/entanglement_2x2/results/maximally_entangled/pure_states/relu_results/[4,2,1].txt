#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 94.06101525381347%
#Sample standard deviation for averaged success rate: 0.09471036893467773%
#Same average success rate for supplementary tests: [0.5018699999999999, 0.509775, 0.522235, 0.54203, 0.5915250000000001, 0.5199400000000001, 0.5296900000000001, 0.5425700000000001, 0.56749, 0.6165050000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.0027528213082218043, 0.002703034862659008, 0.002623357626925845, 0.002494077776453653, 0.0021957535651229167, 0.0026424268807291526, 0.0025822519619510408, 0.002502616581700041, 0.00235941835946913, 0.002084947061378297]%
#Epoch	Loss	Sample STD
1	0.674711	0.003803
2	0.607074	0.018320
3	0.520547	0.036277
4	0.439347	0.049435
5	0.369990	0.057241
6	0.318216	0.060114
7	0.277727	0.060962
8	0.246963	0.061957
9	0.223771	0.062815
10	0.202758	0.063655
11	0.184761	0.064231
12	0.171029	0.064476
13	0.161381	0.064786
14	0.153983	0.065116
15	0.148136	0.065389
16	0.143439	0.065604
17	0.139361	0.065769
18	0.135781	0.065907
19	0.132615	0.066059
20	0.129920	0.066182
21	0.127512	0.066281
22	0.125340	0.066305
23	0.123490	0.066322
24	0.121862	0.066325
25	0.120513	0.066313
26	0.119241	0.066298
27	0.118081	0.066263
28	0.117056	0.066254
29	0.116049	0.066226
30	0.115123	0.066181
31	0.114025	0.066125
32	0.112755	0.066041
33	0.111237	0.065901
34	0.108966	0.065649
35	0.107131	0.065466
36	0.105712	0.065361
37	0.104610	0.065296
38	0.103655	0.065231
39	0.102970	0.065209
40	0.102345	0.065182
41	0.101797	0.065158
42	0.101248	0.065139
43	0.100810	0.065128
44	0.100382	0.065114
45	0.100023	0.065111
46	0.099736	0.065101
47	0.099431	0.065093
48	0.099161	0.065093
49	0.098844	0.065090
50	0.098695	0.065089
51	0.098415	0.065082
52	0.098162	0.065077
53	0.097934	0.065085
54	0.097791	0.065073
55	0.097598	0.065082
56	0.097425	0.065077
57	0.097279	0.065081
58	0.097148	0.065074
59	0.096946	0.065067
60	0.096792	0.065076
61	0.096641	0.065077
62	0.096492	0.065081
63	0.096427	0.065085
64	0.096359	0.065088
65	0.096080	0.065078
66	0.096007	0.065092
67	0.096010	0.065090
68	0.095847	0.065091
69	0.095807	0.065100
70	0.095619	0.065098
71	0.095565	0.065094
72	0.095510	0.065094
73	0.095489	0.065101
74	0.095250	0.065097
75	0.095432	0.065104
76	0.095267	0.065100
77	0.095116	0.065109
78	0.095115	0.065106
79	0.095088	0.065110
80	0.095046	0.065108
81	0.094940	0.065114
82	0.094822	0.065113
83	0.094770	0.065123
84	0.094841	0.065116
85	0.094611	0.065124
86	0.094675	0.065113
87	0.094627	0.065123
88	0.094480	0.065128
89	0.094411	0.065125
90	0.094311	0.065131
91	0.094386	0.065125
92	0.094345	0.065135
93	0.094116	0.065133
94	0.094400	0.065138
95	0.094094	0.065132
96	0.094376	0.065134
97	0.094196	0.065136
98	0.094101	0.065137
99	0.094180	0.065136
100	0.094141	0.065133
101	0.093978	0.065132
102	0.094010	0.065138
103	0.093853	0.065138
104	0.093804	0.065137
105	0.093957	0.065135
106	0.093890	0.065141
107	0.093831	0.065143
108	0.093716	0.065145
109	0.093917	0.065138
110	0.093745	0.065139
111	0.093651	0.065138
112	0.093654	0.065144
113	0.093878	0.065140
114	0.093697	0.065139
115	0.093655	0.065143
116	0.093560	0.065148
117	0.093515	0.065144
118	0.093749	0.065141
119	0.093652	0.065139
120	0.093553	0.065152
121	0.093505	0.065142
122	0.093555	0.065144
123	0.093579	0.065146
124	0.093545	0.065144
125	0.093725	0.065140
126	0.093620	0.065139
127	0.093910	0.065141
128	0.093711	0.065143
129	0.093876	0.065141
130	0.093605	0.065136
131	0.093354	0.065146
132	0.093578	0.065141
133	0.093412	0.065150
134	0.093383	0.065143
135	0.093190	0.065147
136	0.093391	0.065139
137	0.093484	0.065145
138	0.093444	0.065140
139	0.093480	0.065147
140	0.093350	0.065146
141	0.093368	0.065138
142	0.093328	0.065143
143	0.093430	0.065147
144	0.093223	0.065142
145	0.093356	0.065140
146	0.093204	0.065152
147	0.093275	0.065150
148	0.093385	0.065143
149	0.093335	0.065146
150	0.093372	0.065149
151	0.093729	0.065136
