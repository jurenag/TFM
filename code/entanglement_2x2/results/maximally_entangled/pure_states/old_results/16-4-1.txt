#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [16, 4, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.7775%
#Sample standard deviation for averaged success rate: 0.013479734848488762%
#Epoch	Loss	Sample STD
1	0.699822	0.012194
2	0.693189	0.000126
3	0.693027	0.000324
4	0.692696	0.001001
5	0.691967	0.003095
6	0.690206	0.008238
7	0.686338	0.017775
8	0.678818	0.033057
9	0.666755	0.053271
10	0.650237	0.076129
11	0.629917	0.100612
12	0.606906	0.124870
13	0.582196	0.146794
14	0.556654	0.165340
15	0.531469	0.180177
16	0.506981	0.191479
17	0.482857	0.199948
18	0.459476	0.206215
19	0.437604	0.210249
20	0.417249	0.212253
21	0.398440	0.212787
22	0.381046	0.212345
23	0.364905	0.211109
24	0.349833	0.209325
25	0.335598	0.207303
26	0.322385	0.205487
27	0.310067	0.203913
28	0.298599	0.202499
29	0.287806	0.201149
30	0.277734	0.199876
31	0.268258	0.198533
32	0.259292	0.197014
33	0.250662	0.195084
34	0.242077	0.192586
35	0.233284	0.189517
36	0.224308	0.186193
37	0.215367	0.182907
38	0.206696	0.179719
39	0.198317	0.176380
40	0.190133	0.172715
41	0.182102	0.168613
42	0.174028	0.163962
43	0.165890	0.158751
44	0.157633	0.153039
45	0.149274	0.146916
46	0.140801	0.140497
47	0.132279	0.133910
48	0.123800	0.127286
49	0.115433	0.120761
50	0.107265	0.114287
51	0.099397	0.107954
52	0.091893	0.101758
53	0.084767	0.095784
54	0.078069	0.089966
55	0.071741	0.084232
56	0.065814	0.078635
57	0.060370	0.073272
58	0.055376	0.068183
59	0.050813	0.063233
60	0.046627	0.058476
61	0.042819	0.053879
62	0.039361	0.049404
63	0.036199	0.045067
64	0.033327	0.040902
65	0.030678	0.036825
66	0.028267	0.033009
67	0.026078	0.029496
68	0.024113	0.026471
69	0.022348	0.023959
70	0.020816	0.022014
71	0.019518	0.020522
72	0.018381	0.019296
73	0.017362	0.018234
74	0.016454	0.017332
75	0.015655	0.016564
76	0.014920	0.015926
77	0.014261	0.015410
78	0.013679	0.014980
79	0.013147	0.014684
80	0.012665	0.014421
81	0.012241	0.014217
82	0.011846	0.014023
83	0.011492	0.013828
84	0.011171	0.013671
85	0.010866	0.013499
86	0.010584	0.013318
87	0.010309	0.013149
88	0.010064	0.012975
89	0.009820	0.012793
90	0.009599	0.012641
91	0.009369	0.012458
92	0.009165	0.012287
93	0.008944	0.012109
94	0.008750	0.011956
95	0.008572	0.011804
96	0.008403	0.011653
97	0.008212	0.011471
98	0.008050	0.011342
99	0.007897	0.011193
100	0.007740	0.011043
101	0.007595	0.010900
102	0.007451	0.010757
103	0.007318	0.010629
104	0.007178	0.010498
105	0.007051	0.010326
106	0.006917	0.010199
107	0.006804	0.010082
108	0.006689	0.009947
109	0.006567	0.009841
110	0.006464	0.009722
111	0.006364	0.009604
112	0.006240	0.009479
113	0.006153	0.009365
114	0.006036	0.009246
115	0.005944	0.009141
116	0.005865	0.009039
117	0.005751	0.008915
118	0.005669	0.008818
119	0.005584	0.008717
120	0.005500	0.008632
121	0.005415	0.008518
122	0.005334	0.008425
123	0.005240	0.008317
124	0.005166	0.008228
125	0.005089	0.008137
126	0.005012	0.008060
127	0.004938	0.007960
128	0.004857	0.007874
129	0.004798	0.007804
130	0.004719	0.007691
131	0.004653	0.007623
132	0.004599	0.007552
133	0.004524	0.007439
134	0.004460	0.007376
135	0.004395	0.007294
136	0.004338	0.007220
137	0.004277	0.007117
138	0.004219	0.007064
139	0.004161	0.006986
140	0.004106	0.006902
141	0.004041	0.006829
142	0.004005	0.006780
143	0.003948	0.006673
144	0.003892	0.006612
145	0.003845	0.006537
146	0.003794	0.006475
147	0.003745	0.006397
148	0.003695	0.006324
149	0.003645	0.006233
150	0.003598	0.006168
151	0.003553	0.006085
152	0.003501	0.006022
153	0.003454	0.005926
154	0.003400	0.005839
155	0.003373	0.005787
156	0.003310	0.005696
157	0.003262	0.005625
158	0.003228	0.005562
159	0.003192	0.005483
160	0.003132	0.005388
161	0.003107	0.005344
162	0.003059	0.005274
163	0.003020	0.005199
164	0.002969	0.005137
165	0.002942	0.005067
166	0.002902	0.005023
167	0.002872	0.004931
168	0.002829	0.004898
169	0.002803	0.004835
170	0.002765	0.004774
171	0.002742	0.004717
172	0.002703	0.004672
173	0.002659	0.004614
174	0.002643	0.004567
175	0.002613	0.004508
176	0.002583	0.004466
177	0.002551	0.004420
178	0.002526	0.004358
179	0.002496	0.004306
180	0.002465	0.004276
181	0.002448	0.004208
182	0.002422	0.004168
183	0.002379	0.004114
184	0.002367	0.004090
185	0.002354	0.004054
186	0.002316	0.003985
187	0.002289	0.003955
188	0.002264	0.003903
189	0.002252	0.003863
190	0.002224	0.003815
191	0.002204	0.003768
192	0.002165	0.003720
193	0.002174	0.003684
194	0.002130	0.003634
195	0.002121	0.003598
196	0.002084	0.003553
197	0.002082	0.003531
198	0.002054	0.003492
199	0.002032	0.003452
200	0.002024	0.003414
