#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [8, 4, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.71725%
#Sample standard deviation for averaged success rate: 0.014856590277742752%
#Epoch	Loss	Sample STD
1	0.700655	0.012408
2	0.693197	0.000120
3	0.693092	0.000177
4	0.692945	0.000352
5	0.692712	0.000717
6	0.692292	0.001518
7	0.691447	0.003245
8	0.689791	0.006692
9	0.686769	0.012800
10	0.681905	0.022023
11	0.674673	0.034834
12	0.664686	0.051808
13	0.651903	0.072515
14	0.637007	0.094410
15	0.620869	0.114790
16	0.604043	0.132789
17	0.586553	0.148544
18	0.568404	0.162018
19	0.549260	0.173361
20	0.528615	0.183149
21	0.506567	0.191854
22	0.483670	0.199840
23	0.460848	0.206841
24	0.438775	0.212279
25	0.417338	0.215808
26	0.396604	0.217475
27	0.376526	0.217528
28	0.356890	0.216167
29	0.337755	0.213558
30	0.318952	0.209835
31	0.300644	0.205389
32	0.283004	0.200478
33	0.266307	0.195227
34	0.250543	0.189751
35	0.235826	0.184148
36	0.222163	0.178570
37	0.209626	0.173166
38	0.198039	0.167875
39	0.187305	0.162772
40	0.177420	0.157946
41	0.168333	0.153443
42	0.159928	0.149139
43	0.152173	0.145012
44	0.144973	0.140905
45	0.138221	0.136742
46	0.131865	0.132581
47	0.125875	0.128340
48	0.120211	0.124083
49	0.114799	0.119795
50	0.109659	0.115455
51	0.104699	0.111059
52	0.099906	0.106634
53	0.095310	0.102241
54	0.090909	0.097938
55	0.086637	0.093740
56	0.082562	0.089707
57	0.078689	0.085867
58	0.075004	0.082193
59	0.071474	0.078643
60	0.068110	0.075230
61	0.064907	0.071932
62	0.061823	0.068712
63	0.058864	0.065561
64	0.056007	0.062458
65	0.053244	0.059389
66	0.050605	0.056397
67	0.048046	0.053458
68	0.045601	0.050568
69	0.043236	0.047714
70	0.040988	0.044982
71	0.038809	0.042339
72	0.036766	0.039807
73	0.034813	0.037471
74	0.032969	0.035297
75	0.031252	0.033320
76	0.029654	0.031506
77	0.028179	0.029845
78	0.026774	0.028290
79	0.025478	0.026899
80	0.024263	0.025613
81	0.023131	0.024447
82	0.022079	0.023368
83	0.021102	0.022387
84	0.020184	0.021475
85	0.019320	0.020640
86	0.018521	0.019865
87	0.017757	0.019160
88	0.017062	0.018504
89	0.016395	0.017904
90	0.015771	0.017324
91	0.015181	0.016787
92	0.014629	0.016285
93	0.014097	0.015796
94	0.013591	0.015373
95	0.013129	0.014955
96	0.012696	0.014560
97	0.012272	0.014211
98	0.011876	0.013883
99	0.011500	0.013557
100	0.011145	0.013264
101	0.010814	0.013007
102	0.010493	0.012741
103	0.010186	0.012502
104	0.009901	0.012272
105	0.009621	0.012074
106	0.009365	0.011870
107	0.009125	0.011696
108	0.008886	0.011534
109	0.008671	0.011362
110	0.008452	0.011206
111	0.008276	0.011082
112	0.008074	0.010939
113	0.007906	0.010832
114	0.007736	0.010706
115	0.007571	0.010577
116	0.007417	0.010475
117	0.007273	0.010373
118	0.007136	0.010268
119	0.006999	0.010180
120	0.006873	0.010083
121	0.006753	0.009985
122	0.006626	0.009900
123	0.006512	0.009811
124	0.006403	0.009736
125	0.006311	0.009659
126	0.006198	0.009585
127	0.006103	0.009512
128	0.006013	0.009445
129	0.005921	0.009364
130	0.005831	0.009292
131	0.005761	0.009237
132	0.005676	0.009164
133	0.005587	0.009095
134	0.005514	0.009028
135	0.005445	0.008971
136	0.005369	0.008902
137	0.005303	0.008842
138	0.005219	0.008777
139	0.005165	0.008728
140	0.005094	0.008649
141	0.005038	0.008600
142	0.004977	0.008548
143	0.004915	0.008488
144	0.004860	0.008422
145	0.004801	0.008345
146	0.004749	0.008309
147	0.004696	0.008243
148	0.004636	0.008164
149	0.004583	0.008134
150	0.004543	0.008077
151	0.004493	0.008013
152	0.004435	0.007956
153	0.004387	0.007889
154	0.004338	0.007832
155	0.004291	0.007792
156	0.004240	0.007733
157	0.004205	0.007690
158	0.004156	0.007622
159	0.004117	0.007574
160	0.004065	0.007518
161	0.004026	0.007467
162	0.003977	0.007416
163	0.003938	0.007370
164	0.003894	0.007316
165	0.003863	0.007264
166	0.003823	0.007235
167	0.003775	0.007149
168	0.003739	0.007124
169	0.003709	0.007083
170	0.003662	0.007034
171	0.003639	0.006991
172	0.003595	0.006949
173	0.003564	0.006909
174	0.003525	0.006848
175	0.003499	0.006836
176	0.003458	0.006779
177	0.003427	0.006730
178	0.003393	0.006699
179	0.003357	0.006654
180	0.003341	0.006636
181	0.003299	0.006591
182	0.003275	0.006561
183	0.003250	0.006518
184	0.003219	0.006476
185	0.003175	0.006436
186	0.003159	0.006413
187	0.003140	0.006382
188	0.003102	0.006312
189	0.003068	0.006299
190	0.003056	0.006273
191	0.003019	0.006237
192	0.003011	0.006203
193	0.002972	0.006183
194	0.002954	0.006141
195	0.002936	0.006110
196	0.002919	0.006075
197	0.002882	0.006049
198	0.002857	0.006028
199	0.002842	0.005991
200	0.002822	0.005965
