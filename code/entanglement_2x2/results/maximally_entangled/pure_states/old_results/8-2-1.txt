#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [8, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.23025000000001%
#Sample standard deviation for averaged success rate: 0.017426615530265886%
#Epoch	Loss	Sample STD
1	0.704053	0.015546
2	0.693204	0.000101
3	0.693095	0.000163
4	0.692958	0.000383
5	0.692735	0.000909
6	0.692299	0.002085
7	0.691441	0.004604
8	0.689780	0.009492
9	0.686820	0.017509
10	0.682200	0.028648
11	0.675924	0.041629
12	0.668163	0.055027
13	0.658917	0.068004
14	0.648018	0.080726
15	0.635253	0.093715
16	0.620932	0.106570
17	0.605359	0.118935
18	0.588947	0.130564
19	0.571850	0.141468
20	0.554354	0.151620
21	0.536718	0.160853
22	0.518947	0.169112
23	0.501209	0.176418
24	0.483570	0.182846
25	0.466303	0.188411
26	0.449512	0.193137
27	0.433349	0.197064
28	0.417781	0.200198
29	0.402747	0.202621
30	0.388232	0.204508
31	0.374215	0.205971
32	0.360796	0.207169
33	0.348086	0.208102
34	0.336068	0.208771
35	0.324659	0.209187
36	0.313821	0.209348
37	0.303534	0.209339
38	0.293737	0.209118
39	0.284383	0.208644
40	0.275407	0.207952
41	0.266709	0.206999
42	0.258234	0.205849
43	0.250132	0.204554
44	0.242442	0.203227
45	0.235119	0.201853
46	0.228109	0.200447
47	0.221300	0.198904
48	0.214587	0.197229
49	0.208119	0.195537
50	0.201944	0.193881
51	0.195993	0.192186
52	0.190236	0.190457
53	0.184593	0.188656
54	0.179036	0.186712
55	0.173540	0.184660
56	0.168135	0.182546
57	0.162946	0.180474
58	0.157983	0.178462
59	0.153229	0.176443
60	0.148605	0.174394
61	0.144116	0.172347
62	0.139743	0.170289
63	0.135475	0.168179
64	0.131271	0.166014
65	0.127074	0.163708
66	0.122830	0.161248
67	0.118472	0.158713
68	0.114199	0.156275
69	0.110101	0.153874
70	0.106139	0.151412
71	0.102241	0.148851
72	0.098510	0.146486
73	0.095031	0.144369
74	0.091826	0.142444
75	0.088804	0.140639
76	0.085913	0.138912
77	0.083166	0.137200
78	0.080524	0.135434
79	0.077917	0.133586
80	0.075353	0.131562
81	0.072766	0.129424
82	0.070239	0.127246
83	0.067686	0.125010
84	0.065068	0.122670
85	0.062464	0.120300
86	0.060059	0.117997
87	0.057848	0.115720
88	0.055669	0.113318
89	0.053431	0.110726
90	0.051167	0.108195
91	0.049045	0.106034
92	0.047121	0.104219
93	0.045386	0.102757
94	0.043878	0.101559
95	0.042559	0.100596
96	0.041352	0.099787
97	0.040283	0.099081
98	0.039288	0.098455
99	0.038374	0.097879
100	0.037513	0.097345
101	0.036705	0.096827
102	0.035918	0.096338
103	0.035188	0.095859
104	0.034492	0.095401
105	0.033804	0.094934
106	0.033159	0.094457
107	0.032535	0.093965
108	0.031914	0.093432
109	0.031299	0.092831
110	0.030690	0.092189
111	0.030076	0.091390
112	0.029421	0.090446
113	0.028728	0.089234
114	0.027941	0.087737
115	0.027044	0.085966
116	0.026053	0.084318
117	0.025101	0.083130
118	0.024314	0.082491
119	0.023742	0.082179
120	0.023292	0.081979
121	0.022927	0.081825
122	0.022603	0.081688
123	0.022309	0.081536
124	0.022032	0.081387
125	0.021782	0.081229
126	0.021547	0.081064
127	0.021307	0.080867
128	0.021073	0.080632
129	0.020842	0.080322
130	0.020581	0.079867
131	0.020318	0.079256
132	0.020047	0.078625
133	0.019816	0.078138
134	0.019599	0.077711
135	0.019390	0.077277
136	0.019179	0.076875
137	0.018991	0.076484
138	0.018789	0.076091
139	0.018607	0.075687
140	0.018420	0.075298
141	0.018221	0.074853
142	0.018028	0.074407
143	0.017830	0.073969
144	0.017637	0.073509
145	0.017407	0.072981
146	0.017122	0.072251
147	0.016871	0.071648
148	0.016660	0.071230
149	0.016474	0.070870
150	0.016297	0.070551
151	0.016118	0.070262
152	0.015966	0.070023
153	0.015812	0.069820
154	0.015677	0.069640
155	0.015549	0.069452
156	0.015410	0.069296
157	0.015278	0.069152
158	0.015177	0.069002
159	0.015038	0.068882
160	0.014930	0.068758
161	0.014803	0.068645
162	0.014674	0.068525
163	0.014553	0.068393
164	0.014420	0.068283
165	0.014281	0.068176
166	0.014157	0.068072
167	0.013999	0.067982
168	0.013869	0.067899
169	0.013730	0.067826
170	0.013597	0.067766
171	0.013496	0.067729
172	0.013414	0.067693
173	0.013339	0.067657
174	0.013278	0.067621
175	0.013209	0.067599
176	0.013173	0.067564
177	0.013103	0.067537
178	0.013070	0.067509
179	0.013018	0.067482
180	0.012974	0.067452
181	0.012936	0.067421
182	0.012907	0.067391
183	0.012852	0.067365
184	0.012832	0.067332
185	0.012788	0.067303
186	0.012760	0.067268
187	0.012730	0.067235
188	0.012697	0.067194
189	0.012651	0.067148
190	0.012624	0.067094
191	0.012581	0.067025
192	0.012561	0.066955
193	0.012512	0.066875
194	0.012476	0.066794
195	0.012447	0.066696
196	0.012400	0.066581
197	0.012367	0.066442
198	0.012310	0.066261
199	0.012265	0.066045
200	0.012196	0.065773
