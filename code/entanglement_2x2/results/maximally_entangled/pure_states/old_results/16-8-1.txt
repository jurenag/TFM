#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.9195%
#Sample standard deviation for averaged success rate: 0.004469896464659731%
#Epoch	Loss	Sample STD
1	0.696292	0.004578
2	0.693278	0.000115
3	0.693162	0.000157
4	0.693000	0.000292
5	0.692742	0.000602
6	0.692220	0.001479
7	0.691091	0.003764
8	0.688593	0.009091
9	0.683466	0.019774
10	0.674254	0.036817
11	0.659482	0.059689
12	0.638262	0.086833
13	0.611223	0.116968
14	0.581118	0.146128
15	0.550766	0.170095
16	0.520705	0.188295
17	0.491028	0.201938
18	0.462012	0.212189
19	0.433947	0.219299
20	0.406513	0.223266
21	0.379439	0.224012
22	0.352574	0.221937
23	0.326007	0.217864
24	0.300295	0.212697
25	0.276519	0.206996
26	0.255044	0.200454
27	0.235385	0.192801
28	0.217269	0.184694
29	0.200767	0.176710
30	0.185732	0.169167
31	0.171946	0.162157
32	0.159516	0.156053
33	0.148382	0.150691
34	0.138327	0.145896
35	0.129222	0.141427
36	0.120914	0.137239
37	0.113249	0.133153
38	0.106201	0.129133
39	0.099628	0.124903
40	0.093400	0.120358
41	0.087470	0.115347
42	0.081695	0.109740
43	0.076063	0.103511
44	0.070533	0.096745
45	0.065160	0.089441
46	0.059917	0.081829
47	0.054834	0.074093
48	0.049959	0.066590
49	0.045391	0.059572
50	0.041101	0.053015
51	0.037114	0.046915
52	0.033488	0.041313
53	0.030183	0.036187
54	0.027200	0.031519
55	0.024541	0.027449
56	0.022139	0.023893
57	0.020031	0.020856
58	0.018133	0.018325
59	0.016483	0.016236
60	0.015037	0.014572
61	0.013760	0.013225
62	0.012651	0.012177
63	0.011684	0.011354
64	0.010839	0.010710
65	0.010099	0.010219
66	0.009459	0.009836
67	0.008860	0.009478
68	0.008362	0.009207
69	0.007889	0.008974
70	0.007485	0.008769
71	0.007095	0.008554
72	0.006763	0.008353
73	0.006453	0.008175
74	0.006157	0.007977
75	0.005911	0.007834
76	0.005672	0.007669
77	0.005454	0.007512
78	0.005256	0.007342
79	0.005080	0.007201
80	0.004892	0.007030
81	0.004732	0.006878
82	0.004568	0.006718
83	0.004428	0.006573
84	0.004289	0.006425
85	0.004165	0.006273
86	0.004034	0.006114
87	0.003927	0.005970
88	0.003798	0.005819
89	0.003700	0.005686
90	0.003593	0.005541
91	0.003489	0.005393
92	0.003386	0.005258
93	0.003295	0.005116
94	0.003206	0.005008
95	0.003128	0.004878
96	0.003041	0.004755
97	0.002962	0.004628
98	0.002888	0.004520
99	0.002808	0.004410
100	0.002738	0.004296
101	0.002678	0.004205
102	0.002602	0.004100
103	0.002547	0.004012
104	0.002485	0.003931
105	0.002425	0.003838
106	0.002371	0.003747
107	0.002316	0.003680
108	0.002268	0.003594
109	0.002208	0.003527
110	0.002169	0.003451
111	0.002120	0.003389
112	0.002074	0.003329
113	0.002036	0.003269
114	0.001986	0.003203
115	0.001958	0.003170
116	0.001913	0.003095
117	0.001877	0.003052
118	0.001836	0.003011
119	0.001805	0.002962
120	0.001763	0.002906
121	0.001735	0.002851
122	0.001707	0.002820
123	0.001673	0.002781
124	0.001646	0.002734
125	0.001617	0.002705
126	0.001591	0.002670
127	0.001557	0.002614
128	0.001534	0.002592
129	0.001502	0.002557
130	0.001483	0.002524
131	0.001457	0.002479
132	0.001435	0.002447
133	0.001407	0.002421
134	0.001383	0.002378
135	0.001366	0.002359
136	0.001340	0.002320
137	0.001320	0.002291
138	0.001308	0.002278
139	0.001291	0.002250
140	0.001262	0.002218
141	0.001242	0.002181
142	0.001221	0.002163
143	0.001214	0.002155
144	0.001195	0.002124
145	0.001170	0.002096
146	0.001159	0.002076
147	0.001140	0.002047
148	0.001122	0.002022
149	0.001112	0.002007
150	0.001086	0.001971
151	0.001080	0.001966
152	0.001057	0.001935
153	0.001048	0.001915
154	0.001032	0.001906
155	0.001017	0.001877
156	0.001000	0.001855
157	0.000995	0.001835
158	0.000980	0.001833
159	0.000965	0.001809
160	0.000954	0.001794
161	0.000946	0.001777
162	0.000932	0.001761
163	0.000916	0.001734
164	0.000906	0.001725
165	0.000896	0.001698
166	0.000879	0.001694
167	0.000868	0.001670
168	0.000863	0.001656
169	0.000857	0.001644
170	0.000839	0.001628
171	0.000827	0.001602
172	0.000811	0.001584
173	0.000812	0.001585
174	0.000794	0.001547
175	0.000780	0.001538
176	0.000776	0.001537
177	0.000759	0.001504
178	0.000750	0.001483
179	0.000735	0.001478
180	0.000742	0.001459
181	0.000726	0.001449
182	0.000715	0.001439
183	0.000717	0.001443
184	0.000712	0.001418
185	0.000706	0.001406
186	0.000688	0.001398
187	0.000677	0.001383
188	0.000675	0.001363
189	0.000670	0.001352
190	0.000659	0.001353
191	0.000651	0.001327
192	0.000640	0.001320
193	0.000639	0.001306
194	0.000620	0.001287
195	0.000613	0.001269
196	0.000610	0.001259
197	0.000608	0.001258
198	0.000590	0.001231
199	0.000590	0.001230
200	0.000573	0.001200
