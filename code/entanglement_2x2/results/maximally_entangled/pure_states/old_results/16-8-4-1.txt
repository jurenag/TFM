#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.05825000000002%
#Sample standard deviation for averaged success rate: 0.011331888257569016%
#Epoch	Loss	Sample STD
1	0.699619	0.009956
2	0.693246	0.000063
3	0.693202	0.000080
4	0.693123	0.000153
5	0.692923	0.000509
6	0.692307	0.002141
7	0.690033	0.008858
8	0.683423	0.026945
9	0.670182	0.055066
10	0.650120	0.086751
11	0.623751	0.117554
12	0.591452	0.145966
13	0.555549	0.171927
14	0.520557	0.193342
15	0.488828	0.208828
16	0.459897	0.219123
17	0.432207	0.225394
18	0.405982	0.229314
19	0.381089	0.231001
20	0.357599	0.231373
21	0.336970	0.231119
22	0.318744	0.229413
23	0.301813	0.227044
24	0.286623	0.225035
25	0.273100	0.222668
26	0.260771	0.219585
27	0.249104	0.215648
28	0.237993	0.211585
29	0.227713	0.208107
30	0.218443	0.205334
31	0.210124	0.202892
32	0.202484	0.200341
33	0.195166	0.197412
34	0.187908	0.194257
35	0.180947	0.191115
36	0.174356	0.187794
37	0.167888	0.184041
38	0.161434	0.179882
39	0.154737	0.175277
40	0.147708	0.170145
41	0.140237	0.164533
42	0.132396	0.158622
43	0.124400	0.152539
44	0.116468	0.146599
45	0.108873	0.140902
46	0.101579	0.135481
47	0.094681	0.130243
48	0.088091	0.124805
49	0.081810	0.119198
50	0.075543	0.113201
51	0.069454	0.107458
52	0.064027	0.102333
53	0.058996	0.097618
54	0.054370	0.093471
55	0.050178	0.090064
56	0.046394	0.087311
57	0.042969	0.085138
58	0.039977	0.083518
59	0.037376	0.082396
60	0.035179	0.081675
61	0.033290	0.081241
62	0.031739	0.080972
63	0.030422	0.080796
64	0.029272	0.080691
65	0.028315	0.080612
66	0.027465	0.080566
67	0.026688	0.080526
68	0.026075	0.080495
69	0.025486	0.080476
70	0.024996	0.080429
71	0.024545	0.080415
72	0.024156	0.080372
73	0.023777	0.080339
74	0.023470	0.080308
75	0.023164	0.080271
76	0.022885	0.080255
77	0.022642	0.080209
78	0.022412	0.080152
79	0.022189	0.080088
80	0.021988	0.080027
81	0.021779	0.079950
82	0.021605	0.079854
83	0.021409	0.079736
84	0.021268	0.079628
85	0.021073	0.079496
86	0.020918	0.079372
87	0.020758	0.079221
88	0.020575	0.079030
89	0.020422	0.078792
90	0.020258	0.078547
91	0.020097	0.078270
92	0.019914	0.077944
93	0.019725	0.077463
94	0.019465	0.076700
95	0.019161	0.075624
96	0.018612	0.073300
97	0.017793	0.069516
98	0.016809	0.065419
99	0.015700	0.061616
100	0.014429	0.058945
101	0.013981	0.058013
102	0.013578	0.057101
103	0.013213	0.056038
104	0.012679	0.054170
105	0.012084	0.052042
106	0.011376	0.050108
107	0.010505	0.048596
108	0.010118	0.048292
109	0.009920	0.048242
110	0.009767	0.048210
111	0.009644	0.048201
112	0.009553	0.048197
113	0.009472	0.048186
114	0.009396	0.048187
115	0.009330	0.048184
116	0.009269	0.048181
117	0.009200	0.048178
118	0.009147	0.048175
119	0.009088	0.048166
120	0.009018	0.048169
121	0.008963	0.048162
122	0.008922	0.048161
123	0.008877	0.048149
124	0.008810	0.048156
125	0.008796	0.048149
126	0.008746	0.048146
127	0.008712	0.048150
128	0.008682	0.048143
129	0.008627	0.048146
130	0.008563	0.048144
131	0.008536	0.048136
132	0.008511	0.048139
133	0.008482	0.048120
134	0.008435	0.048126
135	0.008401	0.048123
136	0.008372	0.048115
137	0.008331	0.048120
138	0.008288	0.048112
139	0.008256	0.048102
140	0.008213	0.048098
141	0.008199	0.048094
142	0.008160	0.048093
143	0.008152	0.048091
144	0.008109	0.048072
145	0.008092	0.048085
146	0.008068	0.048080
147	0.008045	0.048074
148	0.008009	0.048076
149	0.008000	0.048074
150	0.007972	0.048074
151	0.007936	0.048070
152	0.007902	0.048064
153	0.007894	0.048063
154	0.007871	0.048056
155	0.007822	0.048054
156	0.007819	0.048051
157	0.007777	0.048049
158	0.007773	0.048048
159	0.007758	0.048045
160	0.007725	0.048034
161	0.007711	0.048034
162	0.007685	0.048031
163	0.007676	0.048029
164	0.007662	0.048018
165	0.007627	0.048018
166	0.007612	0.048007
167	0.007602	0.048005
168	0.007571	0.047994
169	0.007575	0.048003
170	0.007534	0.047980
171	0.007510	0.047960
172	0.007491	0.047962
173	0.007472	0.047962
174	0.007482	0.047951
175	0.007429	0.047948
176	0.007421	0.047952
177	0.007423	0.047940
178	0.007411	0.047940
179	0.007371	0.047924
180	0.007367	0.047901
181	0.007349	0.047885
182	0.007332	0.047883
183	0.007303	0.047871
184	0.007281	0.047861
185	0.007292	0.047863
186	0.007260	0.047841
187	0.007260	0.047847
188	0.007246	0.047831
189	0.007217	0.047820
190	0.007185	0.047821
191	0.007204	0.047808
192	0.007188	0.047793
193	0.007166	0.047788
194	0.007147	0.047775
195	0.007128	0.047752
196	0.007125	0.047740
197	0.007110	0.047700
198	0.007101	0.047675
199	0.007079	0.047651
200	0.007051	0.047622
