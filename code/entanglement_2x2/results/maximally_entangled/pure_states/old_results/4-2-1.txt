#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 85.143%
#Sample standard deviation for averaged success rate: 0.3326116262626142%
#Epoch	Loss	Sample STD
1	0.705638	0.017164
2	0.693254	0.000234
3	0.693088	0.000180
4	0.692976	0.000317
5	0.692817	0.000593
6	0.692585	0.001106
7	0.692195	0.002008
8	0.691561	0.003500
9	0.690546	0.005675
10	0.688968	0.008727
11	0.686601	0.012837
12	0.683226	0.018246
13	0.678655	0.025063
14	0.672882	0.032945
15	0.665864	0.041599
16	0.657595	0.050980
17	0.648184	0.060886
18	0.637768	0.071206
19	0.626618	0.081625
20	0.614983	0.091973
21	0.603092	0.101895
22	0.591006	0.111309
23	0.578580	0.120248
24	0.565696	0.128698
25	0.552222	0.136776
26	0.538291	0.144437
27	0.523975	0.151717
28	0.509419	0.158597
29	0.494737	0.165064
30	0.479972	0.171116
31	0.465261	0.176740
32	0.450756	0.181920
33	0.436660	0.186611
34	0.423104	0.190797
35	0.410224	0.194542
36	0.398247	0.197782
37	0.387049	0.200529
38	0.376460	0.202820
39	0.366403	0.204717
40	0.356886	0.206266
41	0.347918	0.207584
42	0.339557	0.208651
43	0.331772	0.209445
44	0.324478	0.209989
45	0.317615	0.210344
46	0.311156	0.210537
47	0.305039	0.210631
48	0.299301	0.210593
49	0.293897	0.210436
50	0.288830	0.210165
51	0.284021	0.209791
52	0.279374	0.209313
53	0.274875	0.208728
54	0.270346	0.208024
55	0.265658	0.207289
56	0.260961	0.206627
57	0.256673	0.206051
58	0.252795	0.205427
59	0.249141	0.204757
60	0.245672	0.204044
61	0.242319	0.203301
62	0.239065	0.202546
63	0.235925	0.201761
64	0.232917	0.200958
65	0.230026	0.200118
66	0.227247	0.199264
67	0.224604	0.198384
68	0.222046	0.197504
69	0.219596	0.196635
70	0.217228	0.195766
71	0.214933	0.194926
72	0.212724	0.194083
73	0.210588	0.193260
74	0.208508	0.192460
75	0.206479	0.191664
76	0.204514	0.190890
77	0.202592	0.190119
78	0.200723	0.189353
79	0.198868	0.188562
80	0.197013	0.187738
81	0.195149	0.186881
82	0.193302	0.185997
83	0.191477	0.185125
84	0.189689	0.184264
85	0.187901	0.183351
86	0.186090	0.182416
87	0.184257	0.181409
88	0.182399	0.180382
89	0.180554	0.179394
90	0.178727	0.178458
91	0.176884	0.177551
92	0.175058	0.176702
93	0.173247	0.175925
94	0.171534	0.175239
95	0.169895	0.174555
96	0.168334	0.173849
97	0.166826	0.173084
98	0.165282	0.172241
99	0.163726	0.171322
100	0.162247	0.170470
101	0.160837	0.169736
102	0.159462	0.169036
103	0.158150	0.168372
104	0.156850	0.167739
105	0.155591	0.167138
106	0.154359	0.166569
107	0.153173	0.166011
108	0.151995	0.165479
109	0.150868	0.164947
110	0.149722	0.164435
111	0.148628	0.163928
112	0.147537	0.163438
113	0.146471	0.162921
114	0.145396	0.162422
115	0.144360	0.161902
116	0.143307	0.161362
117	0.142274	0.160820
118	0.141241	0.160257
119	0.140220	0.159673
120	0.139181	0.159072
121	0.138143	0.158446
122	0.137089	0.157768
123	0.135995	0.157046
124	0.134890	0.156319
125	0.133803	0.155598
126	0.132705	0.154900
127	0.131617	0.154211
128	0.130502	0.153506
129	0.129385	0.152761
130	0.128270	0.152010
131	0.127180	0.151255
132	0.126100	0.150485
133	0.124979	0.149714
134	0.123832	0.148863
135	0.122638	0.147977
136	0.121379	0.147038
137	0.120113	0.146121
138	0.118865	0.145237
139	0.117585	0.144375
140	0.116357	0.143546
141	0.115185	0.142798
142	0.114078	0.142070
143	0.112963	0.141340
144	0.111812	0.140527
145	0.110548	0.139608
146	0.109299	0.138817
147	0.108147	0.138162
148	0.107014	0.137586
149	0.105876	0.137026
150	0.104713	0.136465
151	0.103565	0.135974
152	0.102489	0.135591
153	0.101543	0.135329
154	0.100713	0.135105
155	0.099962	0.134909
156	0.099284	0.134748
157	0.098637	0.134568
158	0.098019	0.134398
159	0.097417	0.134234
160	0.096853	0.134085
161	0.096312	0.133927
162	0.095772	0.133759
163	0.095231	0.133596
164	0.094707	0.133419
165	0.094178	0.133229
166	0.093672	0.133030
167	0.093141	0.132820
168	0.092627	0.132611
169	0.092093	0.132377
170	0.091520	0.132134
171	0.090939	0.131829
172	0.090329	0.131557
173	0.089775	0.131355
174	0.089217	0.131205
175	0.088699	0.131046
176	0.088222	0.130915
177	0.087729	0.130798
178	0.087283	0.130670
179	0.086832	0.130537
180	0.086373	0.130435
181	0.085906	0.130332
182	0.085487	0.130247
183	0.085061	0.130203
184	0.084676	0.130128
185	0.084297	0.130089
186	0.083939	0.130053
187	0.083592	0.130010
188	0.083269	0.129986
189	0.082911	0.129951
190	0.082614	0.129932
191	0.082311	0.129910
192	0.082023	0.129884
193	0.081741	0.129857
194	0.081479	0.129833
195	0.081240	0.129804
196	0.080963	0.129765
197	0.080721	0.129732
198	0.080477	0.129698
199	0.080247	0.129656
200	0.080007	0.129621
