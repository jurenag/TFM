#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [8, 4, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 93.6175%
#Sample standard deviation for averaged success rate: 0.07876458333330139%
#Epoch	Loss	Sample STD
1	0.701164	0.013565
2	0.693242	0.000135
3	0.693184	0.000048
4	0.693144	0.000079
5	0.693083	0.000162
6	0.692961	0.000374
7	0.692682	0.000934
8	0.692014	0.002456
9	0.690425	0.006134
10	0.687231	0.012982
11	0.681991	0.022349
12	0.674455	0.033118
13	0.664295	0.045843
14	0.651628	0.061240
15	0.637124	0.078338
16	0.621539	0.095935
17	0.605746	0.112271
18	0.590102	0.126123
19	0.573815	0.137960
20	0.556257	0.148860
21	0.537511	0.158957
22	0.517483	0.168120
23	0.496417	0.176546
24	0.475422	0.184472
25	0.455668	0.191561
26	0.437430	0.197867
27	0.420966	0.203330
28	0.406187	0.207800
29	0.392770	0.211066
30	0.380131	0.213483
31	0.368006	0.215476
32	0.356677	0.217311
33	0.346091	0.218859
34	0.336347	0.219785
35	0.327429	0.220369
36	0.319456	0.220738
37	0.312106	0.220775
38	0.305062	0.220406
39	0.298256	0.219825
40	0.291239	0.218987
41	0.283982	0.218242
42	0.276773	0.217407
43	0.268802	0.216429
44	0.260891	0.215327
45	0.253061	0.214560
46	0.246165	0.213611
47	0.239579	0.212482
48	0.233330	0.211119
49	0.227153	0.209623
50	0.220623	0.208192
51	0.214171	0.207062
52	0.208078	0.206181
53	0.202358	0.205340
54	0.197108	0.204450
55	0.192334	0.203419
56	0.187802	0.202214
57	0.183376	0.200888
58	0.178995	0.199458
59	0.174559	0.197953
60	0.170202	0.196515
61	0.166115	0.195107
62	0.162188	0.193696
63	0.158230	0.192150
64	0.154130	0.190528
65	0.149938	0.189067
66	0.145823	0.187908
67	0.141974	0.186922
68	0.138496	0.186064
69	0.135235	0.185210
70	0.132089	0.184355
71	0.129155	0.183638
72	0.126385	0.183005
73	0.123688	0.182325
74	0.120985	0.181603
75	0.118076	0.180694
76	0.115006	0.179749
77	0.112275	0.178943
78	0.109896	0.178182
79	0.107709	0.177397
80	0.105748	0.176644
81	0.103862	0.175782
82	0.101940	0.174687
83	0.099858	0.173259
84	0.097260	0.171302
85	0.094395	0.169401
86	0.091658	0.167638
87	0.089503	0.166329
88	0.087649	0.165144
89	0.085868	0.163890
90	0.084048	0.162595
91	0.082172	0.161425
92	0.080138	0.160499
93	0.078679	0.159847
94	0.077199	0.159048
95	0.075608	0.158321
96	0.074386	0.157761
97	0.073149	0.156787
98	0.072142	0.156158
99	0.071333	0.155795
100	0.070568	0.155460
101	0.069871	0.155135
102	0.069132	0.154751
103	0.068416	0.154279
104	0.067659	0.153786
105	0.065981	0.152076
106	0.063789	0.150682
107	0.061843	0.149650
108	0.060851	0.149484
109	0.060209	0.149401
110	0.059685	0.149319
111	0.059251	0.149213
112	0.058820	0.149113
113	0.058427	0.149020
114	0.058074	0.148947
115	0.057777	0.148894
116	0.057499	0.148870
117	0.057259	0.148847
118	0.057051	0.148827
119	0.056823	0.148813
120	0.056657	0.148805
121	0.056461	0.148794
122	0.056311	0.148770
123	0.056159	0.148743
124	0.056012	0.148708
125	0.055890	0.148672
126	0.055742	0.148641
127	0.055620	0.148570
128	0.055517	0.148509
129	0.055396	0.148446
130	0.055286	0.148373
131	0.055180	0.148291
132	0.055077	0.148199
133	0.054953	0.148066
134	0.054826	0.147880
135	0.054678	0.147645
136	0.054440	0.147154
137	0.053643	0.145471
138	0.052864	0.144134
139	0.052337	0.143407
140	0.051857	0.142866
141	0.050894	0.142170
142	0.050404	0.141925
143	0.050128	0.141775
144	0.049918	0.141592
145	0.049693	0.141288
146	0.049443	0.140885
147	0.049242	0.140580
148	0.049061	0.140284
149	0.048898	0.140020
150	0.048748	0.139782
151	0.048605	0.139583
152	0.048449	0.139323
153	0.048266	0.139008
154	0.048039	0.138653
155	0.047721	0.138128
156	0.047440	0.137746
157	0.047129	0.137370
158	0.046732	0.136917
159	0.046008	0.136375
160	0.045513	0.136174
161	0.045295	0.136107
162	0.045128	0.136059
163	0.044987	0.136012
164	0.044892	0.135964
165	0.044805	0.135917
166	0.044713	0.135884
167	0.044648	0.135807
168	0.044600	0.135732
169	0.044532	0.135637
170	0.044455	0.135497
171	0.044347	0.135327
172	0.044258	0.135129
173	0.044145	0.134858
174	0.044013	0.134508
175	0.043741	0.133757
176	0.043354	0.132767
177	0.043186	0.132280
178	0.043023	0.131931
179	0.042887	0.131630
180	0.042758	0.131334
181	0.042656	0.131094
182	0.042529	0.130842
183	0.042441	0.130628
184	0.042329	0.130411
185	0.042219	0.130172
186	0.042113	0.129947
187	0.042022	0.129752
188	0.041919	0.129532
189	0.041781	0.129269
190	0.041656	0.128990
191	0.041541	0.128708
192	0.041394	0.128417
193	0.041228	0.128103
194	0.041054	0.127722
195	0.040811	0.127224
196	0.040425	0.126423
197	0.039075	0.124233
198	0.037882	0.123030
199	0.037286	0.122425
200	0.036213	0.121529
