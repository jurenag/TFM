#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/entanglement_data/input/entangled_1.txt;
#Architecture of the MLP: [8, 4, 2, 1]; Number of epochs: 200; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.05; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 93.79425%
#Sample standard deviation for averaged success rate: 0.04569108017675335%
#Epoch	Loss	Sample STD
1	0.707120	0.016712
2	0.693271	0.000161
3	0.693164	0.000049
4	0.693126	0.000092
5	0.693042	0.000212
6	0.692873	0.000516
7	0.692501	0.001261
8	0.691583	0.003127
9	0.689449	0.007316
10	0.685011	0.015399
11	0.677306	0.027903
12	0.666239	0.043914
13	0.652693	0.061963
14	0.637774	0.080640
15	0.621850	0.099034
16	0.605311	0.116515
17	0.588851	0.132542
18	0.573236	0.145861
19	0.558615	0.155918
20	0.544696	0.163392
21	0.530778	0.169812
22	0.516561	0.176307
23	0.502422	0.182761
24	0.488421	0.189399
25	0.474841	0.196295
26	0.462117	0.203106
27	0.450774	0.208979
28	0.440395	0.213587
29	0.430482	0.216881
30	0.420761	0.219247
31	0.411474	0.221091
32	0.402755	0.222492
33	0.394480	0.223523
34	0.386454	0.224224
35	0.378486	0.224676
36	0.370573	0.224972
37	0.362699	0.225230
38	0.355129	0.225276
39	0.347779	0.224820
40	0.340332	0.223920
41	0.332689	0.222722
42	0.324988	0.221274
43	0.317199	0.219444
44	0.309346	0.217349
45	0.301444	0.215249
46	0.293627	0.213348
47	0.286066	0.211696
48	0.278814	0.210195
49	0.271904	0.208854
50	0.265385	0.207713
51	0.259333	0.206679
52	0.253432	0.205798
53	0.247628	0.205032
54	0.242100	0.204224
55	0.236775	0.203247
56	0.231492	0.202038
57	0.226156	0.200766
58	0.220661	0.199421
59	0.215087	0.198158
60	0.209453	0.197116
61	0.203937	0.196251
62	0.198484	0.195384
63	0.193014	0.194476
64	0.187517	0.193576
65	0.182212	0.192762
66	0.177203	0.191639
67	0.172174	0.190195
68	0.167302	0.188717
69	0.162410	0.187104
70	0.157267	0.185326
71	0.151178	0.183717
72	0.146016	0.182740
73	0.141396	0.181994
74	0.137407	0.181339
75	0.133793	0.180282
76	0.130571	0.179399
77	0.127567	0.178458
78	0.124324	0.177096
79	0.120419	0.175295
80	0.116668	0.174003
81	0.113210	0.173064
82	0.110338	0.172316
83	0.107929	0.171492
84	0.105656	0.170572
85	0.103566	0.169530
86	0.101481	0.168375
87	0.099348	0.166992
88	0.097173	0.165467
89	0.094964	0.163906
90	0.092584	0.162214
91	0.090151	0.160642
92	0.087834	0.159110
93	0.085404	0.157378
94	0.082732	0.155440
95	0.080145	0.153442
96	0.077650	0.151350
97	0.074986	0.149127
98	0.071715	0.146416
99	0.068565	0.144479
100	0.066038	0.143113
101	0.063822	0.141912
102	0.062095	0.141083
103	0.060618	0.140304
104	0.059425	0.139782
105	0.058404	0.139397
106	0.057394	0.138971
107	0.055986	0.138413
108	0.054936	0.138248
109	0.054249	0.138096
110	0.053635	0.137829
111	0.052980	0.137370
112	0.052456	0.137111
113	0.052027	0.136936
114	0.051658	0.136804
115	0.051316	0.136681
116	0.051004	0.136557
117	0.050684	0.136431
118	0.050396	0.136255
119	0.050086	0.136071
120	0.049794	0.135917
121	0.049521	0.135775
122	0.049237	0.135570
123	0.048945	0.135354
124	0.048669	0.135137
125	0.048345	0.134864
126	0.048008	0.134530
127	0.047622	0.134092
128	0.047102	0.133357
129	0.046511	0.132588
130	0.045980	0.132002
131	0.045528	0.131639
132	0.045105	0.131231
133	0.044358	0.130496
134	0.043227	0.129920
135	0.042837	0.129918
136	0.042623	0.129930
137	0.042468	0.129959
138	0.042343	0.129970
139	0.042274	0.129975
140	0.042176	0.129990
141	0.042103	0.129989
142	0.042034	0.129992
143	0.041979	0.130001
144	0.041903	0.129999
145	0.041859	0.129992
146	0.041824	0.130003
147	0.041770	0.129997
148	0.041728	0.129995
149	0.041689	0.129989
150	0.041625	0.129987
151	0.041587	0.129980
152	0.041544	0.129970
153	0.041512	0.129959
154	0.041458	0.129961
155	0.041435	0.129949
156	0.041376	0.129947
157	0.041354	0.129939
158	0.041323	0.129933
159	0.041300	0.129925
160	0.041253	0.129918
161	0.041221	0.129913
162	0.041189	0.129900
163	0.041173	0.129897
164	0.041118	0.129899
165	0.041095	0.129876
166	0.041084	0.129873
167	0.041068	0.129866
168	0.041034	0.129853
169	0.040993	0.129845
170	0.040973	0.129838
171	0.040934	0.129820
172	0.040921	0.129793
173	0.040888	0.129777
174	0.040871	0.129764
175	0.040841	0.129754
176	0.040809	0.129739
177	0.040773	0.129729
178	0.040761	0.129711
179	0.040746	0.129689
180	0.040713	0.129662
181	0.040684	0.129648
182	0.040655	0.129613
183	0.040636	0.129583
184	0.040609	0.129556
185	0.040578	0.129515
186	0.040545	0.129484
187	0.040521	0.129451
188	0.040491	0.129422
189	0.040473	0.129375
190	0.040417	0.129325
191	0.040394	0.129278
192	0.040351	0.129235
193	0.040343	0.129183
194	0.040302	0.129140
195	0.040254	0.129084
196	0.040243	0.129027
197	0.040194	0.128962
198	0.040145	0.128877
199	0.040098	0.128799
200	0.040071	0.128743
