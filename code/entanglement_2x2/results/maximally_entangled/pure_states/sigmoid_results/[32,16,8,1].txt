#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.89997499374844%
#Sample standard deviation for averaged success rate: 0.01651168300451124%
#Same average success rate for supplementary tests: [0.5011550000000001, 0.501925, 0.5052049999999999, 0.5363349999999999, 0.66927, 0.5805799999999999, 0.59419, 0.6242549999999999, 0.67807, 0.748925]%
#Sample STD for averaged success rate in supplementary tests: [0.0035183033267116113, 0.003513550870380277, 0.0034937401733314403, 0.003307191163018855, 0.002543724307978362, 0.003092821233113871, 0.003027344082690304, 0.002869034637077427, 0.0025488926527023462, 0.0020634297222706676]%
#Epoch	Loss	Sample STD
1	0.698046	0.002646
2	0.693394	0.000034
3	0.693251	0.000065
4	0.693237	0.000049
5	0.692934	0.000191
6	0.691511	0.001069
7	0.685228	0.005215
8	0.662422	0.018278
9	0.614700	0.038981
10	0.562087	0.057530
11	0.525089	0.068258
12	0.497878	0.072123
13	0.466156	0.072611
14	0.424376	0.074656
15	0.393246	0.077985
16	0.373032	0.078876
17	0.356098	0.078117
18	0.335052	0.075490
19	0.309766	0.074440
20	0.295212	0.075407
21	0.285522	0.075709
22	0.275883	0.075001
23	0.264451	0.073026
24	0.252372	0.070464
25	0.239915	0.067575
26	0.225945	0.064137
27	0.209928	0.060180
28	0.193357	0.056414
29	0.178252	0.052917
30	0.165217	0.049902
31	0.154060	0.047212
32	0.143862	0.044809
33	0.134606	0.042438
34	0.126126	0.040159
35	0.117769	0.037970
36	0.110160	0.035923
37	0.102894	0.034159
38	0.096175	0.032600
39	0.090342	0.031286
40	0.084971	0.030155
41	0.080004	0.029130
42	0.075662	0.028158
43	0.071522	0.027267
44	0.067482	0.026408
45	0.063772	0.025575
46	0.060335	0.024801
47	0.056932	0.024078
48	0.053755	0.023285
49	0.050680	0.022468
50	0.047733	0.021582
51	0.044771	0.020613
52	0.041919	0.019443
53	0.039006	0.018085
54	0.035633	0.016269
55	0.031796	0.013717
56	0.027075	0.010227
57	0.022706	0.007141
58	0.019793	0.005415
59	0.017978	0.004493
60	0.016521	0.003864
61	0.015324	0.003457
62	0.014349	0.003188
63	0.013496	0.002977
64	0.012723	0.002801
65	0.012287	0.002739
66	0.011737	0.002657
67	0.011252	0.002598
68	0.010751	0.002574
69	0.010352	0.002498
70	0.010028	0.002508
71	0.009638	0.002469
72	0.009380	0.002464
73	0.009078	0.002403
74	0.008868	0.002434
75	0.008676	0.002391
76	0.008511	0.002438
77	0.008223	0.002410
78	0.007938	0.002327
79	0.007819	0.002358
80	0.007737	0.002343
81	0.007638	0.002363
82	0.007518	0.002350
83	0.007339	0.002303
84	0.007194	0.002310
85	0.007078	0.002289
86	0.006943	0.002266
87	0.006831	0.002254
88	0.006846	0.002270
89	0.006654	0.002231
90	0.006622	0.002250
91	0.006578	0.002261
92	0.006370	0.002202
93	0.006447	0.002229
94	0.006286	0.002192
95	0.006122	0.002134
96	0.006088	0.002171
97	0.006075	0.002151
98	0.005887	0.002090
99	0.005891	0.002097
100	0.005753	0.002069
101	0.005781	0.002066
102	0.005724	0.002077
103	0.005654	0.002039
104	0.005526	0.002009
105	0.005398	0.001957
106	0.005385	0.001972
107	0.005404	0.001975
108	0.005236	0.001916
109	0.005130	0.001862
110	0.005157	0.001913
111	0.005105	0.001860
112	0.005052	0.001875
113	0.004883	0.001795
114	0.004961	0.001841
115	0.004862	0.001819
116	0.004856	0.001806
117	0.004770	0.001802
118	0.004746	0.001808
119	0.004660	0.001769
120	0.004593	0.001757
121	0.004553	0.001760
122	0.004537	0.001743
123	0.004567	0.001745
124	0.004514	0.001752
125	0.004440	0.001732
126	0.004362	0.001688
127	0.004388	0.001708
128	0.004341	0.001745
129	0.004297	0.001673
130	0.004284	0.001701
131	0.004145	0.001611
132	0.004135	0.001665
133	0.004080	0.001635
134	0.004090	0.001621
135	0.004051	0.001643
136	0.004030	0.001624
137	0.003867	0.001552
138	0.003952	0.001601
139	0.003850	0.001572
140	0.003925	0.001598
141	0.003872	0.001563
142	0.003801	0.001548
143	0.003831	0.001576
144	0.003749	0.001565
145	0.003693	0.001532
146	0.003655	0.001516
147	0.003522	0.001462
148	0.003621	0.001517
149	0.003537	0.001494
150	0.003461	0.001454
151	0.003623	0.001539
