#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [8, 4, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.89497374343587%
#Sample standard deviation for averaged success rate: 0.016503915697729393%
#Same average success rate for supplementary tests: [0.5008050000000001, 0.503515, 0.5180699999999999, 0.562465, 0.72525, 0.6192549999999999, 0.6400399999999999, 0.6813, 0.7462449999999999, 0.834455]%
#Sample STD for averaged success rate in supplementary tests: [0.0035213623214247628, 0.0035041129032538317, 0.0034119384160620487, 0.0031368664601398, 0.002349451398731203, 0.0028892104888273555, 0.0027957986193572685, 0.00260677300507735, 0.0022811060910773117, 0.0016925624504726586]%
#Epoch	Loss	Sample STD
1	0.696557	0.001143
2	0.693198	0.000030
3	0.693077	0.000065
4	0.692951	0.000131
5	0.692645	0.000286
6	0.692200	0.000580
7	0.691361	0.001166
8	0.689856	0.002170
9	0.687317	0.003759
10	0.683617	0.005822
11	0.678116	0.008222
12	0.669653	0.011317
13	0.655976	0.016118
14	0.635626	0.023458
15	0.610565	0.031959
16	0.583479	0.039931
17	0.556282	0.046539
18	0.529358	0.051915
19	0.502901	0.056167
20	0.477362	0.059537
21	0.453137	0.062335
22	0.430775	0.064846
23	0.410731	0.066966
24	0.392009	0.068562
25	0.373707	0.069613
26	0.355446	0.070164
27	0.337317	0.070460
28	0.319639	0.070359
29	0.303037	0.069838
30	0.287339	0.068883
31	0.272598	0.067615
32	0.258490	0.066054
33	0.244574	0.064125
34	0.231055	0.061853
35	0.217555	0.059339
36	0.204581	0.057034
37	0.192970	0.055196
38	0.182636	0.053802
39	0.173411	0.052666
40	0.165143	0.051667
41	0.157773	0.050660
42	0.150858	0.049618
43	0.144258	0.048428
44	0.137945	0.047117
45	0.131850	0.045716
46	0.125945	0.044184
47	0.120090	0.042560
48	0.114431	0.040838
49	0.108921	0.039074
50	0.103568	0.037260
51	0.098248	0.035404
52	0.092922	0.033449
53	0.087696	0.031488
54	0.082563	0.029524
55	0.077443	0.027546
56	0.072505	0.025622
57	0.067716	0.023772
58	0.063139	0.021994
59	0.058808	0.020343
60	0.054751	0.018826
61	0.050948	0.017437
62	0.047435	0.016159
63	0.044295	0.015035
64	0.041334	0.013946
65	0.038586	0.012899
66	0.036078	0.011905
67	0.033656	0.010953
68	0.031389	0.009970
69	0.029157	0.009011
70	0.027118	0.008059
71	0.025093	0.007102
72	0.023130	0.006210
73	0.021315	0.005360
74	0.019535	0.004577
75	0.017951	0.003898
76	0.016490	0.003333
77	0.015184	0.002919
78	0.014018	0.002614
79	0.012953	0.002401
80	0.012063	0.002281
81	0.011261	0.002203
82	0.010565	0.002168
83	0.009953	0.002127
84	0.009418	0.002106
85	0.008928	0.002074
86	0.008517	0.002056
87	0.008139	0.002035
88	0.007760	0.002007
89	0.007469	0.001984
90	0.007195	0.001959
91	0.006920	0.001935
92	0.006669	0.001904
93	0.006458	0.001883
94	0.006220	0.001862
95	0.006059	0.001837
96	0.005854	0.001814
97	0.005721	0.001788
98	0.005561	0.001777
99	0.005375	0.001746
100	0.005290	0.001735
101	0.005165	0.001724
102	0.005030	0.001699
103	0.004902	0.001671
104	0.004801	0.001662
105	0.004715	0.001648
106	0.004594	0.001628
107	0.004492	0.001609
108	0.004374	0.001587
109	0.004298	0.001566
110	0.004240	0.001565
111	0.004179	0.001551
112	0.004080	0.001538
113	0.003991	0.001509
114	0.003944	0.001508
115	0.003880	0.001497
116	0.003785	0.001487
117	0.003714	0.001471
118	0.003655	0.001462
119	0.003625	0.001448
120	0.003553	0.001433
121	0.003504	0.001426
122	0.003470	0.001421
123	0.003388	0.001404
124	0.003383	0.001421
125	0.003272	0.001400
126	0.003221	0.001376
127	0.003198	0.001371
128	0.003150	0.001364
129	0.003072	0.001345
130	0.003049	0.001337
131	0.003007	0.001327
132	0.002956	0.001311
133	0.002899	0.001300
134	0.002865	0.001286
135	0.002794	0.001285
136	0.002733	0.001252
137	0.002774	0.001281
138	0.002687	0.001244
139	0.002614	0.001232
140	0.002605	0.001228
141	0.002566	0.001220
142	0.002531	0.001205
143	0.002486	0.001200
144	0.002442	0.001183
145	0.002418	0.001186
146	0.002394	0.001183
147	0.002387	0.001165
148	0.002318	0.001148
149	0.002286	0.001155
150	0.002293	0.001136
151	0.002218	0.001140
