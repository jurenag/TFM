#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [4, 2, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 95.75893973493375%
#Sample standard deviation for averaged success rate: 0.10245596163713715%
#Same average success rate for supplementary tests: [0.503575, 0.51531, 0.53253, 0.5621149999999999, 0.664185, 0.6035900000000001, 0.6212, 0.6510100000000001, 0.694805, 0.761545]%
#Sample STD for averaged success rate in supplementary tests: [0.0030661027329086676, 0.002994692003361948, 0.0028854618269871474, 0.002696407116655421, 0.002276310455265274, 0.0026017024416716064, 0.0025197079195811577, 0.002367361188116421, 0.0021589524308678032, 0.0018301053654776274]%
#Epoch	Loss	Sample STD
1	0.699144	0.003499
2	0.693259	0.000053
3	0.693146	0.000015
4	0.693104	0.000028
5	0.693063	0.000039
6	0.692973	0.000070
7	0.692854	0.000114
8	0.692620	0.000209
9	0.692290	0.000370
10	0.691714	0.000652
11	0.690816	0.001081
12	0.689395	0.001725
13	0.687214	0.002712
14	0.683879	0.004167
15	0.678977	0.006239
16	0.672158	0.009044
17	0.663303	0.012528
18	0.652397	0.016653
19	0.639873	0.021052
20	0.626016	0.025593
21	0.610796	0.030050
22	0.593872	0.034434
23	0.575238	0.038712
24	0.555237	0.042756
25	0.534025	0.046572
26	0.512264	0.050242
27	0.490720	0.053713
28	0.469949	0.057003
29	0.450533	0.060042
30	0.432407	0.062764
31	0.415540	0.065135
32	0.399868	0.067270
33	0.385388	0.069266
34	0.372329	0.071072
35	0.360659	0.072622
36	0.349989	0.073917
37	0.339828	0.075023
38	0.330151	0.076004
39	0.320871	0.076896
40	0.312176	0.077780
41	0.304168	0.078634
42	0.296929	0.079427
43	0.290648	0.080050
44	0.285218	0.080499
45	0.280397	0.080775
46	0.276050	0.080917
47	0.271988	0.080946
48	0.268087	0.080850
49	0.264276	0.080640
50	0.260510	0.080271
51	0.256538	0.079744
52	0.252441	0.079023
53	0.248163	0.078189
54	0.243781	0.077228
55	0.239359	0.076191
56	0.234938	0.075103
57	0.230675	0.074031
58	0.226382	0.072947
59	0.222193	0.071901
60	0.217987	0.070853
61	0.213778	0.069838
62	0.209688	0.068866
63	0.205642	0.067921
64	0.201630	0.067017
65	0.197615	0.066142
66	0.193792	0.065325
67	0.190001	0.064542
68	0.186345	0.063795
69	0.182859	0.063115
70	0.179561	0.062485
71	0.176384	0.061900
72	0.173427	0.061343
73	0.170625	0.060824
74	0.167949	0.060338
75	0.165409	0.059882
76	0.162993	0.059422
77	0.160760	0.058999
78	0.158586	0.058586
79	0.156461	0.058174
80	0.154451	0.057759
81	0.152484	0.057349
82	0.150607	0.056934
83	0.148760	0.056519
84	0.146924	0.056089
85	0.145113	0.055665
86	0.143320	0.055230
87	0.141574	0.054794
88	0.139799	0.054351
89	0.138030	0.053892
90	0.136229	0.053430
91	0.134486	0.052973
92	0.132724	0.052510
93	0.130989	0.052054
94	0.129270	0.051602
95	0.127602	0.051170
96	0.125934	0.050748
97	0.124328	0.050346
98	0.122766	0.049963
99	0.121261	0.049618
100	0.119861	0.049286
101	0.118488	0.048984
102	0.117203	0.048697
103	0.115939	0.048431
104	0.114741	0.048177
105	0.113619	0.047956
106	0.112536	0.047750
107	0.111517	0.047566
108	0.110549	0.047391
109	0.109665	0.047227
110	0.108794	0.047090
111	0.108005	0.046948
112	0.107174	0.046834
113	0.106459	0.046722
114	0.105771	0.046615
115	0.105119	0.046518
116	0.104534	0.046437
117	0.103953	0.046359
118	0.103365	0.046289
119	0.102878	0.046217
120	0.102362	0.046154
121	0.101857	0.046097
122	0.101385	0.046045
123	0.100952	0.045994
124	0.100531	0.045935
125	0.100127	0.045890
126	0.099690	0.045852
127	0.099301	0.045815
128	0.098938	0.045772
129	0.098548	0.045731
130	0.098231	0.045700
131	0.097896	0.045660
132	0.097521	0.045631
133	0.097189	0.045599
134	0.096916	0.045569
135	0.096560	0.045539
136	0.096302	0.045504
137	0.095963	0.045474
138	0.095691	0.045444
139	0.095400	0.045421
140	0.095122	0.045390
141	0.094828	0.045361
142	0.094595	0.045340
143	0.094258	0.045317
144	0.094035	0.045286
145	0.093718	0.045263
146	0.093482	0.045238
147	0.093248	0.045208
148	0.092987	0.045195
149	0.092704	0.045169
150	0.092477	0.045133
151	0.092209	0.045117
