#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [16, 8, 4, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.91497874468618%
#Sample standard deviation for averaged success rate: 0.016150036961862096%
#Same average success rate for supplementary tests: [0.500545, 0.501165, 0.5104650000000001, 0.58485, 0.7446450000000001, 0.56792, 0.5910449999999999, 0.6432899999999999, 0.73011, 0.822975]%
#Sample STD for averaged success rate in supplementary tests: [0.0035259693913518306, 0.003522145246685605, 0.0034641375822490068, 0.0030006039183804313, 0.0020667283804965755, 0.003176435687999995, 0.0030638073370807778, 0.0027794241840712257, 0.002265252611741128, 0.001638683150810434]%
#Epoch	Loss	Sample STD
1	0.694766	0.000534
2	0.693277	0.000016
3	0.693198	0.000016
4	0.693194	0.000016
5	0.693146	0.000026
6	0.693047	0.000052
7	0.692775	0.000173
8	0.691965	0.000663
9	0.689447	0.002448
10	0.682709	0.007232
11	0.668665	0.016165
12	0.645923	0.028103
13	0.616658	0.040629
14	0.584074	0.052270
15	0.551134	0.062236
16	0.522161	0.070007
17	0.498669	0.074157
18	0.475783	0.075253
19	0.447589	0.074711
20	0.416087	0.074172
21	0.381795	0.073167
22	0.344864	0.072141
23	0.315289	0.072264
24	0.294893	0.072171
25	0.279859	0.071748
26	0.267672	0.071212
27	0.257260	0.070732
28	0.248168	0.070252
29	0.239769	0.069777
30	0.231384	0.069149
31	0.222954	0.068218
32	0.213788	0.066728
33	0.203795	0.064525
34	0.193449	0.062067
35	0.183788	0.059904
36	0.174549	0.057968
37	0.165203	0.056051
38	0.155727	0.054109
39	0.146718	0.052281
40	0.138144	0.050610
41	0.130002	0.049027
42	0.122275	0.047518
43	0.114798	0.045945
44	0.107492	0.044334
45	0.099998	0.042481
46	0.092345	0.040405
47	0.084457	0.038069
48	0.076514	0.035556
49	0.068616	0.032966
50	0.060935	0.030366
51	0.053831	0.027805
52	0.047425	0.025215
53	0.041581	0.022520
54	0.036305	0.019688
55	0.031664	0.016933
56	0.027623	0.014330
57	0.024039	0.011886
58	0.021187	0.009915
59	0.018619	0.008249
60	0.016568	0.006918
61	0.014839	0.005886
62	0.013284	0.005059
63	0.012036	0.004551
64	0.011017	0.004219
65	0.010226	0.004066
66	0.009437	0.003942
67	0.008933	0.003916
68	0.008451	0.003868
69	0.008086	0.003880
70	0.007762	0.003875
71	0.007549	0.003872
72	0.007319	0.003842
73	0.007148	0.003848
74	0.006970	0.003811
75	0.006893	0.003789
76	0.006777	0.003781
77	0.006630	0.003720
78	0.006551	0.003723
79	0.006442	0.003697
80	0.006347	0.003663
81	0.006165	0.003605
82	0.006160	0.003624
83	0.006081	0.003591
84	0.005916	0.003577
85	0.005844	0.003530
86	0.005800	0.003505
87	0.005713	0.003478
88	0.005633	0.003462
89	0.005523	0.003408
90	0.005456	0.003367
91	0.005436	0.003372
92	0.005340	0.003316
93	0.005271	0.003324
94	0.005180	0.003268
95	0.005141	0.003266
96	0.005087	0.003233
97	0.004935	0.003173
98	0.004920	0.003139
99	0.004818	0.003130
100	0.004734	0.003085
101	0.004687	0.003042
102	0.004627	0.003033
103	0.004607	0.003012
104	0.004592	0.002991
105	0.004459	0.002922
106	0.004318	0.002870
107	0.004342	0.002878
108	0.004246	0.002847
109	0.004128	0.002796
110	0.004117	0.002758
111	0.004034	0.002709
112	0.003972	0.002683
113	0.003910	0.002644
114	0.003830	0.002562
115	0.003784	0.002534
116	0.003720	0.002516
117	0.003579	0.002461
118	0.003524	0.002410
119	0.003513	0.002393
120	0.003397	0.002311
121	0.003295	0.002257
122	0.003269	0.002244
123	0.003217	0.002192
124	0.003102	0.002131
125	0.003024	0.002091
126	0.002996	0.002053
127	0.002977	0.001983
128	0.002906	0.001952
129	0.002807	0.001896
130	0.002794	0.001854
131	0.002682	0.001788
132	0.002601	0.001752
133	0.002522	0.001726
134	0.002457	0.001680
135	0.002415	0.001637
136	0.002361	0.001569
137	0.002290	0.001546
138	0.002283	0.001531
139	0.002170	0.001460
140	0.002133	0.001419
141	0.002121	0.001407
142	0.002091	0.001378
143	0.002028	0.001348
144	0.001948	0.001285
145	0.001935	0.001289
146	0.001902	0.001268
147	0.001840	0.001229
148	0.001832	0.001213
149	0.001756	0.001186
150	0.001718	0.001141
151	0.001704	0.001154
