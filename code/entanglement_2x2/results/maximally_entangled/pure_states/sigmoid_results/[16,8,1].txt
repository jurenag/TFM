#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.8649662415604%
#Sample standard deviation for averaged success rate: 0.02281891604729243%
#Same average success rate for supplementary tests: [0.501375, 0.50214, 0.5047800000000001, 0.5336099999999999, 0.67404, 0.57399, 0.58837, 0.62373, 0.68538, 0.7545700000000001]%
#Sample STD for averaged success rate in supplementary tests: [0.0035156906104988816, 0.0035110640865697682, 0.003495691287857095, 0.003323577649912817, 0.0025492163344839926, 0.0031422641510541406, 0.003071544750610025, 0.0028888569287868855, 0.0025327875512960022, 0.002075694041760488]%
#Epoch	Loss	Sample STD
1	0.695645	0.000970
2	0.693282	0.000034
3	0.693130	0.000039
4	0.693069	0.000049
5	0.692821	0.000092
6	0.692525	0.000199
7	0.691796	0.000447
8	0.690123	0.001206
9	0.686160	0.003074
10	0.677843	0.006920
11	0.662715	0.013662
12	0.638764	0.023171
13	0.607740	0.032881
14	0.569849	0.041706
15	0.526308	0.050351
16	0.482090	0.057250
17	0.441739	0.061180
18	0.405635	0.062388
19	0.372741	0.062053
20	0.342400	0.061316
21	0.316111	0.061032
22	0.294794	0.061371
23	0.278469	0.062005
24	0.265315	0.062574
25	0.254374	0.062915
26	0.244659	0.062885
27	0.236117	0.062472
28	0.228068	0.061731
29	0.220650	0.060878
30	0.213800	0.059992
31	0.207249	0.059061
32	0.200903	0.058056
33	0.194448	0.056857
34	0.187776	0.055558
35	0.180874	0.054043
36	0.173566	0.052290
37	0.165997	0.050342
38	0.158152	0.048255
39	0.150268	0.046075
40	0.142378	0.043811
41	0.134457	0.041479
42	0.126657	0.039047
43	0.118662	0.036415
44	0.110553	0.033464
45	0.102311	0.030281
46	0.094028	0.026917
47	0.085689	0.023489
48	0.077609	0.020113
49	0.070053	0.017035
50	0.063073	0.014286
51	0.056548	0.011918
52	0.050607	0.009898
53	0.045423	0.008333
54	0.040816	0.007191
55	0.036993	0.006429
56	0.033643	0.005920
57	0.030829	0.005569
58	0.028492	0.005311
59	0.026528	0.005073
60	0.024785	0.004863
61	0.023214	0.004646
62	0.021865	0.004455
63	0.020612	0.004265
64	0.019517	0.004104
65	0.018421	0.003946
66	0.017532	0.003812
67	0.016606	0.003677
68	0.015791	0.003553
69	0.014997	0.003429
70	0.014327	0.003350
71	0.013659	0.003244
72	0.013086	0.003169
73	0.012563	0.003089
74	0.012055	0.003013
75	0.011614	0.002955
76	0.011186	0.002870
77	0.010762	0.002804
78	0.010416	0.002740
79	0.010098	0.002694
80	0.009810	0.002626
81	0.009509	0.002569
82	0.009206	0.002504
83	0.008969	0.002456
84	0.008756	0.002408
85	0.008515	0.002364
86	0.008305	0.002312
87	0.008108	0.002267
88	0.007965	0.002233
89	0.007798	0.002190
90	0.007626	0.002154
91	0.007487	0.002126
92	0.007399	0.002084
93	0.007178	0.002042
94	0.007042	0.002010
95	0.006953	0.001994
96	0.006797	0.001953
97	0.006667	0.001915
98	0.006527	0.001882
99	0.006443	0.001861
100	0.006348	0.001835
101	0.006220	0.001816
102	0.006157	0.001800
103	0.006004	0.001766
104	0.005959	0.001749
105	0.005862	0.001736
106	0.005768	0.001701
107	0.005716	0.001697
108	0.005607	0.001665
109	0.005512	0.001651
110	0.005437	0.001629
111	0.005362	0.001611
112	0.005344	0.001602
113	0.005205	0.001575
114	0.005189	0.001560
115	0.005110	0.001547
116	0.005026	0.001538
117	0.004977	0.001520
118	0.004898	0.001508
119	0.004878	0.001502
120	0.004790	0.001475
121	0.004770	0.001479
122	0.004697	0.001476
123	0.004656	0.001452
124	0.004607	0.001439
125	0.004507	0.001424
126	0.004472	0.001411
127	0.004484	0.001414
128	0.004430	0.001401
129	0.004352	0.001378
130	0.004333	0.001369
131	0.004291	0.001357
132	0.004214	0.001351
133	0.004211	0.001339
134	0.004166	0.001326
135	0.004130	0.001310
136	0.004066	0.001302
137	0.004048	0.001306
138	0.004007	0.001278
139	0.003995	0.001270
140	0.003915	0.001273
141	0.003933	0.001270
142	0.003886	0.001246
143	0.003803	0.001237
144	0.003778	0.001241
145	0.003709	0.001214
146	0.003733	0.001222
147	0.003709	0.001218
148	0.003664	0.001201
149	0.003605	0.001201
150	0.003614	0.001194
151	0.003588	0.001176
