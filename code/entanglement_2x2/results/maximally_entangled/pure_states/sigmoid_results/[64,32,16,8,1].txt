#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled DMs were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2/input_data/received_from_DM/pure_states/entangled_1.txt;
#Architecture of the MLP: [64, 32, 16, 8, 1]; Number of epochs: 151; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: sigmoid; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Batch size: 40; Test tolerance: 0.5; DMs were read with redundancy: False;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.71992998249563%
#Sample standard deviation for averaged success rate: 0.029467426238288703%
#Same average success rate for supplementary tests: [0.50196, 0.5091150000000001, 0.526835, 0.5709599999999999, 0.64546, 0.5782799999999999, 0.5990449999999999, 0.6278550000000002, 0.6533650000000001, 0.684015]%
#Sample STD for averaged success rate in supplementary tests: [0.0034992939173496126, 0.003454210595599231, 0.003344206802628987, 0.003062643289709071, 0.002568446888685845, 0.003023906757821743, 0.002893843361128933, 0.0027054722228753328, 0.0025217422228986834, 0.0023136981196236464]%
#Epoch	Loss	Sample STD
1	0.703488	0.005552
2	0.693474	0.000037
3	0.693354	0.000042
4	0.693393	0.000038
5	0.693275	0.000075
6	0.690747	0.002182
7	0.672995	0.014877
8	0.624125	0.032184
9	0.535788	0.052354
10	0.463052	0.069176
11	0.420190	0.077258
12	0.384813	0.078304
13	0.345815	0.077055
14	0.314784	0.075512
15	0.292721	0.074275
16	0.275804	0.073002
17	0.259158	0.070686
18	0.247477	0.069520
19	0.239120	0.069127
20	0.230088	0.068202
21	0.217974	0.066391
22	0.205278	0.065362
23	0.196384	0.065223
24	0.190507	0.065288
25	0.186419	0.065354
26	0.182766	0.065344
27	0.179752	0.065166
28	0.176741	0.064876
29	0.174040	0.064510
30	0.171411	0.064165
31	0.168864	0.063700
32	0.166065	0.063067
33	0.162634	0.062065
34	0.158374	0.060927
35	0.155934	0.060331
36	0.154371	0.059896
37	0.152241	0.059582
38	0.150564	0.059224
39	0.149052	0.058509
40	0.146588	0.057872
41	0.143700	0.056849
42	0.139587	0.055196
43	0.132419	0.052202
44	0.123679	0.048348
45	0.114989	0.045365
46	0.104916	0.042487
47	0.093676	0.038872
48	0.081180	0.034075
49	0.067576	0.028203
50	0.058513	0.023595
51	0.050388	0.020719
52	0.045626	0.018108
53	0.041220	0.016037
54	0.037893	0.014357
55	0.036069	0.012990
56	0.033177	0.012051
57	0.031316	0.011137
58	0.029918	0.010300
59	0.028740	0.009578
60	0.026600	0.008798
61	0.025901	0.008126
62	0.024645	0.007480
63	0.024652	0.006915
64	0.022618	0.006252
65	0.022332	0.005905
66	0.021266	0.005182
67	0.020759	0.004819
68	0.019336	0.004289
69	0.019282	0.004018
70	0.018546	0.003717
71	0.017786	0.003431
72	0.016865	0.003224
73	0.015927	0.002968
74	0.015656	0.003030
75	0.015384	0.002926
76	0.015793	0.003155
77	0.015682	0.003355
78	0.015360	0.003264
79	0.015360	0.003382
80	0.014339	0.003112
81	0.014164	0.003038
82	0.014346	0.003154
83	0.014553	0.003282
84	0.014882	0.003520
85	0.013940	0.003235
86	0.013918	0.003210
87	0.013695	0.003159
88	0.015408	0.004134
89	0.013783	0.003269
90	0.013403	0.003278
91	0.013932	0.003402
92	0.013426	0.003214
93	0.013575	0.003279
94	0.013520	0.003310
95	0.012949	0.003137
96	0.013430	0.003307
97	0.013935	0.003625
98	0.013862	0.003615
99	0.013198	0.003306
100	0.013354	0.003405
101	0.013458	0.003485
102	0.014066	0.003718
103	0.013132	0.003377
104	0.013193	0.003304
105	0.012378	0.003249
106	0.013365	0.003354
107	0.013036	0.003330
108	0.012577	0.003359
109	0.012658	0.003440
110	0.012577	0.003258
111	0.012079	0.003060
112	0.012716	0.003346
113	0.012325	0.003244
114	0.012312	0.003234
115	0.013170	0.003586
116	0.012319	0.003260
117	0.012569	0.003443
118	0.012747	0.003344
119	0.012998	0.003627
120	0.013274	0.003562
121	0.013201	0.003593
122	0.012103	0.003339
123	0.012353	0.003174
124	0.012737	0.003581
125	0.011848	0.003223
126	0.013293	0.003777
127	0.011819	0.003176
128	0.012415	0.003451
129	0.012442	0.003515
130	0.012621	0.003554
131	0.011793	0.003255
132	0.012045	0.003383
133	0.012023	0.003363
134	0.011795	0.003301
135	0.011940	0.003422
136	0.011833	0.003334
137	0.012264	0.003360
138	0.011567	0.003153
139	0.011609	0.003339
140	0.011556	0.003291
141	0.012443	0.003609
142	0.011868	0.003331
143	0.012373	0.003556
144	0.011577	0.003358
145	0.011800	0.003299
146	0.012171	0.003487
147	0.012785	0.003742
148	0.012622	0.003453
149	0.011877	0.003416
150	0.012120	0.003516
151	0.011907	0.003382
